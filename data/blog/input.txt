---
categories:
- Ideas
date: 2017-02-03T20:57:30+01:00
description: "A non geek article (for now) to talk about the dinner of the philosophers in the cloud"
draft: false
images:
- /assets/images/default-post.png
tags:
- tupleSpace
- topology
- linda
title: Linda, 31yo, with 5 starving philosophers...
---

> __The hand is the tool of tools__ - _Aristotle_.

It ain't no secret to anyone actually knowing me: I am a fan of automation. Automation and configuration management 
have come a long way since [Mark Burgess](http://markburgess.org/) wrote the first version of [cfengine](https://cfengine.com/).

But even if the landscape has changed, operators are still scripting (only the DSL has changed), and the area targeted by those scripts remains technical.

There is no real abstraction nor automation of a design. 

Let me explain that:

* You still need a human to read and understand the architecture of an application. 
* You still need another human to transpile it into a language understandable by a CM tool.
* And you need to configure/script this tool to react on some events to keep the application running and healthy. 

*Note* With a bunch of IT specialists from different major companies, we are trying to figure out the best way to achieve this goal. I will have the opportunity to talk about that in a dedicated post soon.

To describe an application I have had the opportunity to work with [TOSCA](http://docs.oasis-open.org/TOSCA/TOSCA-Simple-Profile-YAML/v1.1/cs0prd01/TOSCA-Simple-Profile-YAML-v1.1-csprd01.html) for a major bank last year (by the way, if you want to play with TOSCA, you can use my [TOSCAlib by Cisco](https://github.com/CiscoCloud/TOSCAlib).

I have really liked the idea of an independent DSL that was able to fully describe an application in a way that it can be writable and understandable by a human as well as a machine.

But it is not enough. TOSCA is based on the idea that you need an orchestrator to operate the workflow. And orchestrator is "bad". The management system must be distributed and autonomous.
(for more about that cf [Configuration management, choreography and self-aware applications](https://blog.owulveryck.info/2016/02/10/configuration-management-choreography-and-self-aware-applications/index.html)

This leads to the idea that the application is a community of elements. And every single element of the community will act regarding the information it gets from the environments and from its peers.

> __Don't communicate by sharing memory; share memory by communicating.__ - _R. Pike_

How can those elements share the information?

# [Tuple Spaces (or, Good Ideas Don't Always Win)](https://software-carpentry.org/blog/2011/03/tuple-spaces-or-good-ideas-dont-always-win.html)

The title of this section is taken from [this blog post](https://software-carpentry.org/blog/2011/03/tuple-spaces-or-good-ideas-dont-always-win.html) which is indeed a good introduction on the tuple-space and how to use them.

## First: What is a tuple

A tuple is simply a finite list of element... the element can be of any type. Therefore a tuple set could be used to describe a lot of things. Because actually we can use a tuple set to describe a vector.
And with several vectors we can describe a matrix, and with matrix...

For example, a digraph can be represented by a tuple set that discribes its adjacency matrix. Therefore, for example, it can then be possible to transpile a TOSCA description to a tuple-set (cf [Orchestrate a digraph with goroutine, a concurrent orchestrator](https://blog.owulveryck.info/2015/12/02/orchestrate-a-digraph-with-goroutine-a-concurrent-orchestrator/index.html) for the decomposition of a TOSCA lifecycle in a matrix).

Now ok, we can describe a workflow... but in a distributed application, how can the node share their states?

## Tuple space...

In short, a tuple space is a repository of tuples that can be accessed concurrently. A tuple space can be seen as a big bucket full of tuple.

The tuple space is visible and consistent through all nodes. The tuple space is the memory!

Ok, so last question: How do we access the tuples? 

# Meet Linda

Linda is a "coordination language" developed by Sudhir Ahuja at AT&T Bell Laboratories in collaboration with David Gelernter and Nicholas Carriero at Yale University in 1986 ([cf wikipedia](https://en.wikipedia.org/wiki/Linda_(coordination_language)))

Linda's principle is very simple as it relies on 4 basic operations:

* _in(t)_ is used to get a tuple from the tuple space if the tuple matches the tuple t. In blocks until a matching tuples exists in the tuple space.
* _rd(t)_ (read) is used to read a tuple from the tuple space if the tuple matches the tuple t. It does not remove it from the tuple space.
* _out(t)_ puts a tuple in the tuple space
* _eval(t)_ is a promise. It evaluates the function contained in a tuple t, immediately returns and will place the result in the tuple space later.

_Important_ A tuple can be __actual__ or __formal__. An actual tuple holds real values. Therefore the _in_ and _rd_ operations on an actual tuple succeed if every single value of the tuple matches.
A formal tuple may holds "variables". Therefore the _in_ and _rd_ operations succeed if the real values match and if the type of the formal match the actual value.

You can find a more complete description of the language and examples [here](http://www.cs.bu.edu/~best/crs/cs551/lectures/lecture-22.html).

# Think big, start small, move fast

Since my colleague [Xavier Talon](https://www.linkedin.com/in/xavier-talon-7bb5261) told me about linda and the idea of using it with TOSCA, I have thousand ideas running around.
What we would like is to use the linda language to coordinate the nodes of an application topology described by TOSCA.
As the topology is  obviously distributed the tuple space I will use/implement must exists at the scale of a cloud platform.

A raft based key/value store could be used as a tuple space. 
And of course the virtual operator that will implement the linda language and interact with the tuple space must be self-contained.
GO would be a good choice for the implementation of the communication agent because of it self-contained, static binary design (maybe RUST would be too but I don't know RUST yet).
Moreover the built-in concurrency could make the development easy (an eval can be triggered simply in a goroutine).

So __let's POC__

First of all First we need to be sure that a distributed tuple-space could work in the cloud.

As a proof of concept, I will use the philosophers dinning problem as simply described in page 452 of the paper [Linda in context](http://www.inf.ed.ac.uk/teaching/courses/ppls/linda.pdf) from Nicholas Carriero and David Gelernter.

My goals are:

* To implement a basic Linda language in go
* To run the philosopher problem locally
* To modify the code so it uses etcd as a tuple space
* To run the philosopher problem on AWS with a philosopher per region
* To use my TOSCAlib to read a topology and encode it in the tuple space
* To run a deployment at scale...

In this post I will present a basic implementation of the language that solves the dinning problem locally.

## The problem

Here is the problem as exposed in the paper:

_A round table is set with some number of plates (traditionally five); there is a single chopstick between each two plates, and a bowl of rice in the center of the table. Philosophers think, then enter the room, eat, leave the room and repeat the cycle. A philosopher can eat without two chopsticks in hand; the two he needs are the ones to the left and the right of the plate at which he is seated.  If the table is full and all philosophers simultaneously grab their left chopsticks, no right chopsticks are available and deadlock ensues. To prevent deadlock, we allow only four philosophers (or one less than the total number of plates) into the room at any one time._ 

## The implementation

I have extracted the C-Linda implementation of this problem and copied it here. 

#### The C linda implenentation
{{< highlight c >}}
Phil(i)
  int i;
{
    while(1) {
      think();
      in("room ticket");
      in("chopstick", i) ;
      in("chopstick", (i+l)%Num) ;
      eat();
      out("chopstick", i);
      out("chopstick", (i+i)%Num);
      out("room ticket");
    }
}
{{</ highlight >}}


{{< highlight c >}}
initialize()
{
  int i;
  for (i = 0; i < Hum; i++) C
    out("chopstick", i);
    eval(phil(i));
    if (i < (Num-1)) 
      out("room ticket");
  }
}
{{</ highlight >}}

### What is needed

To solve this particular problem I don't have to fully implement the linda language. There is no need for the _rd_ action. _eval_ is simply a fork that I will implement using a goroutine and _in_ and _out_ do not use formal tuples.

The actions will communicate with the tuple space via `channels`. Therefore I can create a type Linda composed of two channels for input and output. The actions will be methods of the Linda type.
both _in_ and _rd_ method will get all the tuples in a loop and decide to put them back in the space or to keep it.

{{< highlight go >}}
type Linda struct {
  Input  <-chan interface{}
  Output chan<- interface{}
}
{{</ highlight  >}}

#### The _Tuple_ type
As a tuple I will use a flat go structure. Therefore I can describe a tuple as an interface{}

{{< highlight go >}}
type Tuple interface{}
{{</ highlight  >}}

#### The _in_ action

In will read from the input channel until an object matching its argument is present. If the object read is different, It is sent back in the tuple space via the output channels:

{{< highlight go >}}
func (l *Linda) In(m Tuple) {
  for t := range l.Input {
      if match(m, t) {
        // Assign t to m
        m = t
        return
      }
      // Not for me, put the tuple back
      l.Output <- m
  }
}
{{</ highlight  >}}

### The _eval_ function

The eval function is a bit trickier because we cannot simply pass the function as it would be evaluated before the substitution of the arguments.
What I will do is to pass an array of interface{}. The first argument will hold the function as a first class citizen and the other elements are the arguments of the function.
I will use the reflection to be sure that the argument is a function and executes it in a go routine.

{{< highlight go >}}
func (l *Linda) Eval(fns []interface{}) {
	// The first argument of eval should be the function
	if reflect.ValueOf(fns[0]).Kind() == reflect.Func {
		fn := reflect.ValueOf(fns[0])
		var args []reflect.Value
		for i := 1; i < len(fns); i++ {
			args = append(args, reflect.ValueOf(fns[i]))
		}
		go fn.Call(args)
	}
}
{{</ highlight  >}}

## Back to the philosophers...

#### The Go-linda implementation
Regarding the implementation of Linda, the transcription of the algorithm is simple:

{{< highlight go >}}
for i := 0; i < num; i++ {
    ld.Out(chopstick(i))
    ld.Eval([]interface{}{phil, i})
    if i < (num - 1) {
        ld.Out(ticket{})
    }
}
{{</ highlight >}}


{{< highlight go >}}
func phil(i int) {
    p := philosopher{i}
    fmt.Printf("Philosopher %v is born\n", p.ID)
    for {
        p.think()
        fmt.Printf("[%v] is hungry\n", p.ID)
        ld.In(ticket{})
        ld.In(chopstick(i))
        ld.In(chopstick((i + 1) % num))
        p.eat()
        ld.Out(chopstick(i))
        ld.Out(chopstick((i + 1) % num))
        ld.Out(ticket{})
    }
}
{{</ highlight >}}

### The tuple space
We have Linda... that can put and read tuples via channels... But we still need to plug those channels to the tuple space.
As a first example, we won't store the information and simply pass them from output to input in an endless loop.

{{< highlight go >}}
go func() {
    for i := range output {
        input <- i
    }
}()
{{</ highlight >}}

## Execution

After compiling and executing the code, I can see my philosophers are eating and thinking... 
<pre>
Philosopher 1 is born
[1] is thinking
Philosopher 0 is born
[0] is thinking
Philosopher 3 is born
[3] is thinking
Philosopher 2 is born
[2] is thinking
Philosopher 4 is born
[4] is thinking
[2] has finished thinking
[2] is hungry
[2] is eating
[1] has finished thinking
[1] is hungry
[1] is eating
[4] has finished thinking
[4] is hungry
[4] is eating
...
</pre>
The code can be found here [github.com/owulveryck/go-linda](https://github.com/owulveryck/go-linda/releases/tag/v0.1)

# Conclusion

This is a very basic implementation of the first step. 

In my next experiment, I will try to plug etcd as a tuple space so the philosophers could be distributed around the world.
+++
date = "2015-10-23T10:10:12+01:00"
draft = true
title = "Born in the cloud"

+++

I'm 38, and I'm qualified as a person "_born in the datacenter_". Developping an app _born in the cloud_ is not an evidence for me.
When i'm conceiving an application, or just imaginig a service, I'm still thinking of the technical infrastructure, keeping in mind the technical solution for hosting, resiliency and clustering.

* I'm thinking of the need of a frontend for proxyfing the application and anticipate the scalability. 
* I'm thinking about the storage: which FS, which backup solution, snapshots, SAN, ... 
* I'm evaluating the hosting solution and the application isolement (container, jail, chroot)... 

Anyway, I'm thinking the hosting of the application in the Datacenter way, and any developpement is compliant with this architecture.


---
author: Olivier Wulveryck
date: 2016-01-24T15:25:54+01:00
description: The setup of my new Freeebsd
draft: true
keywords:
- FreeBSD
tags:
title: Setting up my new BSD box
topics:
- topic 1
type: post
---
# About the server

I have subscribed for a now dedicated BSD box.
The provider is OVH, and this box is hosted in Canada.

It's been a few years since I first subscribed a box a OVH, and I really enjoy their service.

The main usage for this box is a `geek box`. I use it to exeperiment some stuffs, I cannot experiment into my home PC.
Therefore I usually create a bunch of `jails`, and each jail is dedicated to a task

Here are the informations of my new box:

```shell
~ uname -a
FreeBSD localhost 10.2-RELEASE-p9 FreeBSD 10.2-RELEASE-p9 #0: Thu Jan 14 01:32:46 UTC 2016
root@amd64-builder.daemonology.net:/usr/obj/usr/src/sys/GENERIC  amd64
```

## ZFS

My root is a ZFS pool named `zroot`

# Basic installation

This box is a `10.2 release` therefore it uses the "new" `pkg` tool instead of the legacy `pkg_*` tools.

## ZSH

### Installation
```shell
pkg install zsh
```
### Oh-my-zsh

Install `git` 

```shell
pkg install git
```

```shell
sh -c "$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)"
```

# Setting up openvpn

## Installation
```shell
pkg update
pkg install openvpn
```

## Configuration
```shell
mkdir /usr/local/etc/openvpn
cp /usr/local/share/examples/openvpn/sample-config-files/server.conf /usr/local/etc/openvpn/server.conf 
```

The default options are used, and only user and groups ares set to nobody for security reasons

```shell
...
# It's a good idea to reduce the OpenVPN
# daemon's privileges after initialization.
#
# You can uncomment this out on
# non-Windows systems.
user nobody
group nobody
...
```

### Generating the keys

```shell
cp -r /usr/local/share/easy-rsa /usr/local/etc/openvpn/easy-rsa
```

### Generating a client certificate

I will generate an openvpn configuration for my chromebook

```shell
cd /usr/local/etc/openvpn/easy-rsa/
./build-key chromebook
```

Then generate the `ovpn` config file

```shell
export CLIENTNAME=chromebook
cp /usr/local/share/examples/openvpn/sample-config-files/client.conf ./client.conf
# Modifying remote...
cp client.conf $CLIENTNAME.ovpn
printf "\n<ca>\n" >> ./$CLIENTNAME.ovpn && \
cat ./ca.crt >> ./$CLIENTNAME.ovpn && \
printf "</ca>\n" >> ./$CLIENTNAME.ovpn && \
printf "\n<cert>" >> ./$CLIENTNAME.ovpn && \
grep -v '^ ' ./$CLIENTNAME.crt | grep -v 'Certificate' >> ./$CLIENTNAME.ovpn && \
printf "</cert>\n" >> ./$CLIENTNAME.ovpn && \
printf "\n<key>\n" >> ./$CLIENTNAME.ovpn && \
cat ./$CLIENTNAME.key >> ./$CLIENTNAME.ovpn && \
printf "</key>\n" >> $CLIENTNAME.ovpn
```

### Firewall

#### Firewall and routing

```shell
# default openvpn settings for the client network
vpnclients = "10.8.0.0/24"
#put your wan interface here (it will almost certainly be different)
wanint = "em0"
# put your tunnel interface here, it is usually tun0
vpnint = "tun0"
# OpenVPN by default runs on udp port 1194
udpopen = "{1194}"
icmptypes = "{echoreq, unreach}"

set skip on lo
# the essential line
nat on $wanint inet from $vpnclients to any -> $wanint

block in
pass in on $wanint proto udp from any to $wanint port $udpopen 
pass in on $wanint proto tcp from any to $wanint port 22 
# the following two lines could be made stricter if you don't trust the clients
pass out quick 
pass in on $vpnint from any to any
pass in inet proto icmp all icmp-type $icmptypes
```

```shell
~ /etc/rc.conf
...
openvpn_enable="YES"
openvpn_configfile="/usr/local/etc/openvpn/server.conf"
```

# Rescue...

Of course, I forgot one rue in my pf.conf and therefore I could not access to my box anymore

## Maganer
Boot into rescue mode

```shell
rescue-bsd# zpool import zroot
rescue-bsd# zpool list
internal error: failed to initialize ZFS library
```

That's because I did import the zroot into /

```shell
zpool import -o altroot=/mnt zroot
```
---
date: 2017-07-07T21:06:46+02:00
description: "Imagine a CCTV at home that would trigger an alert when it detects a movement. Ok, this is easy. Imagine a CCTV that would trigger an alert when it detects a human. A little bit trickier. Now imagine a CCTV that would trigger an alert when it sees someone who is not from the family."
draft: false
images:
- /assets/images/tensorflowserving-4.png
title: A "Smart" CCTV with Tensorflow, and Inception? On a rapsberry pi?
---

Imagine a CCTV at home that would trigger an alert when it detects a movement. 

Ok, this is easy. 

Imagine a CCTV that would trigger an alert when it detects a human (and not the cat). 

A little bit trickier. 

Now imagine a CCTV that would trigger an alert when it sees someone who is not from the family...

__Disclaimer__: This article will not cover everything. I may post a second article later (or not). As you may now, I am doing those experiments during the night as all of this is not part of my job. I hope that I will find time to actually conclude the experiment. If you are a geek and you want to test that with me, feel free and welcome to contact me via the comments or via twitter [@owulveryck](https://twitter.com/owulveryck).
In this article, I will describe the method. I will also retrain a neural network to detect people. I will also use a GO static binary to run it live and evaluate the performances. By the end, I will try a static cross compilation to run it on a raspberry pi, but as my rpi is by now out-of-order, I will test it on qemu.

# Machine learning and computer vision

Machine learning and tooling around it has increased and gained in efficiency in the past years. it now "easy" to code a model that can be trained to detect and classify elements from a picture. 

Cloud providers are offering services that can instantly tag and label elements from an image. To achieve the goal of the CCTV, it would be really easy to use, for example, [AWS rekognition](https://aws.amazon.com/rekognition/), train the model, and post a request for each image seen.

This solution presents a couple of problems:

* The network bandwidth: you need a reliable network and a bandwidth big enough to upload the flow of images

* The cost: these services are cheap for thousand images, but consider about 1 fps to process (I don't even dream of 24fps), it is 86400 images a day and 2.6 million images a month... and considering that 1 million images are 1000 dollar...

I don't even talk about network latency because my CCTV would be pseudo-real-time and the ms of latency can be neglected.

The best solution would be to run the computer vision locally. There are several methods to detect people. The most up-to-date-and-accurate one is based on machine learning and precisely on neural network.

__The very simplified principle of neural network and computer vision:__

There is a lot of literacy on the web around that, but here a very small explanation to understand the rest of this post:

Imagine a picture as a sequence of numbers from 0 to 1 (0 for black 1 for white for example). Imagine a mathematical equation `f`.
You do not know what the content of `f` is. You only tell the computer: "guess a value of `f` such as `f(pictures of a man) = man`.
Then you feed him with a lot of pictures of men, and for every picture, the computer not only guess a function `f` but it adapts it so it can detect every man in every picture.

Sounds magical?

Actually, the computer does not start with a void `f` function. You provide it with a kind of skeleton that you call the neural network.
A neural network is a network of tiny function (neuron) that are applied on the decomposed values of the input (such as a pixel in a photo). 

Depending on the mathematical function coded in the neuron, it is activated by its inputs (there can be several inputs for a single neuron) or not.

You can use several layers of neurons. Each layer is composed of neurons feed by the outputs of the neurons of the previous layer.

The pictures used to feed the model is called the training set. 
You also use a test set (same kind of pictures), that is used to check whether your model generalized well and actually converge to your goal.

I won't dig any further into this description. You can read papers about the [perceptron](https://en.wikipedia.org/wiki/Perceptron) for more accuracy in the description. I heavily recommend this article if you know a little bit of go: [Perceptrons - the most basic form of a neural network](https://appliedgo.net/perceptron/)

<center>
{{< figure src="https://imgs.xkcd.com/comics/machine_learning.png" link="https://xkcd.com/1838/" caption="XKCD 1838" >}}
</center>

## Tools

### Tensorflow

I have already blogged about tensorflow. Tensorflow is not a ML library. It is a mathematical library. It is self-described as  _an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them._

It is, therefore, an excellent tool, suitable for machine learning and especially for dealing with neural networks. It is brilliant with computer vision as the pictures are arrays of pixels and if you add the colour, you can represent every picture by a tensor.

Even better, the models generated by tensorflow can be saved once learned and transferred to another device. For example, you can train your models on very powerful machines and simply copy the resulted graph to your client (for example a phone). The graph can then be applied to an input taken from the device such as a photo.

### Inception

"[Inception](https://research.google.com/pubs/pub43022.html)" is a deep convolutional neural network architecture used to classify images originally developed by Google.

Inception is exceptionally accurate for computer vision. It can reach 78% accuracy in "Top-1" and 93.9% in "Top-5". That means that if you feed the model with a picture of sunglasses, you have 93.9% chance that the algorithm detects sunglasses amongst the top 5 results.

On top of that, Inception is implemented with Tensorflow, and well documented. Therefore, it easy "easy" to use it, to train it and "to retrain it".

here is a graphical representation of the inception v3 model. You can see the different layers of the model as explained earlier.

<center>
{{< figure src="https://raw.githubusercontent.com/tensorflow/models/master/inception/g3doc/inception_v3_architecture.png" link="https://github.com/tensorflow/models/tree/master/inception" caption="Inception v3 architecture" >}}
</center>

Actually, training the model is a very long process (several days on very efficient machines with GPU). But some folks (at google?) have discovered that retraining only the last layer of the neural network for new classes of pictures was giving good results.

# Geek

I am using the excellent blog post [How to Retrain Inception's Final Layer for New Categories](https://www.tensorflow.org/tutorials/image_retraining).
The purpose of the article is to retrain the network in order to give it the ability to categorize (recognize) different kind of flowers.
I will use exactly the same principle to recognize a class "people".

I will perform the task on a spot instance on AWS (to get it cheap), and download the model to use it locally from a go code.

## Phase 1: recognizing usual people

To keep it simple, I've created a "class" people with the flowers classes. It means that I simply added a directory "people" to my "flowers" for now.

```
[~/flower_photos]$ ls -lrt
total 696
-rw-r----- 1 ubuntu ubuntu 418049 Feb  9  2016 LICENSE.txt
drwx------ 2 ubuntu ubuntu  45056 Feb 10  2016 tulips
drwx------ 2 ubuntu ubuntu  36864 Feb 10  2016 sunflowers
drwx------ 2 ubuntu ubuntu  36864 Feb 10  2016 roses
drwx------ 2 ubuntu ubuntu  57344 Feb 10  2016 dandelion
drwx------ 2 ubuntu ubuntu  36864 Feb 10  2016 daisy
drwxrwxr-x 2 ubuntu ubuntu  77824 Jul  7 14:26 people
```

### Getting a training set full of people

I need a training set of people. That means that I need a certain amount of pictures actually representing some people.
Nowadays it is easy to get training set for free (as in free speech) on the web. 

_Note_ You can see that, by offering services, the GAFA is increasing its training set to make their service more powerful than ever.
<center>
{{< tweet 857609299731791872 >}}
</center>

Let's get back to my experiment:
I download pictures of people from [http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n07942152](http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n07942152)

{{< highlight shell >}}
curl -s  "http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n07942152" | \
sed 's/^M//' | \
while read file
do
  curl -m 3 -O $file
done
{{</ highlight >}}

Then I remove all "non-image" files:

{{< highlight shell >}}
for i in $(ls *jpg)
do
    file $i | egrep -qi "jpeg|png" || rm $i 
done
{{</ highlight >}}

## Learning phase

I've had one "issue" during the learning phase. When I executed:

`bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos/` 

it failed with a message about `ModuleNotFoundError: No module named 'backports'`. I Googled and found the solution in this [issue](https://github.com/tensorflow/serving/issues/489#issuecomment-313671459). It is because I am using python3 and the tutorial has been written for python 2. No big deal.

At the end of the training (which took 12 minutes on a c4.2xlarge spot instance on AWS) I have two files that hold the previous information.

```
...
2017-07-07 19:22:53.667219: Step 3990: Cross entropy = 0.111931
2017-07-07 19:22:53.728059: Step 3990: Validation accuracy = 93.0% (N=100)
2017-07-07 19:22:54.287266: Step 3999: Train accuracy = 98.0%
2017-07-07 19:22:54.287365: Step 3999: Cross entropy = 0.148188
2017-07-07 19:22:54.348603: Step 3999: Validation accuracy = 91.0% (N=100)
Final test accuracy = 92.7% (N=492)
Converted 2 variables to const ops.
...
```

And a trained graph with a label file that I can export and use elsewhere.

```
(customenv) *[r1.2][~/sources/tensorflow]$ ls -lrth /tmp/output_*
-rw-rw-r-- 1 ubuntu ubuntu  47 Jul  7 19:22 /tmp/output_labels.txt
-rw-rw-r-- 1 ubuntu ubuntu 84M Jul  7 19:22 /tmp/output_graph.pb
```
I have followed the tutorial to [Use the retrained model](https://www.tensorflow.org/tutorials/image_retraining#using_the_retrained_model) to make sure that everything was ok before using it with my own code.

## Using the model with go

Tensorflow is coded in C++, but has some bindings for different languages. The most up-to-date is python, in which a lot of helper libraries are developed (see [tflearn](http://tflearn.org/getting_started/) for example.
A binding for go exists, but it is only implementing the core library of tensorflow. Anyway, it is an excellent choice for applying a model.

The workflow is:

- read the exported model from the disk and create a new graph
- read the label files and set the labels in an array of string
- grab jpeg pictures from the webcam in jpeg (via v4l) in an endless for loop
- Normalize the picture (see below) and create a tensor from the jpeg file.
- Apply the inception model onto the Tensor and getting the `final_result`
- Extract the most important value from the output vector (the better probability) and display the corresponding label.

I will only expose the trickiest parts.

### Getting the pictures

I use a wrapper around `v4l` in go called [go-webcam](https://github.com/blackjack/webcam). As my webcam has MJPEG capabilities, each frame is already in JPEG format.

I am applying the tensorflow model sequentially within the for loop. The problem is that it takes some time to process. And while it is processing the driver may buffer some pictures. Therefore I am totally losing the synchronism. My code may warn me that it has found a person too late.
To avoid this, I am using a non-blocking tick in a go channel within the loop. Therefore I do not process every single frame, but I process a frame every x milliseconds and I discard the rest.
I could have used a pool, but that would have add complexity for the example.

{{< gist owulveryck 1f3fc2366e5a35ab119633d57ad074b6 "tick.go" >}}

### Normalizing the picture

The [example described on the go package](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go) is using an old inception implementation (actually version 5h which is older than the v3). Therefore it needs some adjustments. The function that produces a Tensorflow graph that will be used to normalize the picture didn't have the correct normalization values (those defined by the author of the inception v3 model) 

Here is an extract from [Image Recognition](https://www.tensorflow.org/tutorials/image_recognition):

_The model expects to get square 299x299 RGB images, so those are the `input_width` and `input_height` flags. We also need to scale the pixel values from integers that are between 0 and 255 to the floating point values that the graph operates on. We control the scaling with the `input_mean` and `input_std` flags: we first subtract `input_mean` from each pixel value, then divide it by `input_std`._
 
_These values probably look somewhat magical, but they are just defined by the original model author based on what he/she wanted to use as input images for training. If you have a graph that you've trained yourself, you'll just need to adjust the values to match whatever you used during your training process._

{{< gist owulveryck 1f3fc2366e5a35ab119633d57ad074b6 "normalizationgraph.go" >}}

Apart from that the rest of the code remains similar.

#  Conclusion

## Running it on a laptop

The program runs as expected at the rate of 2 images per seconds without overheating on a modern laptop. I have used it to monitor my house while I was on vacation. Every success was sent on an s3 bucket, so in case of the intrusion in my house, I would still have the pictures. I say that it has worked because the only pictures it has recorded were:

* me, leaving the house
* me, entering the house 2 weeks later.

You can find the full code on [my github](https://github.com/owulveryck/smarcctv)

## Further work

### Running on ARM
I want to test it on a raspberry pi, so I have cross compiled the code for ARM with those commands but I didn't have time to test it yet:

{{< highlight bash >}}
# Download a tensorflow release for rpi:
$ wget https://github.com/meinside/libtensorflow.so-raspberrypi/releases/download/v1.2.0/libtensorflow_v1.2.0_20170619.tgz
# Install the toolchain
$ sudo apt install gcc-arm-linux-gnueabihf
# Compile it
$ export CC=arm-linux-gnueabihf-gcc
$ CC=arm-linux-gnueabihf-gcc-5 GOOS=linux GOARCH=arm GOARM=6 CGO_ENABLED=1 go build  -o myprogram -ldflags="-extld=$CC"
{{</ highlight >}}

#### Performances

Inception is very good. But it requires a decent CPU (or even better a GPU). I could use another model called [MobileNet](https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.md) which is a _low latency, low power_ model. 
It has been [opensourced](https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html) in June 2017. The tensorflow team has added the ability to retrain it the same way inception is (by retraining the last layer). It's worth a look.

### Detecting only the family
As I explained in the beginning of the post, I want this system to trigger only if it detects someone that is not part of the family.
To do that I need to train the neuron network to classify classes such as: 

* people 
* me 
* my wife 
* kid1
* kid2 

To do so, I need training sets (labeled pictures) of my family. The best way to get it is to write a "memory cortex" to use it with my [cortical](https://github.com/owulveryck/cortical) project as explained in my previous post: [Chrome, the eye of the cloud - Computer vision with deep learning and only 2Gb of RAM](https://blog.owulveryck.info/2017/05/16/chrome-the-eye-of-the-cloud---computer-vision-with-deep-learning-and-only-2gb-of-ram.html).

---
author: Olivier Wulveryck
date: 2016-02-01T09:42:15+01:00
description: Configuration management camp
draft: true
keywords:
- key
- words
tags:
- one
- two
title: cfg mgm camp
topics:
- topic 1
type: post
---


## To reach the next level we must *stop doing configuration management*

## from *scarcity* to abundance


The problem is now:

    * How do we choose and manage software *

## this is the age of *big software*

Actually: knoledge scarce...

## scarcity has shifted from code to *ops*

## the solution is *reusable, onpensource ops*

## encapsulation of a sofware

deb, rpm, ... but encapsulation requires a model

## the modeling language

the modeling language for applicatios
* model the software, not the machines
* model the software, not the configuration files

Introducing juju
...


https://xkcd.com/1319/

## beyond automation: reuse & sharing !
You should stop doing configuration management for software uniq to your organisation



# Valut
hashicorp product

## What is a secret 

Secret vs sensisitv

Secret:
* db credentials
* SSL CA / certificates
* Cloud access key
* wifi password
* source code

Sensitive :
* Phone numbers
* email addresses
* Datacenter location


## Why not config management

* No access control
* No auditing
* No revocation
* No key rolling


## Why not (onlinr) database?
* Not designed for secrets
* limited access controls
* typically plaintext storage
* no auditing or recovation abilities

## how to handle secret sprawl?
* secret material is distributed
* who has access?
* when were serets used?
* what is the attack surface ?
* what do we do in the event of compromise?

## Goals of vault:

* single source for secrets, certificates
* programmatic application access (automated)
* operator access (manual)
* practical security
* modern data center friendly(private or cloud, commodity hardware, highly available, etc.)




MK :Everything is about resuse


self-healing infrastrycture

# Container and metadata
containers labels

The power of system packages lies not in the file format but in the *metadata*

## Standards : the power of agreement

Without complete metadata we can't trust the tools built on top
---
author: Olivier Wulveryck
date: 2016-02-10T17:19:47+01:00
description: From system configuration management to self aware application.
  How did we pass from automation to infrastructure as data ?
  What is the future of automation ?
  In this post I describe some ideas I have collected about the principles that could lead
  to self-aware application.
  I will also present a proof of concept I made to illustrate those ideas.
draft: false
keywords:
- choreography
- orchestration
- topology
tags:
- config management
- choreography
- orchestration
- topology
- TOSCA
- go
- khoreia
title: Configuration management, choreography and self-aware applications
topics:
- application deployment
type: post
---

Thanks to the [company I'm working for (Techsys)](https://www.linkedin.com/company/techsys?trk=company_logo) I've had the opportunity to attend the [configuration management camp](http://cfgmgmtcamp.eu/) in Gent (_be_) for its 2016 edition.

I really enjoyed those two days of talks, watching people present different ideas of a possible future for
the infrastructure and deployment engineering. 
Beyond the technical demonstrations and the experience sharing, I've spotted a bunch of ideas

Among all, those that comes to me spontaneously are:

> You don't need configuration management, what you need is a description of the topology of your application - *[Mark Shuttleworth](http://www.markshuttleworth.com/biography)* in its keynote _The magic of modeling_

> You don't need orchestration, what you need is choreography - Exposed by _[Julian Dunn](https://www.linkedin.com/in/julian)_
(you can find a transcription [here on youtube](https://www.youtube.com/watch?v=kfF9IATUask))

> What we need is a new way to do configuration management - _[James Shubin](https://www.linkedin.com/in/james-shubin-74a89a44)_, see [his blog post](https://ttboj.wordpress.com/2016/01/18/next-generation-configuration-mgmt/) which ispired my project [khoreia](http://github.com/owulveryck/khoreia)

I came back home very excited about this.
This post tries to expose my reflection and how I've implemented an idea (see it as a POC)
I've passed some time to learn about TOSCA, and the to code an orchestrator. 

In a first part I will expose why, according to me, the topological description of the application may be what
company needs.

Therefore, I will notice the need for orchestration tools.

Even if the concepts remains actuals, the future may be an evolution of this mechanism of central command and control. 
In the last part of this post, I will expose what I've understood of the concept of choreography so far.

Finally I will demonstrate the idea with a POC based on a development on [the etcd product](https://github.com/coreos/etcd) from CoreOS.
(and a youtube demo for those who don't want to `git clone...`)

## Configuration management and orchestration

Configuration management has been for a long time, a goal for IT automation. 
Years ago, it allowed system engineers to control a huge park of machines while maintaining a TCO at a relatively decent level.

Over the last decade, 4 major tools have emerged and are now part of most CTO common vocabulary.

Let's take a look at the trends from 4 major tools categorized as "configuration management tools":

| Tool        | Founded in |
| ----------- |:----------:|
| Ansible     | 2012       |
| Puppet      | 2005       |
| Chef        | 2009       |
| Salt        | 2011       |

_Note_: I do not represent CFEngine because it is doesn't seem not so widely used in dotcom companies (even if it seems to be a great tool and on a certain extent the father of the others)

The "interest" for those tools as seen by Google is be represented like this:

<center>
<script type="text/javascript" src="//www.google.com/trends/embed.js?hl=en&q=/m/0k0vzjb,+/m/03d3cjz,+/m/05zxlz3,+/m/0hn8c6s&date=1/2014+25m&cmpt=q&tz=Etc/GMT-1&tz=Etc/GMT-1&content=1&cid=TIMESERIES_GRAPH_0&export=5&w=700&h=350"></script>
</center>

As we can see, Ansible seems to be the emerging technology. Indeed its acquisition by redhat in late 2015 may have boosted a bit the trends, but anyway, the companies that do not implement infrastructure as code may seem to prefer this tool.
Cause or consequence, Gartner has nominated Ansible as a _cool vendor_ for 2015 (according to Gartner, a Cool Vendor is an emerging and innovative vendor that has original, interesting, and unique technology with real market impact)

Why did a newcomer such as Ansible did present such interest?

Beside its simplicity, Ansible is not exactly a configuration management tool, it is **an orchestrator** (see [the Ansible webpage](https://www.Ansible.com/orchestration))

According to [Rogger's theory](https://en.wikipedia.org/wiki/Diffusion_of_innovations) about the diffusion of innovation, and regarding the trends, I think that it is accurate to say
that the position of Ansible is near the "late majority"
<center>
![Diffusion of ideas](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Diffusionofideas.PNG/330px-Diffusionofideas.PNG)
</center>

What does this mean ?

To me,it means that people do feel the need for orchestration, or if they don't feel it, they will thanks to Ansible. 
Via orchestration, they may feel the need for representing their product.

We are now talking about **infrastructure as data**; soon we will talk about **architecture as data**

### From system configuration management...

I did system administration and engineering for years. Configuration management was the answer to the growing of the infrastructure.
Config management allowed us to

- Get the systems reliable
- Get the best efficiency possible from the infrastructure
- Maintain a low TCO
...

It was all "system centric", so the application could be deposed and run in best conditions.

### ... to application's full description

A couple of years ago, maybe because of the DevOps movement, my missions were getting more and more application centric (which is good). 
Actually infrastructure has not been considered as a needed cost anymore.

Thanks to _Agility_, _DevOps_, and the emergent notion of product (as opposed to project), **Application and infrastructure are now seen as a whole**.  
(I'm talking of the application "born in the data center", it is different for those "born in the cloud")

Therefore, the setup of the application must not rely only on programmed configuration management tools anymore, but on its complete **representation**

# The self-sufficient application

Some times ago, I wrote article published on [pulse](https://www.linkedin.com/pulse/from-integration-self-sufficient-application-olivier-wulveryck?trk=prof-post) because I wanted to lay down on paper what I thought about the future of application deployment.
I've described some layers of the application.
I kept on studying, and with a some help from my colleagues and friends, I've  finally been able to put a word on those ideas I had in mind.

This word is **Topology**

## and then came TOSCA

To describe a whole application, I needed a _domain specific language_ (DSL).
All of the languages I was trying to document were by far too system centric.
Then I discovered [TOSCA](http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/csprd01/TOSCA-Simple-Profile-YAML-v1.0-csprd01.html).
TOSCA is __THE DSL__ for representing the topology of an application.

### Pros...
What's good about Tosca is its goal:

It describes a standard for representing a cloud application. It is written by the Oasis consortium and 
therefore most of the big brand in IT may be aware of its existence.
The promise is that if you describe any application with Tosca, it could be deployed on any platform, with a decent __orchestrator__.

### ...and cons
But... Tosca is complex.
It's not that simple to write a Tosca representation. The standard wants to cover all the possible cases, and according [Pareto](https://en.wikipedia.org/wiki/Vilfredo_Pareto)'s law,
I can say that 80% of the customers will only need 20% of the standard.

On top of that, Tosca is young (by now, the YAML version is still in pre-release), and I could not find any decent tool to orchestrate and deploy an application. 
Big companies claim their compliance with the standard, but actually very few of them (if any) does really implement it.

## Let's come back to orchestration (and real world)
As seen before, a Tosca file would need a tool to transform it to a real application.
This tool is **an orchestrator**.

The tool should be called __conductor__, because what is does actually is to conduct the symphony, and yet in our context the symphony is not 
represented by the topology, but by its 'score': its execution plan, and the purpose of the 'orchestrator' is to make every node to play its part
so the application symphony could be rendered in best condition of reliability and efficiency...

Wait, that was the promise of the configuration management tools, isn't it?

### The execution plan
So what is the execution plan.
An execution plan is a program. It describes exactly what needs to be done by systems.
The execution plan is deterministic.

With the description of the application, the execution plan, and the orchestration, the ultimate goal of automation seems fulfilled, indeed!
We have a complete suite of tools that allows to describe the application and architecture base on its functions and it is possible to 
generate and executes all the commands a computer **must** do to get things done.

Why do we need more?
Because now systems are so complex that we could not rely anymore on IT infrastructure to do exactly what we told it to.
Mark Burgess, considered by a lot of people as a visionary, wrote a book entitled: 
[In Search of Certainty: The science of our information infrastructure](http://www.amazon.com/In-Search-Certainty-information-infrastructure/dp/1492389161)

Julian Dunn told about it in its speech, and I've started reading IT.

The conclusion is roughly: 

_We may not rely on command and control anymore, we should make the system work on its own to reach a level of stability_

# Dancing, Choreography, Jazz ?

A solution to the orchestration SPOF and  a workaround for dealing with the uncertainty of the infrastructure may be to implement a choreography. 
Or to replace the symphony with a piece of jazz. 
You give every attendee (dancer, jazzman or TOSCA node) the structure of the piece to play.
And given the chords and the structure, they all react and do what they have committed to do.

This should produce similar to the same mechanism controlled by an orchestrator, but more fault tolerant.
Actually, the brain will not have to take care of unpredicted event; each node will do so.
The application has become self-aware.

## Implementation: a distributed system

This concept, described in so many sci-fi books, may become applicable because science formalized consensus algorithm such as
paxo or raft.
And even better, it is easy to find very good implementation of those concepts (for free)

`etcd` from CoreOS is one of those tools.
It is a service oriented key/value store, distributed on a cluster of machine.

It can be used as a communication based for a cluster of nodes composing a choreography.

Even more, etcd clients have the ability to monitor an event allowing us to implement the self awareness of the application.

## Proof of concept: khoreia

khoreia is a little program I made in `go` that relies on the [etcd](http://github.com/coreos/eetcd) distributed system.
Etcd itself is an implementation of the raft consensus algorithm. I do heavily advice that you take a look at [this page](http://thesecretlivesofdata.com/raft/)
for a complete and clear explanation.

The khoreia single binary takes a topology description in yaml 
(by now very simple, but sooner or later I may implement the TOSCA DSL, as I already
have a [Tosca library](http://github.com/owulveryck/toscalib)).

Then it triggers the nodes and every node reacts on events.
Regarding the events, it implements the expected life cycle for the node.

Without actually coding it, the complement life cycle of the application is then applied.
Even better, the application is fault tolerant (if a check fails, the do method is called again) and the execution 
is completely stateless because of the event based mechanism.

### Screencast: a little demo on distributed systems based on event on filesystems

Here is a little screencast I made as a POC.
Two machines are used (linked by a VPN):

- my chromebook, linux-based at home in France;
- a FreeBSD server located in Canada.

Both machines are part of an etcd cluster.
The topology is composed of 8 nodes with dependencies which can be represented like this (same example as the one I used in a previous post):
<img class="img-responsive" src="/assets/images/digraph1.png" alt="digraph example"/> 

Nodes 0-3 are targeted on the chromebook while nodes 5-7 are targeted on the BSD machine.

The topology is described as a simple yaml file [here](https://github.com/owulveryck/khoreia/blob/e32af3d77d0b57bf36b68a2460e5a47132f29118/samples/topology.yaml)

Each node is fulfilling two methods:

* Create
* Configure

And each method is implementing an interface composed of:

* `Check()` which check whether the action has been release and the "role" is ok
* `Do()` which actually implements the action

_Note_: The signature of the method is actually a bit different, and the go implementation involve channels, but I does not change the principle,
so I've decided to omit that for clarity.

#### Example
Each node will:

1. **Wait for an event** and call Create.Check() and Configure.Check().
2. watch for events from their dependencies
3. if an event is detected, call the appropriate Do() method

#### Engine
The interfaces `Check()` and `Do()` may be implemented on different engines.

For my demo, as suggested by James I'm using a "file engine" base on iNotify (linux) and kQueue (freebsd).

The `Check()` method is watching the presence of a file. It sends the event "true" if the file is created of "false" if its deleted.

The `Do()` method actually create an empty file.

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/l96uFQUrcp8" frameborder="0" allowfullscreen></iframe>
</center>

#### Khoreia on github:

[github.com/owulveryck/khoreia](http://github.com/owulveryck/khoreia)

# Conclusion

Self-awareness, self-healing, elasticity, modularity, ... with a choreography based configuration and deployment tools, standard application may get 
new capabilities without totally rethinking their infrastructure.

Some of the stuff that still need to be implemented are, for example,  the notion of interface and commitment of the node, and the notion of
machine learning for every node to teach them how to react to different events in an efficient way.
---
categories:
date: 2017-05-16T21:43:46+02:00
description: "Is this post about Machine Learning? Well, not really, but it is highly related. In this post I will explain how to use a web browser to get information about the environment (pictures and sound). Then, I will present a simple way to process and interact with this information. Why do I do that? At first, simply because I am (trying) to play with tensorflow, chatbots etc, and I need a simple way to grab information to create a training set... But with the evolution of my code, I am now using it alongside with the cloud API of AWS. Welcome to my world."
draft: false
images:
- https://lh3.googleusercontent.com/nYhPnY2I-e9rpqnid9u9aAODz4C04OycEGxqHG5vxFnA35OGmLMrrUmhM9eaHKJ7liB-=w300
tags:
- ML
- Chrome
title: Chrome, the eye of the cloud - Computer vision with deep learning and only 2Gb of RAM
---

**TL;DR:** Thank you for passing by. This article is, as usual, geek oriented. However, if you are not a geek, and/or you are in a hurry, you can jump to the conclusion: _[Any real application?](#any-real-application)_

During the month of may, I have had the chance to attend to the Google Next event in London and the dotAI in Paris. In both conferences I learned a lot about machine learning. 

What those great speakers have taught me is that you should not reinvent the wheel in AI. Actually a lot of research is done and there are very good implementation of the latest efficient algorithm.

*The tool* that every engineer that wants to try AI must know is [tensorflow](https://www.tensorflow.org/). Tensorflow is a generic framework that has been developed by Google's Machine Intelligence research organization. The tool has been open-sourced last year and has reached the v1.0 earlier this year.

## So what makes tensorflow so great?

### Bindings
First of all, it has bindings so it can be used within various programming languages such as:

* python
* c++
* java
* go

However, to be honest, mainly python and c++ are described in the documentation. And to be even more honest I think that python is the language that you should use to prototype applications.

### ML and neuron network examples

Tensorflow is easy to use for machine learning, and a lot of deep-learning implementation are available.
Actually it is very easy to download a trained model and use it to recognize some pictures for example.

### Built-in computation at scale

Tensorflow's model has a built-in way to perform distributed computation. It is really important as machine learning is usually a very intensive task in term of computation.

### GCP's ML engine

Tensorflow is the engine used by Google for their service called ML engine.
That means that you can write your function locally and run them serverless on the cloud.
You only pay for what you have effectively consumed.
That means for example that you can train a neuron network on GCP (so you don't need GPU. TPU, or whatever computing power) and transfer your model locally.

For example, this is how the mobile app "google translate" works. A pre-trained model is downloaded on your phone, and the live translation is done locally.

![Image](http://technews.wpengine.netdna-cdn.com/wp-content/uploads/2015/01/www.lanacion.com_.ar_.jpg)

_Note_ Other ML services from GCP such as cloud vision, translate, or image search, are "just" API that query a neuron network with a model trained by google.

# So What?

I want to play with image recognition. Actually I already did a test with AWS's rekognition service ([See this post](/2016/12/16/image-rekognition-with-a-webcam-go-and-aws..html)).  However, the problems were:

* I relied on a low-level webcam implementation. Therefore, the code was not portable;
* I had no preview of what my computer was looking at;
* I could not execute it on any mobile app for a demo; 

As I am using a Chromebook for a while, I found a solution: Using a Javascript API and the Chrome browser to access the camera. Then, the pictures can be transfered to a backend via a websocket. The backend would do the ML and reply with whatever information via the websocket. I can then display the result or even use the voice api of Chrome to tell the result loud.

# Chrome as the eye of the computer

The idea is to get a video stream and grab pictures from this stream in order to activate a neural network.

I will present different objects in front of my webcam, and their name will be displayed on the screen.

The architecture is client server: The Chrome is the eye of my bot, it communicates with the brain (a webservice in go that is running a pre-trained tensorflow neural network) via a websocket.

**The rest of this paragraph is geek/javascript, if you're not interested you can jump to the next paragraph about the brain implementation called _[Cortical](#the-brain-cortical)_**

## getUserMedia

I am using the Web API [MediaDevices.getUserMedia()](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia) to open the webcam and get the stream.

This API is compatible with chrome on desktop *and* mobile on Android phone (but not on iOS). This means that I will be able to use a mobile phone as an "eye" of my bot.

See the [compatibility matrix here](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia#Browser_compatibility)

Here is the code to get access to the camera and display the video stream:

_html_
{{< highlight html >}}
<body>
  <video autoplay id="webcam"></video>
</body>
{{</ highlight >}}

_Javascript_
{{< highlight js >}}
// use MediaDevices API
// docs: https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia
if (navigator.mediaDevices) {
    // access the web cam
    navigator.mediaDevices.getUserMedia({video: true})
      // permission granted:
      .then(function(stream) {
          video.src = window.URL.createObjectURL(stream);
      })
      // permission denied:
      .catch(function(error) {
          document.body.textContent = 'Could not access the camera. Error: ' + error.name;
      });
}
{{</ highlight >}}

## Websockets

According to Wikipedia's definition, Websocket is _a computer communications protocol, providing full-duplex communication channels over a single TCP connection_.
The full duplex mode is important in my architecture. 

Let me explain why with a simple use case:

Imagine that your eye captures a scene and sends it to the brain for analysis. In a classic RESTfull architecture, the browser (the eye) would perform a POST request.
The brain would reply with a process ID, and the eye would poll the endpoint every x seconds to get the processing status.

This can be tedious in case of multiple stimuli.

Thanks to the websocket, the server can send the query, and the server will send an event back once the processing is done.
Of course this is stateless in a sort, as the query is lost once the browser is closed.

Another use case would be to get a stimulus from another "sense". For example, imagine that you want to "warn" the end user that he has been mentioned in a tweet. The brain can be in charge of polling
twitter, and it would send a message through the websocket in case of event.

### Connecting to the websocket

A websocket URI is prefixed by `ws` or `wss` if the communication is encrypted (aka https).
This code allows a connection through ws(s).

{{< highlight js >}}
var ws
// Connecting the websocket
var loc = window.location, new_uri;
if (loc.protocol === "https:") {
  new_uri = "wss:";
} else {
  new_uri = "ws:";
}
new_uri += "//" + loc.host + "/ws";
ws = new WebSocket(new_uri);
{{</ highlight >}}

### Messages

Web socket communication is message oriented. A message can be sent simply by calling the function `ws.send(message)`. Websockets are supporting texts and binary messages.
But for this test only text messages will be used (images will be encoded in base64).

The browser implementation of a websocket in javascript is event based. 
When the server sends a message, an interruption is fired and the `ws.onmessage` call is triggered.

This code will display the message received on the console:

{{< highlight js >}}
ws.onmessage = function(event) {
  console.log("Received:" + event.data);
};
{{</ highlight >}}

### Sending pictures to the websocket: actually seeing

I didn't find a way to send the video stream to the brain via the websocket. Therefore, I will do what everybody does: create a canvas and "take" a picture from the video:

The method [toDataURL()](https://developer.mozilla.org/en-US/docs/Web/API/HTMLCanvasElement/toDataURL) will take care of encoding the picture in a well-known format (png or jpeg).

{{< highlight js >}}
function takeSnapshot() {
  var context;
  var width = video.offsetWidth
  , height = video.offsetHeight;

  canvas = canvas || document.createElement('canvas');
  canvas.width = width;
  canvas.height = height;

  context = canvas.getContext('2d');
  context.drawImage(video, 0, 0, width, height);

  var dataURI = canvas.toDataURL('image/jpeg')
  //...
};
{{</ highlight >}}

To make the processing in the brain easier, I will serialize the video into a json object and sending it via the websocket:

{{< highlight js >}}
var message = {"dataURI":{}};
message.dataURI.content = dataURI.split(',')[1];
message.dataURI.contentType = dataURI.split(',')[0].split(':')[1].split(';')[0]
var json = JSON.stringify(message);
ws.send(json);
{{</ highlight >}}

## Bonus: ear and voice

It is relatively easy to make chrome speak out loud the message received. This snippet will speak out loud the message Received:

{{< highlight js >}}
function talk(message) {
  var utterance = new SpeechSynthesisUtterance(message);
  window.speechSynthesis.speak(utterance);
}
{{</ highlight >}}

Therefore, simply adding a call to this function in the "onmessage" event of the websocket will trigger the voice of Chrome. 

Listening is a bit trickier. It is done by a call to the `webkitSpeechRecognition();` method. This [blog post](https://developers.google.com/web/updates/2013/01/Voice-Driven-Web-Apps-Introduction-to-the-Web-Speech-API) explains in detail how this works.

The call is also event based. What's important is that, in chrome, by default, it will use an API call to the Google's engine. Therefore the recognition won't work offline.

When the language processing is done by chrome, five potential sentences are stored in a json array.
The following snippet will take the most relevant one and send it to the brain via the websocket:

{{< highlight js >}}
recognition.onresult = function(event) { 
  for (var i = event.resultIndex; i < event.results.length; ++i) {
    if (event.results[i].isFinal) {
      final_transcript += event.results[i][0].transcript;
      ws.send(final_transcript);
    }
  }
};
{{</ highlight >}}

_Now that we have set up the senses, let's make a "brain"_

# The _brain_: **Cortical**
![Picture](https://github.com/owulveryck/cortical/raw/master/doc/cortical.png)


Now, let me explain what is, according to me, the **most interesting part** of this post. By now, all that I have done is a bit of javascript to grab a picture. This is not a big deal, and there is no machine learning yet (besides the speech recognition built-in in chrome).
What I need now is to actually process the messages so the computer can tell what it sees.

For this purpose I have developed a message dispatcher. This dispatcher, called _Cortical_  is available on [github](https://github.com/owulveryck/cortical)

Here is an extract from the README of the project:

----

**What is Cortical?**

Cortical is a go ~~framework~~ ~~middleware~~ piece of code that acts as a message dispatcher. The messages are transmitted in full duplex over a websocket.
Cortical is therefore a very convenient way to distribute messages to "processing units" (other go functions) and to get the responses back in a **concurrent** and **asynchronous** way.

The "processing units" are called _Cortexes_ and do not need to be aware of any web mechanism.

----

So far, so good, I can simply create a handler to receive the messages sent by the chrome browser in go:

{{< highlight go >}}
brain := &cortical.Cortical{
    Upgrader: websocket.Upgrader{},
    Cortexes:  []cortical.Cortex{
                    &sampleTensorflowCortex{}, // cortex?
               }, 
}
http.HandleFunc("/ws", brain.ServeWS)
log.Fatal(http.ListenAndServe(":8080", nil))
{{</ highlight >}}

_Note_: **Concurrency** and **asynchronicity** are really built in _Cortical_, this is what makes this code so helpful actually.

## _Cortexes_

Cortexes are processing units. That is the place where messages are analyzed and where the ML magic happens.

From the readme, I quote:

----

A cortex is any go code that provides two functions:

* A "send" function that returns a channel of `[]byte`. The content of the channel is sent to the websocket once available (cf [`GetInfoFromCortexFunc`](https://godoc.org/github.com/owulveryck/cortical#GetInfoFromCortexFunc))
* A "receive" method that take a pointer of `[]byte`. This function is called each time a message is received (cf [`SendInfoToCortex`](https://godoc.org/github.com/owulveryck/cortical#SendInfoToCortex))

A cortex object must therefore be compatible with the `cortical.Cortex` interface:

----

Ok, let's build Cortexes!

### A tensorflow cortex runnig locally

The tensorflow go package is a binding to the `libtensorflow.so`. It has a very nice example described in the [godoc here](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go#ex-package).
This example is using a pre-trained inception model ([http://arxiv.org/abs/1512.00567](http://arxiv.org/abs/1512.00567)).
The program starts by downloading the pre-trained model, creates a graph, and try to guess labels on a given image.

I will simply add the expected interface to transform this example into a Cortex compatible with my previous declaration (_some error check and some code has been omited for clarity_):

{{< highlight go >}}
type sampleTensorflowCortex struct{}

func (t *sampleTensorflowCortex) NewCortex(ctx context.Context) (cortical.GetInfoFromCortexFunc, cortical.SendInfoToCortex) {
        c := make(chan []byte)
        class := &classifier{
                c: c,
        }
        return class.Send, class.Receive
}

type classifier struct {
        c chan []byte
}

func (t *classifier) Receive(ctx context.Context, b *[]byte) {
        var m message
        // omited for brievety 
        tensor, err := makeTensorFromImage(m.DataURI.Content)
        output, err := session.Run(
                map[tf.Output]*tf.Tensor{
                        graph.Operation("input").Output(0): tensor,
                },
                []tf.Output{ graph.Operation("output").Output(0),
                }, nil)
        probabilities := output[0].Value().([][]float32)[0]
        label := printBestLabel(probabilities, labelsfile)
        t.c <- []byte(fmt.Sprintf("%v (%2.0f%%)", label.Label, label.Probability*100.0))
}

func (t *classifier) Send(ctx context.Context) chan []byte {
      return t.c
}
{{</ highlight >}}

#### Demo

This demo has been made with my Chromebook that has only 2 Gb or RAM. The tensorflow library is compiled without any optimization.
It works!

{{< youtube psb9r_YhwiY >}}

The code is [here](https://github.com/owulveryck/socketcam).

### In the cloud with AWS

Now that I have seen that it works on my Chromebook, I can maybe use the cloud API to recognize some faces for example.
Let's try with AWS' rekognition service.

I will use the face compare API to check whether the person in front of the webcam is me.
I will provide a sample picture of me to the cortex.

I took the sample picture at work, to make the task a little bit trickier for the engine because the environment will not match what it will see.

I won't dig into the code that can be found [here](https://github.com/owulveryck/socketcam/blob/master/processors/rekognition/main.go).

And does it work?

{{< youtube KbvRr7XXoyE >}}

Cool!

# Any real application?

This is really fun and exciting.Now I will be able to code a memory cortex to fetch a training set. Then I will play with tensorflow. And do not think that everything has already been done, this area is full of surprises to come ([This is the Moravec's paradox](https://en.wikipedia.org/wiki/Moravec%27s_paradox)).

However, on top of that, we can imagine a lot of application. Actually, this service is working out-of-the box on Android (and it will on iOS as soon as Apple supports the getUSerMedia interface).
I imagine a simple web app (no need for an APK), that would warn you when it sees someone he knows.

I also imagine a web gallery, and the webcam would watch your reaction in front of different items and then tells you which one has been your favorite.

Indeed, there may be a lot of great application for e-commerce.

You can turn your laptop into a CCTV system so it can warn you when an unknown person is in the room. We would do a preprocessing to detect humans before actually sending the info to the cloud. That would be cheaper and a lot more efficient than the crappy CV implemented in the webcam.

And finally, combined with react.js, this can be used to do magic keynotes... But I will keep that for another story.

As a conclusion, I will put this XKCD of September 2014. It is only three years old, and yet, so many things have already changed:

![XKCD](https://imgs.xkcd.com/comics/tasks.png)
---
categories:
- cfgmgmtcamp
date: 2017-02-21T15:26:22+01:00
description: "My review of the configuration managenent camp of February 2017"
draft: true
images:
- /assets/images/default-post.png
tags:
- cfgmgmtcamp
title: Configuration management is a solved problem!
---

This year, I have attended to the configuration management camp in gent thanks to [Techsys](http://www.techsys.fr).
It is the second year I attend to those conferences.
Last year, as I mentionned in [my review](/2016/02/10/configuration-management-choreography-and-self-aware-applications/index.html) the main topic was about orchestration and choreography.

This year's edition was more about the maturity of the configuration management.

Actually, the title of this blog post is the same as the keynote given by John Vincent.

# The maturity of the tools

![xkcd 1629 / tools](https://imgs.xkcd.com/comics/tools.png)

# The economy around the tools

# Containers...

## Operating systems are assholes

# Conclusion


---
author: Olivier Wulveryck
date: 2016-09-11T20:27:49+02:00
description: Which language gives the best performances to process a big/huge csv file between Python, Perl and Golang
draft: false
keywords:
- golang
- csv
title: Processing CSV files with golang, python and perl
topics:
- topic 1
type: post
---

# Introduction

System engineers are used to CSV files.
They may be considered as a good bridge between Excel and the CLI.
They are also a common way to format the output of a script so data can be exploited easily from command lines tools such as _sort_ , _awk_,_uniq_, _grep_, and so on.

The point is that when there is a significant amount of data, parsing it with shell may be painful and extremely slow.

This is a very simple and quick post about parsing CSV files in python, perl and golang.

# The use case

I consider that I have a CSV file with 4 fields per row.
The first field is a server name, and I may have 700 different servers.
The second field is a supposed disk size for a certain partition. The other fields are just present to discriminate the rows in my example

What I would like to know is the total disk size per server.

I will implement three versions of parsing, and I will look for the result of a certain server to see if the computation is ok.
Then I will compare the exeuction time of each implementation


## Generating the samples
I'm using a very simple shell loop to generate the samples. I'm generating a file with 600000 lines.

{{< gist owulveryck 4f9ddb952c5f1ef708b60a9907733969 "Generation.sh" >}}

I've randmly chosen to check the size of SERVER788 (but I will compute the size for all the servers).

I have a lot of entries for my server.
```bash
grep SERVER788 sample.csv| wc -l
3012
```
## The implementations
Here are the implementation in each language:

### The go implementation
The go implementation relies on the <code>encoding/csv</code> package.
The package has implemented a `Reader` method that can take the famous `io.Reader` as input. Therefore I will read a stream of data and not load the whole file in memory.

{{< gist owulveryck 4f9ddb952c5f1ef708b60a9907733969 "main.go" >}}

### The perl implementation
I did not find a csv implementation in perl that would be more efficient than the code below. Any pointer appreciated.
{{< gist owulveryck 4f9ddb952c5f1ef708b60a9907733969 "main.pl" >}}

### The python implementation
Python does have a _csv_ module. This module is optimized and seems to be as flexible as the implementation of go. It reads a stream as well.
{{< gist owulveryck 4f9ddb952c5f1ef708b60a9907733969 "main.py" >}}

## The results
I've run all the scripts through the _GNU time_ command. I didn't used the built-in time command because 
I wanted to check the memory footprint as well as the execution time.

Here are the results
{{< gist owulveryck 4f9ddb952c5f1ef708b60a9907733969 "result.sh" >}}

# Conclusion

All of the languages have very nice execution time: below 4 seconds to process the sample file. Go gives the best performances, but it's insignificant as long as the files do not exceed millions of records.
The memory footprint is low for eache implementation.

It's definitly worth a bit of work to implement a decent parser in a "modern language" 
instead of relying on a `while read` loop or a `for i in $(cat...` in shell.
I didn't write a shell implementation, but it would have take ages to run on my chromebook anyway.
---
author: Olivier Wulveryck
date: 2016-03-31T23:39:35+01:00
description: Some notes about Behaviour driver development, gherkin and Cucumber.
  The example describes here will test a service on an AWS's EC2 instance.
draft: false
keywords:
- EC2
- BDD
- Gherkin
- Cucumber
- Ruby
tags:
- EC2
- BDD
- Gherkin
- Cucumber
- Ruby
title: Behaviour Driven Development with Gherkin and Cucumber (an introduction)
topics:
- BDD
type: post
---
#### Opening remarks

All my previous posts were about choreography, deployment, topology, and more recently about an attempt to include _AI_ in those systems.
This post is a bit apart, because I'm facing a new challenge in my job which is to implement BDD in a _CI_ chain. Therefore, I'm using
this blog as a reminder of what I did personally. The following of the _Markov_ saga will come again later.

# Introduction

Wikipedia defines the word contract like this:

> A contract is a voluntary arrangement between two or more parties that is enforceable at law as a binding legal agreement.

If law usually describes what you can and cannot do, a contract is more likely to describe what's you are expected to do.

A law's goal is not only to give rules to follow, 
but also to maintain a stability in an ecosystem. 
In IT there are laws, that may be implicit, didactic, empiric, ... but the IT with all its laws should not 
dictate the expected behavior of the customer. But how often have you heard:

> "those computer stuffs are not for me, just get the thing done"

> "we've always done it this way"

There are laws that cannot be changed, but the contract between a customer and its provider could and should evolve.

In IT, like everywhere else where a customer/provider relationship exists, a special need is formalized via specifications.
Specifications are hard to follow, but even more they're hard to evaluate.

<center>
![Babies (xkcd)](http://imgs.xkcd.com/comics/babies.png)
</center>

The __B__ehavior __D__riven __D__evelopment is the assurance that everything have been made respectfully i
with the contract that has been established between the parties (customers and providers). 
To do things right, this contract should be established at the very beginning. 

Hence, every single item must be developed with all the _features_ of the contract in mind. And then, it should be
possible to use automation to perform the tests of behaviour, so that the developer can see if the contract is fulfilled, and if, for 
example, no regression has been introduced.

In a continuous integration chain, this is an essential piece that can be use to fully automate the process of delivery.

## Gherkin

To express the specification in a way that can be both human and computer readable, the easiest way is to use a special dedicated
language. 

Such a language is known as [DSL](https://en.wikipedia.org/wiki/Domain-specific_language) ( Domain Specific Language). 

[Gherkin](https://github.com/cucumber/cucumber/wiki/Gherkin) is a DSL that _lets you describe software's behaviour without dealing how that behaviour
is implemented_

The behaviour is a scenario detailed as a set of _features_. A feature is a human readable English 
(or another human language among 37 implemented languages) text file with a bunch of key words in it (eg: __Given__, __And__, __When__, __Then__,...).
Those words do not only help the writer of the feature to organize its idea, but they are used by the Gherkin processor to localize the
test of the feature in the code. Of course, there is no magic in it: the test must have been implemented manually.

## And here comes Cucumber

The historic Gherkin processor is called Cucumber. It's a Ruby implementation of the Gherkin DSL.
Its purpose is to read a scenario, and to localize the Ruby code that is implementing the all the tests corresponding to the scenario.
Finally it executes the code, and for each feature it simply says ok or ko.

Easy.

Nowadays there are many implementation of Gherkin parser for different languages, but in this post I will stick to the Cucumber.

# Let's play

Let's see how we can implement a basic behaviour driver development with the help of cucumber and Ruby.
The idea here is not to test a Ruby development, but instead to use ruby to validate a shell script.
That's the main reason why I stick to Ruby (instead of GO which I know better). The Go implementation 
([GoDoc](https://github.com/DATA-DOG/godog), [GoConvey](https://github.com/smartystreets/goconvey), ...) relies 
on `go test` and therefore are related to a pure GO development. 
Of course I could do a complete GO development to encapsulate my scripts, but that's not the point; for my purpose, a scripting
language is a better choice.

Ruby is a scripting language and all the tests implemented here are neither dependent on the Ruby test framework nor on [RSpec](http://rspec.info/).

I will write a script that will deploy an EC2 instance via vagrant-aws and install an Openvpn instance on it.

## The scenario

#### The customer point of view
With my role of customer, the feature I'm expecting is:

* Given the execution of the program, and waiting for it to be successful
* Then I may be able to watch netflix US from France.

The feature may be:

```gherkin
Feature: I want a program that
  will simply allows me to watch netflix US

  Scenario: I want to watch netflix
     Given I am on my chromebook
     And I have access to the shell
     When I want to watch netflix
     And I launch a program from the command line
     And it displays ready
     Then I open a navigator windows on http://www.netflix.com
     And I can watch Grey's anatomy (which is not available in france)
```

#### The architect point of view
As an architect the implementation I'm thinking of is

* start an EC2 instance (I will not create it in this post)
* register it to my DNS (with blog-test.owulveryck.info)
* install Openvpn
* configure Openvpn to make it accessible via blog-test.owulveryck.info 

#### The developer point of view
And as a developer, I'm thinking about using [vagrant-aws](https://github.com/mitchellh/vagrant-aws) to perform the tasks.
All the implementation will be based on a Vagrant file and a provisioning script.
The vagrant file will be evaluated by `vagrant up` on CLI (aka in the real world, by the end user) and 
the same vagrant file will be evaluated within my cucumber scripts.

__Therefore I can say that I am doing BDD/TDD for a configuration management and provisioning.__

## The basic _feature_

I will describe here a single feature, just for testing purpose.

## Setting up the Ruby environment 

I will use the _Ruby_ implementation of cucumber.
To install it, assuming that we have a ` gem` installed, just run this command

```shell
# gem install cucumber
```

This will load all the required dependencies.
It may also be a good idea to use `bundle` if we plan to do further development of the steps in ruby.

#### The test environment with bundler

The whole development will run with the help of bundler (and RVM).
See this [post](http://dev.owulveryck.info/2016/03/31/rvm-from-a-usb-stick-on-a-chromebook/) for more explanation on
how I set it up on my Chromebook.

```shell
> mkdir /media/removable/Lexar/tools/vpn-blog
> cd /media/removable/Lexar/tools/vpn-blog
> rvmrc --create 2.2.0@vpn-blog
> source .rvmrc
> gem install bundler -v 1.5.2 
> bundle init
Writing new Gemfile to /home/chronos/user/gherkin/Gemfile
```

#### the _Gemfile_

Let's add the cucumber, vagrant (as installed in a previous [post](http://dev.owulveryck.info/2016/03/31/rvm-from-a-usb-stick-on-a-chromebook/) ),
and vagrant-aws dependencies in the Gemfile:

```shell
> cat Gemfile
source "https://rubygems.org"

gem "vagrant", :path => "/media/removable/Lexar/tools/vagrant"
gem "vagrant-aws"
gem "bundler", "1.5.2"
gem "cucumber"
```

and then _install_ the bundle:

```shell
> bundle _1.5.2_ install
Resolving dependencies...
Using builder 3.2.2
Using gherkin 3.2.0
Using cucumber-wire 0.0.1
Using diff-lcs 1.2.4
Using multi_json 1.7.9
Using multi_test 0.1.2
Using bundler 1.11.2
Using cucumber-core 1.4.0
Using cucumber 2.3.3
...
Bundle complete! 1 Gemfile dependency, 9 gems now installed.
Use `bundle show [gemname]` to see where a bundled gem is installed.
```

And now let's run cucumber within the bundle:

```shell
> bundle _1.5.2_ exec cucumber
No such file or directory - features. You can use `cucumber --init` to get started.
```

### The skeleton of the test

First, as requested by cucumber, let's initialize a couple of files in the directory to be "cucumber compliant".
Cucumber do have a helpful _init_ function. Let's run it now:

```shell
bundle _1.5.2_ exec cucumber --init
  create   features
  create   features/step_definitions
  create   features/support
  create   features/support/env.rb
```

#### Adding the _feature_ file

In the _features/_ directory, I create a file `basic_feature.feature` which contains the YAML we wrote earlier, then I run cucumber again.

```shell
$ bundle _1.5.2_ exec cucumber
Feature: I want a program that
  will simply allows me to watch netflix US
  
  Scenario: I want to watch netflix                                   # features/basic_feature.feature:4
    Given I am on my chromebook                                       # features/basic_feature.feature:5
    And I have access to the shell                                    # features/basic_feature.feature:6
    When I want to watch netflix                                      # features/basic_feature.feature:7
    And I launch a program on the command line                        # features/basic_feature.feature:8
    And it displays ready                                             # features/basic_feature.feature:9
    Then I open a navigator windows on http://www.netflix.com         # features/basic_feature.feature:10
    And I can watch Grey's anatomy (which is not available in france) # features/basic_feature.feature:11
                                
1 scenario (1 undefined)
7 steps (7 undefined)
0m0.054s

You can implement step definitions for undefined steps with these snippets:

Given(/^I am on my chromebook$/) do
  pending # Write code here that turns the phrase above into concrete actions
end
...
```

We notice that the feature has been read and understood correctly by cucumber.
ON top of that Cucumber gives the skeleton of a ruby implementation for the tests.

I will copy all the ruby code in its own file:

```ruby
# cat > features/step_definitions/tests.rb
Given(/^I am on my chromebook$/) do
  pending # Write code here that turns the phrase above into concrete actions
  end
...
```

And run cucumber once more:

```shell
Feature: I want a program that
  will simply allows me to watch netflix US

  Scenario: I want to watch netflix                                   # features/basic_feature.feature:4
      Given I am on my chromebook                                       # features/step_definitions/tests.rb:1
        TODO (Cucumber::Pending)
        ./features/step_definitions/tests.rb:2:in `/^I am on my chromebook$/'
        features/basic_feature.feature:5:in `Given I am on my chromebook'
      And I have access to the shell                                    # features/step_definitions/tests.rb:5
      When I want to watch netflix                                      # features/step_definitions/tests.rb:9
      And I launch gonetflix.sh                                         # features/step_definitions/tests.rb:13
      And it displays ready                                             # features/step_definitions/tests.rb:17
      Then I open a navigator windows on http://www.netflix.com         # features/step_definitions/tests.rb:21
      And I can watch Grey's anatomy (which is not available in france) # features/step_definitions/tests.rb:25
      
1 scenario (1 pending)
7 steps (6 skipped, 1 pending)
0m0.041s`
```

Cool, the framework is ok. Now let's actually implement the scenario and the tests

#### Implementation of the "Given" keywords

There is not much to say about the Given keyword. I can test that I am really on my Chromebook but that does not make any sense.
I will skip this test by not implementing anything in the function.

#### Implementation of the "When" keyword

The actual execution of the "When" is the execution of the Vagrant file.
It will start the EC2 instance and provision the VPN
I also need to mount the VPN locally afterwards

```ruby
#!/usr/bin/env ruby
require "vagrant"

# Starting the EC2 instance (running the vagrantfile)
env = Vagrant::Environment.new
env.cli("up")
# Starting OpenVPN locally
`sudo openvpn --mktun --dev tun0 && sudo openvpn --config ~/Downloads/client.ovpn --dev tun0`
```

#### (trying to) Implement the netflix test with selenium

To test the access, instead of faking my browser with curl, I will use the _selenium_ tool.
So I add it to my _Gemfile_ and `bundle update` it (informations comes from [this starterkit](https://github.com/jonathanchrisp/selenium-cucumber-ruby-kickstarter)):

```shell
$ echo 'gem "selenium-cucumber"' >> Gemfile
$ echo 'gem "selenium-webdriver"' >> Gemfile
$ echo 'gem "require_all"' >> Gemfile
$ bundle _1.5.2_ update 
```

Then I need to create a special file in the `support` subdirectory to define a bunch of objects:

```ruby
# cat features/support/env.rb
require 'selenium-webdriver'
require 'cucumber'

require 'require_all'

require_all 'lib'

Before do |scenario|
    @browser = Browser.new(ENV['DRIVER'])
    @browser.delete_cookies
end

After do |scenario|
    @browser.driver.quit
end
```

I'm also adding the files from the starterkit in the ` lib` subdirectory.

As I am developing on my Chromebook, I also need the [chromedriver](https://sites.google.com/a/chromium.org/chromedriver/)

__Too bad__ chromedriver relies on the libX11 that cannot be installed on my Chromebook / __end of show for selenium__
on the Chromebook...  for now

_Note_ I will continue with the development, but be aware that I won't be able to test it until I am on a true linux box with
the chromedriver installed

```ruby
Then(/^I open a navigator windows on (.*?)$/) do |arg1|
  @browser.open_page("http://www.netflix.com")
end

Then(/^I can watch Grey's anatomy \(which is not available in france\)$/) do
  @browser.open_page("http://www.netflix.com/idtogreysanatomy")
end
```

### The actual implementation of the scenario

What I need to do is to implement the scenario. Not the test scenario, the real one;
the one that will actually allows me to launch my ec2 instance, configure and start Openvpn.

As I said before, I will use vagrant-aws to do so.

__Note__ vagrant was depending on _bsdtar_, and I've had to install it manually from source:

(`tar xzvf libarchive-3.1.2.tar.gz && ... && ./configure --prefix=/usr/local && make install clean`)

#### Installing vagrant-aws plugin

The vagrant-aws plugin has been installed by the bundler because I've indicated it as a dependency in the Gemfile.
But, I will have to have it as a requirement in the Vagrantfile because I'm not using the "official vagrant" and that
I am running in a bundler environment:

> Vagrant's built-in bundler management mechanism is disabled because
> Vagrant is running in an external bundler environment. In these
> cases, plugin management does not work with Vagrant. To install
> plugins, use your own Gemfile. To load plugins, either put the
> plugins in the `plugins` group in your Gemfile or manually require
> them in a Vagrantfile.

#### Installing the base box 

The documentation says that the quickest way to get started is to install the dummy box. 
That's what I did:

```shell
$ bundle _1.5.2_ exec vagrant box add dummy https://github.com/mitchellh/vagrant-aws/raw/master/dummy.box
...
==> box: Successfully added box 'dummy' (v0) for 'aws'!
```

#### The Vagrantfile

The initial Vagrantfile looks like this:

```ruby
require "vagrant-aws"
Vagrant.configure("2") do |config|
  config.vm.box = "dummy"

  config.vm.provider :aws do |aws, override|
    aws.access_key_id = "YOUR KEY"
    aws.secret_access_key = "YOUR SECRET KEY"
    aws.session_token = "SESSION TOKEN"
    aws.keypair_name = "KEYPAIR NAME"

    aws.ami = "ami-7747d01e"

    override.ssh.username = "ubuntu"
    override.ssh.private_key_path = "PATH TO YOUR PRIVATE KEY"
  end
end
```

So all the rest in the basic implementation of the vagrant file and the provisioning.sh for the Openvpn configuration.
but that goes far behind the topic of this post which was to introduce myself to BDD and TDD.

# Conclusion

I've learned a lot about the ruby and cucumber environment in this post.
Too bad I couldn't end with a fully running example because of my Chromebook.

Anyway the expected results were for me to:

* learn about BDD
* learn about cucumber
* learn about Ruby
* learn about vagrant

I can say that I've reach my goals anyway. I will try to finish the implementation on a true Linux box locally, or on my 
Macbook if I have time to do so.
---
categories:
- category
date: 2017-03-13T15:05:35+01:00
description: ""
draft: true
images:
- /assets/images/default-post.png
tags:
- tag1
- tag2
title: distributed linda
---

[D3JS tutorial](http://cloud-krbabu.blogspot.fr/2014/02/d3js-and-dining-philosophers-problem.html)
---
author: Olivier Wulveryck
date: 2016-04-25T10:06:01+02:00
description: description
draft: true
keywords:
- key
- words
tags:
- one
- two
title: dotscale2016
topics:
- topic 1
type: post
---

I've had the opportunity to attend the dotScale conference in Paris.
---
categories:
- conference
date: 2017-04-24T20:51:22+02:00
draft: false
images:
- https://pbs.twimg.com/profile_images/659732730784129025/TlfVUxEl.png
---
---
categories:
- dev
date: 2017-03-11T09:15:17+01:00
description: "Using an exponential backoff algorithm to fill a dynamodb from a CSV file"
draft: false
images:
- /assets/images/default-post.png
tags:
- golang
- reflection
- dynamodb
- exponentialBackoff
title: 350000 rows, 133 cols... From a huge CSV to DynamoDB (without breaking piggy-bank).
---

In this post I will explain how to:

* Parse a CSV file and extract only certain columns
* Create a table in DynamoDB
* Insert all the data with an adaptive algorithm in order to use the provisioned capacity
* Reduce the capacity once the insertion is done.

# Exploring the problem: AWS Billing

In a [previous post](/2017/01/13/a-foot-in-nosql-and-a-toe-in-big-data/) I explained how I was using dynamodb to store a lot of data about aws billing.

On top of the API that deals with products and offers, AWS can provide a "billing report". Those reports are delivered to am Amazon S3 bucket in CSV format at least once a day.

The rows of the CSV are organized in _topics_ as described [here](http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-reports.html#Topics).

Each line of the CSV represents an item that is billed. But every resource is made of several billable items. For example on EC2, you pay the "compute", the bandwidth, the volume etc...

I would like to use and understand this file to optimize the costs. A kind of BI.

AWS says that you can import your file in Excel (or alike)... That could be a solution but:

![Excel XKCD](https://imgs.xkcd.com/comics/algorithms.png)

On top of that with a lot of resources the file is fat (more thant a 100000 lines several times a day for my client).
I have decided to use dynamodb to store all the information so it will be easy to perform an extract and generate a dashboard.
In this post, I will expose some go techniques I have used to achive that.

# Step 1: Parsing the CSV

As I explained, the CSV file is made of more than a hundreds cols. The columns are identified in the first row of the CSV.
I will store each row in a go struct.

To parse it easily, I an using custom fields `csv` in the struct. The field value corresponds to the header name for the seek value.

For example:

{{< highlight go >}}
type test struct {
    ID   string `csv:"myid"`
    Name string `csv:"prenom"`
    Last string `csv:"nom"`
    Test string `csv:"nonexitent"`
}
{{</ highlight >}}

Then, I am reading the first row of the CSV file and then ranging the field names of the struct to fill a map with the field key as key and the col number as value. I set '-1' if the field is not found:

{{< highlight go >}}
var headers = make(map[string]int, et.NumField())
for i := 0; i < et.NumField(); i++ {
        headers[et.Field(i).Name] = func(element string, array []string) int {
                for k, v := range array {
                        if v == element {
                                return k
                        }
                }
                return -1
        }(et.Field(i).Tag.Get("csv"), header)
}
{{</ highlight >}}

Then I can parse the CSV file and fill a channel with one object by row... 
See the full example [here](https://gist.github.com/owulveryck/0fc68c90fa4875647b54f62e2066707d)

# Step 2: Creating the table in DynamoDB

This step is "easy". I will create a table with one index and a sort key.
For the example the index is a string named `Key`. The sort key is also a string named `SortKey`.

{{< highlight go >}}
AttributeDefinitions: []*dynamodb.AttributeDefinition{
        {
                AttributeName: aws.String("Key"),
                AttributeType: aws.String("S"),
        },
        {
                AttributeName: aws.String("SortKey"),
                AttributeType: aws.String("S"),
        },
},
KeySchema: []*dynamodb.KeySchemaElement{
        {
                AttributeName: aws.String("Key"),
                KeyType:       aws.String("HASH"),
        },
        {
                AttributeName: aws.String("SortKey"),
                KeyType:       aws.String("RANGE"),
        },
},
{{</ highlight >}}

I will set an initial provisioning of 600. This would cost a lot of money but I will reduce it later to spare. The high provisioning rate is needed otherwise it would take me hours to integrate the CSV.

{{< highlight go >}}
ProvisionedThroughput: &dynamodb.ProvisionedThroughput{
        ReadCapacityUnits:  aws.Int64(5),
        WriteCapacityUnits: aws.Int64(300),
},
{{</ highlight >}}

The code for creating the table is [here](https://gist.github.com/owulveryck/6663983b41c669617704558a030a3392#file-dynamodbcreatetable-go)

# Step 3: Inserting the data

The structure is read through the channel I have created previously.
The object is encoded to a dynamodb compatible one thanks the `marshal` function of this helper library `github.com/aws/aws-sdk-go/service/dynamodb/dynamodbattribute`

To make the structure ID match the `Key` attribute of the table, I am using the `dynamodbav` fields.
{{< highlight go >}}
type test struct {
    ID   string `csv:"myid" dynamodbav:"Key"`
    Name string `csv:"prenom" dynamodbav:"SortKey"`
    Last string `csv:"nom" dynamodbav:"Last,omitempty"`
    Test string `csv:"nonexitent"`
}
...
for v := range c {
    item, err := dynamodbattribute.MarshalMap(v)
    params := &dynamodb.PutItemInput{
        Item:      item,
        TableName: aws.String(tableName),
    }
    svc.PutItem(params)
}
{{</ highlight >}}

## Going concurrent

I will add a touch of concurrency. I will use a maximum of 20 goroutines simultaneously to send items to the dynamodb. 
This is an empiric decision.

I am using a "guard" channel. This channel has a buffer of 20. The buffed is filled with am empty struct whenever an item is received in the main communication channel.
I am then launching a gorouting that will insert the event into dynamodb and consume one event from the guard channel when done.

The guard channel is blocking when it is full. Therefore I am sure that 20 goroutines will run at maximum:

{{< highlight go >}}
guard := make(chan struct{}, 20)
for v := range c {
    guard <- struct{}{}
    go func(v *test) {
        item, err := dynamodbattribute.MarshalMap(v)
        params := &dynamodb.PutItemInput{
            Item:      item,
            TableName: aws.String(tableName),
        }
        svc.PutItem(params)
        <-guard
    }
}
{{</ highlight >}}

## Using a backoff algorithm

The problem with this implementation is that it can overload the capacity.
Therefore the rejected event must be resent. Of course I can simply check for the error `dynamodb.ErrCodeProvisionedThroughputExceededException` an immediately resend the failed event.

But this may lead to dramatic performances.
The AWS documentation point an Exponential Backoff algorithm as an advice to optimize the writing: [Cf AWS documentation)](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.RetryAndBackoff)

Wikipedia gives a good [explanation of the exponential backoff](https://en.wikipedia.org/wiki/Exponential_backoff) but to make it simple the idea is to decrease the ration of insertion of the DB in order to get a good performance.

I am using a go implenenation found on [github](http://github.com/cenkalti/backoff) made by [Cenkalti](https://github.com/cenkalti).

I return the error only in case of `dynamodb.ErrCodeProvisionedThroughputExceededException` by now:

{{< highlight go >}}
backoff.Retry(func() error {
    // Now put the item, discarding the result
    _ , err = svcDB.PutItem(params)
    if err != nil {
        if err.(awserr.Error).Code() == dynamodb.ErrCodeProvisionedThroughputExceededException {
            return err
        }
        // TODO: Special case...
        log.Printf("Error inserting %v (%v)", v, err)
    }
    // Do the insert here
    return nil
}, backoff.NewExponentialBackOff())
{{</ highlight >}}

# Step 4: Updating the table and reducing the write capacity

Once the insert is done, to avoid a huge bill, I am reducing the Provisioned capacity of the table.
This is done with an `update` request:

{{< highlight go >}}
params := &dynamodb.UpdateTableInput{
    TableName: aws.String(tableName), // Required
    ProvisionedThroughput: &dynamodb.ProvisionedThroughput{
        ReadCapacityUnits:  aws.Int64(10), // Required
        WriteCapacityUnits: aws.Int64(1),  // Required
    },
}
resp, err := svc.UpdateTable(params)
{{</ highlight >}}

# Conclusion: it works

It took me half an hour to process and insert 350000 lines (with 133 cols each) into the dynamodb from my laptop.

I can see that the adaptative algorithm works on the graphs:

![It works](/assets/images/dynamodb/dynamodb_write_capacity.png)

Now I can analyse the data to find a proper way to optimize the aws bill for my client.

The full example is on [gist](https://gist.github.com/owulveryck/6663983b41c669617704558a030a3392)
---
author: author
date: 2016-08-20T21:45:37+02:00
description: description
draft: true
keywords:
- key
- words
tags:
- one
- two
title: ethereum
topics:
- topic 1
type: post
---


The greeter: https://www.ethereum.org/greeter
# Installation

From sources:

```shell
cd $GOPATH/src/github.com/ethereum
git clone https://github.com/ethereum/go-ethereum.git
```

## installing the solidity compiler

```
sudo add-apt-repository ppa:ethereum/ethereum
sudo apt-get update
sudo apt-get install solc
which solc
```

# Triggering the console and entering the network

## Triggering the console on the test network
`$GOPATH/src/github.com/ethereum/go-ethereum/build/bin/geth --testnet --fast --cache=512 console`



```
> eth.getCompilers()
> I0820 21:59:11.083752 common/compiler/solidity.go:114] solc, the solidity compiler commandline interface
> Version: 0.3.6-0/None-Linux/g++
>
> path: /usr/bin/solc
> ["Solidity"]
```

The client is downloading the headers, I should wait until 512 Mb is filled:

```
$ du -sh ~/.ethereum 
234M    /home/olivier/.ethereum
```
---
categories:
- coordination language
date: 2017-03-13T20:54:27+01:00
description: "Third article about writing a distributed linda interpreter"
draft: false
images:
- /assets/images/the_stars_look_different.jpg
tags:
- zygomys
- Linda
- golang
- etcd
title: Linda's evalc, a (tuple)space oddity
---

For a change, I will start with a good soundtrack

<iframe src="https://embed.spotify.com/?uri=spotify:track:72Z17vmmeQKAg8bptWvpVG&theme=white" width="220" height="80" frameborder="0" allowtransparency="true"></iframe>

([youtube version](https://www.youtube.com/watch?v=iYYRH4apXDo) for those who are spotify-less)

----
This is my third article about the distributed coordination language Linda.

The final target of the work is to use this coordination mechanism to deploy and maintain applications based on the description of their topology (using, for example, TOSCA as a DSL).

Last time, I introduced a lisp based language (zygomys) as an embedded programing mechanism to describe the business logic.

Today I will explain how I have implemented a new _action_ in the linda language to achieve a new step: to distribute the work among different nodes.

My test scenario remains the "dining of the philosophers".

# Introducing _evalc_

Linda is a coordination language, but the language which is more than 30 years old, has not been designed with the idea of running on multiple hosts.
The basic primitives of the language do not allow remote execution.

What I need is a sort of _eval_ function that would trigger the execution of the evaluation on another host instead of another goroutine.

I do not care about catching the result of the execution as it will be posted to the tuple space.
Indeed, if more coordination between the actors of this RPC is needed, it can be encoded using the in/out mechanism of linda.

Therefore, I have decided to introduce a new primitive called _evalc_ (for eval compute... Yeah I know, I have imagination)

# Implementing _evalc_

The evalc will not trigger a function on a new host.
Instead, each participating host will run a sort of agent (actually a clone of the zygo interpreter) that will watch a certain type of event (tainted with the evalc) and will then execute a function.

The tuple space acts like a communication channel and this implementation is like a kind of [CSP](https://en.wikipedia.org/wiki/Communicating_sequential_processes) which I like a lot.

The _evalc_ will work exactly as its equivalent _eval_. Therefore the function declaration in go will look like this:

{{< highlight go >}}
func (l *Linda) EvalC(env *zygo.Glisp, name string, args []zygo.Sexp) (zygo.Sexp, error) {
    ...
    return zygo.SexpNull, nil
}
{{</ highlight >}}

## First attempt

At first I thought I could simply gob/encode the `args` which contains the `SexpFunction`, post it in the tuple space under a prefixed key. Then the worker would read an execute it in a newly created `glisp` env.

That didn't work mainly because the `SexpFunction` does not have any exported fields, therefore I cannot easily encode/decode it.

I though then that I could encode the `datastack` and post it in the tuple space. I could then decode it in the worker.

I asked for some advice to the author of zygomys [Jason E. Aten](https://www.linkedin.com/in/jason-e-aten-ph-d-45a31318) (aka [glycerine](https://github.com/glycerine))

Here is what he told me (thank you Jason btw):

_Evaluating an arbitrary expression remotely will be challenging because an expression can refer to any variable in the environment, and so would theoretically require a copying of the whole environment--the heap as well as the datastack._ 

And of course he is right!
So I will keep the idea of encoding the whole environment and send it to the workers for a later implementation.
It would need to change the zygomys implementation a lot so export and import both stack. That is too much for now.

## Second attempt

What I did as a temporary solution is a lot simpler and not elegant at all: I have posted the function and the variables in the tuple space and then I am evaluating it in a newly created env.

The main problem is that I cannot access to variables and user function defined outside of the scope of the function. But that will do the trick for now.

Regarding the problem of the philosopher, I had to change the definition of `phil` within my lisp code so it do not call `(eat)` and `(think)` functions anymore.

Here is what is posted in the tuple space when the evalc function is called:
{{< highlight lisp >}}
(defn phil [i num] ((begin (begin (printf "%v is thinking\n" i) (sleep 10000) (printf "/%v is thinking\n" i)) (in "room ticket") (printf "%v is in the room\n" i) (in "chopstick" i) (printf "%v took the %v's chopstick\n" i i) (in "chopstick" (mod (+ i 1) num)) (printf "%v took the %v's chopstick\n" i (mod (+ i 1) num)) (begin (printf "%v is eating\n" i) (sleep 10000) (printf "/%v is eating\n" i)) (printf "%v released the %v's chopstick\n" i i) (out "chopstick" i) (printf "%v released the %v's chopstick\n" i (mod (+ i 1) num)) (out "chopstick" (mod (+ i 1) num)) (printf "%v left the room\n" i) (out "room ticket") (phil i num)))) 0 5
{{</ highlight >}}

In the worker process, I am creating a new environment, loading the function (the `defn` part), and constructing an expression to be evaluated by the env. 
This is what the environment evalutates:

{{< highlight lisp >}}
(defn phil [i num] (
   (begin
    (begin
     (printf "%v is thinking\n" i)
     (sleep 10000)
     (printf "/%v is thinking\n" i))
    (in "room ticket")
    (printf "%v is in the room\n" i)
    (in "chopstick" i)
    (printf "%v took the %v's chopstick\n" i i)
    (in "chopstick" (mod (+ i 1) num))
    (printf "%v took the %v's chopstick\n" i (mod (+ i 1) num))
    (begin
     (printf "%v is eating\n" i)
     (sleep 10000)
     (printf "/%v is eating\n" i))
    (printf "%v released the %v's chopstick\n" i i)
    (out "chopstick" i)
    (printf "%v released the %v's chopstick\n" i (mod (+ i 1) num))
    (out "chopstick" (mod (+ i 1) num))
    (printf "%v left the room\n" i)
    (out "room ticket")
    (phil i num))))
(phil 0 5)
{{</ highlight >}}

# Runtime

## Running it locally: one etcd and several workers

To run it locally I need: 

* a local instance of `etcd` 
* 5 workers. 

Each worker will watch for a new event in the tuple space.
Then I can trigger the execution of the logic with a sixth worker that will read the lisp code, and execute it.

Here is a screenshot of the execution
![Runtime screenshot](https://raw.githubusercontent.com/ditrit/go-linda/master/doc/v0.3.png)

# Conclusion

Jason E. Aten also told me about _sigils_ as a way to discriminate the local variables from the variables present in the tuple space.
I haven't worked on it yet, but I think that I will use those _sigils_ to enhance my linda implementation. It can be usefull for the matching of templates and formals.

By now, I have something that is able to run a basic theorical coordination problem.

Now I think that I will go back to the application management task and see how I can encode the TOSCA workflow so it can be used by this mechanism.

Meanwhile, I will try to test this setup on a worldwide cluster (maybe based on CoreOS).

----
Credit:

The illustration has been found [here](https://www.flickr.com/photos/joebehr/23704122254)
+++
date = "2015-10-22T15:59:07+01:00"
draft = false
title = "Welcome"

+++

You are in a place where a new technical blog will stand soon.
---
categories:
date: 2017-09-02T13:28:36+02:00
description: "This article explains how to turn a golang utility into a webservice using gRPC (and protobuf). I take the example of Hashicorp tools because they are often used as a leverage for the DevOps transformation. Often, the Ops use the tools for themselves, but when comes the time to provide a service around them, they are usually scared to open the engine. They prefer to make a factory around the service, which is often less reliable than a little piece of code fully tested."
draft: false
images:
- /assets/images/terraformcli.png
tags:
title: From command line tools to microservices - The example of Hashicorp tools (terraform) and gRPC
---

This post is a little different from the last ones. As usual, the introduction tries to be open, but it quickly goes deeper into a go implementation.
Some explanations may be tricky from time to times and therefore not very clear. As usual, do not hesitate to send me any comment via this blog or via twitter [@owulveryck](https://twitter.com/owulveryck).

**TL;DR**: This is a step-by-step example that turns a golang cli utility into a webservice powered by gRPC and protobuf. The code can be found [here](https://github.com/owulveryck/cli-grpc-example).

# About the cli utilities

I come from the sysadmin world... Precisely the Unix world (I have been a BSD user for years). Therefore, I have learned to use and love "_the cli utilities_". Cli utilities are all those tools that make Unix sexy and "user-friendly". 

<center>
Because, yes, Unix **is user-friendly** (it's just picky about its friends[^1]).
</center>

[^1]: This sentence is not from me. I read it once, somewhere, on the Internet. I cannot find anybody to give the credit to.

From a user perspective, cli tools remains a must nowadays because:

* there are usually developed in the pure Unix philosophy: simple enough to use for what they were made for;
* they can be easily wrapped into scripts. Therefore, it is easy to automate cli actions.

The point with cli application is that they are mainly developed for an end-user that we call "an operator". As Unix is a multi-user operating system, several operators can use the same tool, but they have to be logged onto the same host.

In case of a remote execution, it's possible to execute the cli via `ssh`, but dealing with automation, network interruption and resuming starts to be tricky.
For remote and concurrent execution web-services are more suitable.

Let's see if turning a cli tool into a webservice without re-coding the whole logic is easy in go?

## Hashicorp's cli

For the purpose of this post, and because I am using Hashicorp tools at work, I will take [@mitchellh](https://twitter.com/mitchellh)'s framework for developing command line utilities.
This package is used in all of the Hashicorp tools and is called................ "[cli](https://github.com/mitchellh/cli)"! 

This library provides a [`Command`](https://godoc.org/github.com/mitchellh/cli#Command) type that represents any action that the cli will execute.
`Command` is a go `interface` composed of three methods:

* `Help()` that returns a string describing how to use the command;
* `Run(args []string)` that takes an array of string as arguments (all cli parameters of the command) and returns an integer (the exit code);
* `Synopsis()` that returns a string describing what the command is about.
 
 _Note_: I assume that you know what an interface is (especially in go). If you don't, just google, or even better, buy the book [The Go Programming Language](https://www.amazon.com/Programming-Language-Addison-Wesley-Professional-Computing/dp/0134190440) and read the _chapter 7_ :).

The main object that holds the business logic of the cli package is an implementation of [`Cli`](https://godoc.org/github.com/mitchellh/cli#CLI). 
One of the elements of the Cli structure is `Commands` which is a `map` that takes the name of the action as key. The name passed is a string and is the one that will be used on the command line. The value of the `map` is a function that returns a `Command`. This function is named [`CommandFactory`](https://godoc.org/github.com/mitchellh/cli#CommandFactory). According to the documentation, the factory is needed because _we may need to setup some state on the struct that implements the command itself_. Good idea!

## Example

First, let's create a very simple tool using the "cli" package.
The tool will have two "commands":

* hello: will display _hello args...._  on `stdout` 
* goodbye: will display _goodbye args..._ on `stderr`

{{< highlight go >}}
func main() {
      c := cli.NewCLI("server", "1.0.0")
      c.Args = os.Args[1:]
      c.Commands = map[string]cli.CommandFactory{
            "hello": func() (cli.Command, error) {
                      return &HelloCommand{}, nil
            },
            "goodbye": func() (cli.Command, error) {
                      return &GoodbyeCommand{}, nil
            },
      }
      exitStatus, err := c.Run()
      ... 
}
{{</ highlight >}}
As seen before, the first object created is a `Cli`. Then the `Commands` field is filled with the two commands "hello" and "goodbye" as keys, and an anonymous function that simply returns two structures that will implement the `Command` interface.

Now, let's create the `HelloCommand` structure that will fulfill the [`cli.Command`](https://godoc.org/github.com/mitchellh/cli#Command) interface:

{{< highlight go >}}
type HelloCommand struct{}

func (t *HelloCommand) Help() string {
      return "hello [arg0] [arg1] ... says hello to everyone"
}

func (t *HelloCommand) Run(args []string) int {
      fmt.Println("hello", args)
      return 0
}

func (t *HelloCommand) Synopsis() string {
      return "A sample command that says hello on stdout"
}
{{</ highlight >}}

The `GoodbyeCommand` is similar, and I omit it for brevity.

After a simple `go build`, here is the behavior of our new cli tool:
{{< highlight shell >}}
~ ./server help
Usage: server [--version] [--help] <command> [<args>]

Available commands are:
    goodbye    synopsis...
    hello      A sample command that says hello on stdout

~ ./server hello -help
hello [arg0] [arg1] ... says hello to everyone

~ ./server/server hello a b c
hello [a b c]
{{</ highlight >}}

So far, so good!
Now, let's see if we can turn this into a webservice.

# Micro-services

<center>_The biggest issue in changing a monolith into microservices lies in changing the communication pattern. - Martin Fowler_[^2]</center>

[^2]: from [Martin Fowler's Microservices definition](https://martinfowler.com/articles/microservices.html#SmartEndpointsAndDumbPipes).

There is, according to me, two options to consider turning our application into a webservice:

* a RESTish communication and interface;
* an RPC based communication.

SOAP is not an option anymore because it does not provide any advantage over the REST and RPC methods.

## Rest? 

I've always been a big fan of the REST "protocol". It is easy to understand and to write. On top of that, it is verbose and allows a good description of "business objects".
But, its verbosity, that is a strength, quickly become a weakness when applied to machine-to-machine communication.
The "contract" between the client and the server have to be documented manually (via something like swagger for example). And, as you only transfer objects and states, the server must handle the request, understand it, and apply it to any business logic before returning a result.
Don't get me wrong, REST remains a very good thing. But it is very good when you think about it from the beginning of your conception (and with a user experience in mind).

Indeed, it may not be a good choice for easily turning a cli into a webservice.

## RPC!

RPC, on the other hand, may be a good fit because there would be a very little modification of the code.
Actually, the principle would be to:

1. trigger a network listener
2. receive a _procedure call with arguments_,
3. execute the function
4. send back the result

The function that holds the business logic does not need any change at all.

The drawbacks of RPCs are:

* the development language need a library that supports RPC,
* the client and the server must use the same communication protocol.

Those drawbacks have been addressed by Google. They gave to the community a polyglot RPC implementation called gRPC. 

Let me quote this from the chapter "[The Production Environment at Google, from the Viewpoint of an SRE](https://landing.google.com/sre/book/chapters/production-environment.html#our-software-infrastructure-XQs4iw)" of the SRE book:

> _All of Google's services communicate using a Remote Procedure Call (RPC) infrastructure named Stubby; an open source version, gRPC, is available. Often, an RPC call is made even when a call to a subroutine in the local program needs to be performed. This makes it easier to refactor the call into a different server if more modularity is needed, or when a server's codebase grows. GSLB can load balance RPCs in the same way it load balances externally visible services._

Sounds cool! Let's dig into gRPC!

### gRPC

We will now implement a gRPC server that will trigger the `cli.Commands`.

It will receive "orders", and depending on the expected call, it will: 

* Implements a `HelloCommand` and trigger its `Run()` function;
* Implements a `GoodbyeCommand` and trigger its `Run()` function

We will also implement a gRPC client.

For the server and the client to communicate, they have to share the same protocol and understand each other with a contract.
_Protocol Buffers (a.k.a., protobuf) are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data_ 
Even if it's not mandatory, gRPC is usually used with the _Protocol Buffer_. 

So, first, let's implement the _contract_ with/in _protobuf_!

### The protobuf contract

The protocol is described in a simple text file and a specific DSL. Then there is a compiler that serializess the description and turns it into a contract that can be understood by the targeted language.

Here is a simple definition that matches our need:

{{< highlight protobuf >}}
syntax = "proto3";

package myservice;

service MyService {
    rpc Hello (Arg) returns (Output) {}
    rpc Goodbye (Arg) returns (Output) {}
}

message Arg {
    repeated string args = 1;
}

message Output {
    int32 retcode = 1;
}
{{</ highlight >}}

Here is the English description of the contract:

----
Let's take a service called _MyService_. This service provides to actions (commands) remotely:

* _Hello_ 
* _Goodbye_

Both takes as argument an object called _Arg_ that contains an infinite number of _string_ (this array is stored in a field called _args_).

Both actions return an object called _Output_ that returns an integer.

----

The specification is clear enough to code a server and a client. But the string implementation may differ from a language to another.
You may now understand why we need to "compile" the file.
Let's generate a definition suitable for the go language:

`protoc --go_out=plugins=grpc:. myservice/myservice.proto`

_Note_ the definition file has been placed into a subdirectory `myservice`

This command generates a `myservice/myservice.pb.go` file. This file is part of the `myservice` package, **as specified in the myservice.proto**.

The package myservice holds the "contract" translated in `go`. It is full of interfaces and holds helpers function to easily create a server and/or a client.
Let's see how.

### The implementation of the "contract" into the server

Let's go back to the roots and read the doc of gRPC. In the [gRPC basics -  go](https://grpc.io/docs/tutorials/basic/go.html) tutorial is written:

_To build and start a server, we:_

1. _Specify the port we want to use to listen for client requests..._
2. _Create an instance of the gRPC server using grpc.NewServer()._
3. *__Register our service implementation with the gRPC server.__*
4. _Call Serve() on the server with our port details to do a blocking wait until the process is killed or Stop() is called._

Let's decompose the third step.

#### "service implementation"
The `myservice/myservice.pb.go` file has defined an interface for our service.

{{< highlight go >}}
type MyServiceServer interface {
      // Sends a greeting
      Hello(context.Context, *Arg) (*Output, error)
      Goodbye(context.Context, *Arg) (*Output, error)
}
{{</ highlight >}}

To create a "service implementation" in our "cli" utility, we need to create any structure that implements the Hello(...) and Goodbye(...) methods.
Let's call our structure `grpcCommands`:

{{< highlight go >}}
package main

...
import "myservice"
...

type grpcCommands struct {}

func (g *grpcCommands) Hello(ctx context.Context, in *myservice.Arg) (*myservice.Output, error) {
    return &myservice.Output{int32(0)}, err
}
func (g *grpcCommands) Goodbye(ctx context.Context, in *myservice.Arg) (*myservice.Output, error) {
    return &myservice.Output{int32(0)}, err
}
{{</ highlight >}}

_Note_: *myservice.Arg is a structure that holds an array of string named Args. It corresponds to the `proto` definition exposed before.

#### "service registration"

As written in the doc, we need to register the implementation.
In the generated file `myservice.pb.go`, there is a `RegisterMyServiceServer` function.
This function is simply an autogenerated wrapper around the [`RegisterService`](https://godoc.org/google.golang.org/grpc#Server.RegisterService) method of the gRPC [`Server`](https://godoc.org/google.golang.org/grpc#Server) type.

This method takes two arguments: 

* An instance of the gRPC server
* the implementation of the contract.

The 4 steps of the documentation can be implemented like this:

{{< highlight go >}}
listener, _ := net.Listen("tcp", "127.0.0.1:1234")
grpcServer := grpc.NewServer()
myservice.RegisterMyServiceServer(grpcServer, &grpcCommands{})
grpcServer.Serve(listener)
{{</ highlight >}}

So far so good... The code compiles, but does not perform any action and always return 0.

#### Actually calling the `Run()` method

Now, let's use the `grpcCommands` structure as a bridge between the `cli.Command` and the grpc service.

What we will do is to embed the `c.Commands` object inside the structure and trigger the appropriate objects' `Run()` method from the corresponding gRPC procedures.

So first, let's embed the `c.Commands` object.

{{< highlight go >}}
type grpcCommands struct {
      commands map[string]cli.CommandFactory
}
{{</ highlight >}}

Then change the `Hello` and `Goodbye` methods of `grpcCommands` so they trigger respectively:

* `HelloCommand.Run(args)`
* `GoodbyeCommand.Run(args)`

with `args` being the array of string passed via the `in` argument of the protobuf.

as defined in `myservice.Arg.Args` (the protobuf compiler has transcribed the `repeated string args` argument into a filed `Args []string` of the type `Arg`. 

{{< highlight go >}}
func (g *grpcCommands) Hello(ctx context.Context, in *myservice.Arg) (*myservice.Output, error) {
      runner, err := g.commands["hello"]()
      if err != nil {
            return int32(0), err
      }
      ret = int32(runner.Run(in.Args))
      return &myservice.Output{int32(ret)}, err
}
func (g *grpcCommands) Goodbye(ctx context.Context, in *myservice.Arg) (*myservice.Output, error) {
      runner, err := g.commands["goodbye"]()
      if err != nil {
            return int32(0), err
      }
      ret = int32(runner.Run(in.Args))
      return &myservice.Output{int32(ret)}, err
}
{{</ highlight >}}

Let's factorize a bit and create a wrapper (that will be useful in the next section):

{{< highlight go >}}
func wrapper(cf cli.CommandFactory, args []string) (int32, error) {
      runner, err := cf()
      if err != nil {
            return int32(0), err
      }
      return int32(runner.Run(in.Args)), nil
}

func (g *grpcCommands) Hello(ctx context.Context, in *myservice.Arg) (*myservice.Output, error) {
      ret, err := wrapper(g.commands["hello"])
      return &myservice.Output{int32(ret)}, err
}
func (g *grpcCommands) Goodbye(ctx context.Context, in *myservice.Arg) (*myservice.Output, error) {
      ret, err := wrapper(g.commands["goodbye"])
      return &myservice.Output{int32(ret)}, err
}
{{</ highlight >}}

Now we have everything needed to turn our cli into a gRPC service. With a bit of plumbing, the code compiles and the service runs.
The full implementation of the service can be found [here](https://github.com/owulveryck/cli-grpc-example/blob/master/server/main.go).

## A very quick client

The principle is the same for the client. All the needed methods are auto-generated and wrapped by the `protoc` command.

The steps are:

1. create a network connection to the gRPC server (with TLS)
2. create a new instance of myservice'client
3. call a function and get a result

for example:

{{< highlight go >}}
conn, _ := grpc.Dial("127.0.0.1:1234", grpc.WithInsecure())
defer conn.Close()
client := myservice.NewMyServiceClient(conn)
output, err := client.Hello(context.Background(), &myservice.Arg{os.Args[1:]})
{{</ highlight >}}

_Note_: By default, gRPC requires some TLS. I have specified the `WithInsecure` option because I am running on the local loop and it is just an example. Don't do that in production.

# Going further

Normally, Unix tools should respect a [certain philosophy](http://www.faqs.org/docs/artu/ch01s06.html) such as:

<center>**Rule of Silence: When a program has nothing surprising to say, it should say nothing.**</center>

Anyway, we all know that tools are verbose, so let's add a feature that sends the content of stdout and stderr back to the client. (And anyway, we are implementing a service greeting. It would be useless if it was silent :))

## stdout / stderr

What we want to do is to change the output of the commands. 
Therefore, we simply add two more fields to the `Output` object in the protobuf definition:
{{< highlight protobuf >}}
message Output {
    int32 retcode = 1;
    bytes stdout = 2;
    bytes stderr = 3;
}
{{</ highlight >}}

The generated file contains the following definition for `Output`:

{{< highlight go >}}
type Output struct {
      Retcode int32  `protobuf:"varint,1,opt,name=retcode" json:"retcode,omitempty"`
      Stdout  []byte `protobuf:"bytes,2,opt,name=stdout,proto3" json:"stdout,omitempty"`
      Stderr  []byte `protobuf:"bytes,3,opt,name=stderr,proto3" json:"stderr,omitempty"`
}
{{</ highlight >}}

We have changed the Output type, but as all the fields are embedded within the structure, the "service implementation" interface (`grpcCommand`) has not changed.
We only need to change a little bit the implementation in order to return a completed `Output` object:

{{< highlight go >}}
func (g *grpcCommands) Hello(ctx context.Context, in *myservice.Arg) (*myservice.Output, error) {
    var stdout, stderr []byte
    // ...
    return &myservice.Output{ret, stdout, stderr}, err
}
{{</ highlight >}}

Now we have to change the `wrapper` function that has been defined previously to return the content of stdout and stderr:

{{< highlight go >}}
func wrapper(cf cli.CommandFactory, args []string) (int32, []byte, []byte, error) {
    // ...
}
func (g *grpcCommands) Hello(ctx context.Context, in *myservice.Arg) (*myservice.Output, error) {
    var stdout, stderr []byte
    ret, stdout, stderr, err := wrapper(g.commands["hello"], in.Args)
    return &myservice.Output{ret, stdout, stderr}, err
}
{{</ highlight >}}

All the job of capturing stdout and stderr is done within the wrapper function (This solution has been found on [StackOverflow](https://stackoverflow.com/questions/10473800/in-go-how-do-i-capture-stdout-of-a-function-into-a-string):

* first, we backup the standard `stdout` and `stderr`
* then, we create two times, two file descriptors linked with a pipe (one for stdout and one for stderr)
* we assign the standard `stdout` and `stderr` to the input of the pipe. From now on, every interaction will be written to the pipe and will be received into the variable declared as output of the pipe
* then, we actually execute the function (the business logic)
* we get the content of the output and save it to variable
* and then we restore stdout and stderr

Here is the implementation of the `wrapper`:
{{< highlight go >}}
func wrapper(cf cli.CommandFactory, args []string) (int32, []byte, []byte, error) {
	var ret int32
	oldStdout := os.Stdout // keep backup of the real stdout
	oldStderr := os.Stderr

	// Backup the stdout
	r, w, err := os.Pipe()
        // ...
	re, we, err := os.Pipe()
        //...
	os.Stdout = w
	os.Stderr = we

	runner, err := cf()
        // ...
	ret = int32(runner.Run(args))

	outC := make(chan []byte)
	errC := make(chan []byte)
	// copy the output in a separate goroutine so printing can't block indefinitely
	go func() {
		var buf bytes.Buffer
		io.Copy(&buf, r)
		outC <- buf.Bytes()
	}()
	go func() {
		var buf bytes.Buffer
		io.Copy(&buf, re)
		errC <- buf.Bytes()
	}()

	// back to normal state
	w.Close()
	we.Close()
	os.Stdout = oldStdout // restoring the real stdout
	os.Stderr = oldStderr
	stdout := <-outC
	stderr := <-errC
	return ret, stdout, stderr, nil
}
{{</ highlight >}}

**Et voil**, the cli has been transformed into a grpc webservice. The full code is available on [GitHub](https://github.com/owulveryck/cli-grpc-example).

### Side note about race conditions

The map used for cli.Command is not concurrent safe. But there is no goroutine that actually writes it so it should be ok.
Anyway, I have written a little benchmark of our function and passed it to the race detector. And it did not find any problem:

```shell
go test -race -bench=.      
goos: linux
goarch: amd64
pkg: github.com/owulveryck/cli-grpc-example/server
BenchmarkHello-2             200          10483400 ns/op
PASS
ok      github.com/owulveryck/cli-grpc-example/server   4.130s
```

The benchmark shows good result on my little chromebook, gRPC seems very efficient, but actually testing it is beyond the scope of this article.

### Interactivity

Sometimes, cli tools ask questions. Another good point with gRPC is that it is bidirectional. Therefore, it would be possible to send the question from the server to the client and get the response back. I let that for another experiment.

## Terraform ?

At the beginning of this article, I have explained that I was using this specific cli in order to derivate Hashicorp tools and turned them into webservices.
Let's take an example with the excellent terraform.

We are going to derivate terraform by changing only its cli interface, add some gRPC powered by protobuf... 

$$\frac{\partial terraform}{\partial cli} + grpc^{protobuf} = \mu service(terraform)$$ [^3]

[^3]: I know, this mathematical equation come from nowhere. But I simply like the beautifulness of this language. (I would have been damned by my math teachers because I have used the mathematical language to describe something that is not mathematical. Would you please forgive me, gentlemen :) 

### About concurrency

Terraform uses [backends](https://www.terraform.io/docs/backends/index.html) to store its states.
By default, it relies on the local filesystem, which is, obviously, not concurrent safe. It does not scale and cannot be used when dealing with webservices.
For the purpose of my article, I won't dig into the backend principle and stick to the local one.
Hence, this will only work with one and only one client. If you plan to do more work around terraform-as-a-service, changing the backend is a must!

### What will I test?

In order to narrow the exercise, I will partially implement the `plan` command.

My test case is the creation of an `EC2` instance on AWS. This example is a copy/paste of the example [Basic Two-Tier AWS Architecture](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/two-tier).

I will not implement any kind of interactivity. Therefore, I have added some default values for the ssh key name and path.

Let's check that the basic cli is working:

{{< highlight shell >}}
localhost two-tier [master*] terraform plan | tail
      enable_classiclink_dns_support:   "<computed>"
      enable_dns_hostnames:             "<computed>"
      enable_dns_support:               "true"
      instance_tenancy:                 "<computed>"
      ipv6_association_id:              "<computed>"
      ipv6_cidr_block:                  "<computed>"
      main_route_table_id:              "<computed>"

Plan: 9 to add, 0 to change, 0 to destroy.
{{</ highlight >}}

Ok, let's "hack" terraform!

### hacking Terraform

#### Creating the protobuf contract

The contract will be placed in a `terraformservice` package.
I am using a similar approach as the one used for the greeting example described before:

{{< highlight protobuf >}}
syntax = "proto3";

package terraformservice;

service Terraform {
    rpc Plan (Arg) returns (Output) {}
}

message Arg {
    repeated string args = 1;
}

message Output {
    int32 retcode = 1;
    bytes stdout = 2;
    bytes stderr = 3;
}
{{</ highlight >}}

Then I generate the `go` version of the contract with:

`protoc --go_out=plugins=grpc:. terraformservice/terraform.proto`

### The go implementation of the interface

I am using a similar structure as the one defined in the previous example.
I only change the methods to match the new ones:

{{< highlight go >}}
type grpcCommands struct {
      commands map[string]cli.CommandFactory
}

func (g *grpcCommands) Plan(ctx context.Context, in *terraformservice.Arg) (*terraformservice.Output, error) {
      ret, stdout, stderr, err := wrapper(g.commands["plan"], in.Args)
      return &terraformservice.Output{ret, stdout, stderr}, err
}
{{</ highlight >}}

The wrapper function remains exactly the same as the one defined before because I didn't change the Output format.

### Setting a gRPC server in the main function

The only modification that has to be done is to create a listener for the grpc like the one we did before.
We place it in the main code, just before the execution of the `Cli.Run()` call: 

{{< highlight go >}}
if len(cliRunner.Args) == 0 {
        log.Println("Listening on 127.0.0.1:1234")
        listener, err := net.Listen("tcp", "127.0.0.1:1234")
        if err != nil {
                log.Fatalf("failed to listen: %v", err)
        }
        grpcServer := grpc.NewServer()
        terraformservice.RegisterTerraformServer(grpcServer, &grpcCommands{cliRunner.Commands})
        // determine whether to use TLS
        grpcServer.Serve(listener)
}
{{</ highlight >}}

### Testing it

The code compiles without any problem.
I have triggered the `terraform init` and I have a listening process waiting for a call:

```shelli
~ netstat -lntp | grep 1234
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp        0      0 127.0.0.1:1234          0.0.0.0:*               LISTEN      9053/tfoliv     
```
Let's launch a client:

{{< highlight go >}}
func main() {
      conn, err := grpc.Dial("127.0.0.1:1234", grpc.WithInsecure())
      if err != nil {
            log.Fatal("Cannot reach grpc server", err)
      }
      defer conn.Close()
      client := terraformservice.NewTerraformClient(conn)
      output, err := client.Plan(context.Background(), &terraformservice.Arg{os.Args[1:]})
      stdout := bytes.NewBuffer(output.Stdout)
      stderr := bytes.NewBuffer(output.Stderr)
      io.Copy(os.Stdout, stdout)
      io.Copy(os.Stderr, stderr)
      fmt.Println(output.Retcode)
      os.Exit(output.Retcode)
}
{{</ highlight >}}

```shell
~ ./grpcclient
~ echo $?
~ 0
```

Too bad, the proper function has been called, the return code is ok, but all the output went to the console of the server... Anyway, the RPC has worked.

I can even remove the default parameters and pass them as an argument of my client:

```shell
~ ./grpcclient -var 'key_name=terraform' -var 'public_key_path=~/.ssh/terraform.pub'
~ echo $?
~ 0
```

And let's see if I give a non existent path:

```shell
~ ./grpcclient -var 'key_name=terraform' -var 'public_key_path=~/.ssh/nonexistent'
~ echo $?
~ 1
```

_about the output_: I have been a little optimistic about the stdout and stderr. 
Actually, to make it work, the best option would be to implement a custom `UI` (it should not be difficult because [`Ui is also an interface`](https://godoc.org/github.com/mitchellh/cli#Ui)).
I will try an implementation as soon as I will have enough time to do so. But for now, I have reached my first goal, and this post is long enough :)

# Conclusion

Transforming terraform into a webservice has required a very little modification of the terraform code itself which is very good for maintenance purpose:

{{< highlight diff >}}
diff --git a/main.go b/main.go
index ca4ec7c..da5215b 100644
--- a/main.go
+++ b/main.go
@@ -5,14 +5,18 @@ import (
        "io"
        "io/ioutil"
        "log"
+       "net"
        "os"
        "runtime"
        "strings"
        "sync"
 
+       "google.golang.org/grpc"
+
        "github.com/hashicorp/go-plugin"
        "github.com/hashicorp/terraform/helper/logging"
        "github.com/hashicorp/terraform/terraform"
+       "github.com/hashicorp/terraform/terraformservice"
        "github.com/mattn/go-colorable"
        "github.com/mattn/go-shellwords"
        "github.com/mitchellh/cli"
@@ -185,6 +189,18 @@ func wrappedMain() int {
        PluginOverrides.Providers = config.Providers
        PluginOverrides.Provisioners = config.Provisioners
 
+       if len(cliRunner.Args) == 0 {
+               log.Println("Listening on 127.0.0.1:1234")
+               listener, err := net.Listen("tcp", "127.0.0.1:1234")
+               if err != nil {
+                       log.Fatalf("failed to listen: %v", err)
+               }
+               grpcServer := grpc.NewServer()
+               terraformservice.RegisterTerraformServer(grpcServer, &grpcCommands{cliRunner.Commands})
+               // determine whether to use TLS
+               grpcServer.Serve(listener)
+       }
+
        exitCode, err := cliRunner.Run()
        if err != nil {
                Ui.Error(fmt.Sprintf("Error executing CLI: %s", err.Error()))
{{</ highlight >}}

Of course, there is a bit of work to setup a complete terraform-as-a-service architecture, but it looks promising.

Regarding grpc and protobuf:
gRPC is a very nice protocol, I am really looking forward an implementation in javascript to target the browser
(Meanwhile it is possible and easy to set up a grpc-to-json proxy if any web client is needed). 

But it reminds us that the main target of RPC is machine-to-machine communication. This is something that the ease-of-use-and-read of json has shadowed...

---
categories:
- aws
date: 2017-01-13T22:22:46+01:00
description: "In this post, I will explain how to extract, process and store informations from a webservice to a NoSQL database (DynamoDB)"
draft: false
images:
- /assets/images/bigdata/stones-483138_640.png
tags:
- dynamodb
- aws
- golang
title: A foot in NoSQL and a toe in big data
---

The more I work with AWS, the more I understand their models. This goes far beyond the technical principles of micro service.
As an example I recently had an opportunity to dig a bit into the billing process.
I had an explanation given by a colleague whose understanding was more advanced than mine.
In his explanation, he mentioned this blog post: [New price list API](https://aws.amazon.com/blogs/aws/new-aws-price-list-api/).

# Understanding the model
By reading this post and this [explanation](http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/price-changes.html), I understand that the offers are categorized in families (eg AmazonS3) and that an offer is composed of a set of products.
Each product is characterized by its SKU's reference ([stock-keeping unit](https://en.wikipedia.org/wiki/Stock_keeping_unit))

## Inventory management

So finally, it is just about inventory management. In the retail, when you say "inventory management", the IT usually replies with millions dollars _ERP_.
And the more items we have, the more processing power we need and then more dollar are involved... and richer the IT specialists are (just kidding).

Moreover enhancing an item by adding some attributes can be painful and risky

![xkcd](http://imgs.xkcd.com/comics/exploits_of_a_mom.png)

## The NoSQL approach 

Due to the rise of the online shopping, inventory management must be real time.
The stock inventory is a business service. and placing it in a micro service architecture bring constraints: the request should be satisfied in micro seconds.

More over, the key/value concept allows to store "anything" in a value. Therefore, you can store a list of attributes regardless of what the attributes are.

When it comes to NoSQL, there are usually two approaches to store the data:

* simple Key/Value;
* document-oriented.

At first I did and experiment with a simple key/value store called BoltDB (which is more or less like Redis).
In this approach the value stored was a json representation... A kind of document.
Then I though that it could be a good idea to use a more document oriented service: DynamoDB

# Geek time

In this part I will explain how to get the data from AWS and to store them in the dynamoDB service. The code is written in GO and is just a proof of concept.

## The product informations

A product's technical representation is described [here](http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/reading-an-offer.html).
We have:

{{< highlight js >}}
"Product Details": {
   "sku": {
      "sku":"The SKU of the product",
      "productFamily":"The product family of the product",
      "attributes": {
         "attributeName":"attributeValue",
      }
   }
}
{{</ highlight >}}

There are three important entries but only two are mandatories:

* *SKU*: A unique code for a product. 
* *Product Family*: The category for the type of product. For example, compute for Amazon EC2 or storage for Amazon S3.
* Attributes: A list of all of the product attributes.

## Creating the "table"

As my goal is for now to create a proof of concept and play with the data, I am creating the table manually.
DynamoDB allows the creation of two indexes per table. So I create a table _Products_ with two indexes:

* **SKU**
* **ProductFamily**

![Create Table](/assets/images/bigdata/blog-dynamo-create-table.png)

## Principle

The data is retrieved by a simple `http.Get` method. Then a `json.Decoder` takes the body (an `io.Reader`) as argument and decode it in a predefined structure.
Once the structure is filled, I will store it in the DynamoDB.


### The structures

I need three go structures. Two will be used to decode and range through the offer index. The other one will hold all the product details for a specific offer.

#### Offer Index
The offer index is composed of offers referenced in by an offer name (`map[string]offer`)
{{< highlight go >}}
type offerIndex struct {
    FormatVersion   string           `json:"formatVersion"`
    Disclaimer      string           `json:"disclaimer"`
    PublicationDate time.Time        `json:"publicationDate"`
    Offers          map[string]offer `json:"offers"`
}
{{</ highlight >}}

An offer in the index is characterized by three elements. I am catching all of them, but only `CurrrentVersionURL` is useful in my case.
{{< highlight go >}}
type offer struct {
    OfferCode         string `json:"offerCode:"`
    VersionIndexURL   string `json:"versionIndexUrl"`
    CurrentVersionURL string `json:"currentVersionUrl"`
}
{{</ highlight >}}

#### Products
I hold all the product details in a structure. The product details holds all the products in a map whose key is the SKU. Therefore a SKU field is useless.
The Attribute value is an interface{} because it can be of any type (more on this later in the post).

_Note_ : In case of massive data flow, it would probably be better to decode the stream pieces by pieces (as written in the [the go documentation](https://golang.org/pkg/encoding/json/#Decoder.Decode))

{{< highlight go >}}
type productDetails struct {
    Products map[string]struct { // the key is SKU
        ProductFamily string                 `json:"productFamily"`
        Attributes    map[string]interface{} `json:"attributes"`
    } `json:"products"`
}
{{</ highlight >}}

### Getting the data

#### Offers 
The first action is to grab the json of the offer index and put it in a object of type `offerIndex`
{{< highlight go >}}
resp, err := http.Get("https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/index.json")

var oi offerIndex
err = json.NewDecoder(resp.Body).Decode(&oi)
// oi contains all the offers
{{</ highlight >}}

Then loop for each offer and do a `GET` of every `CurrentVersionURL`
{{< highlight go >}}
for _ , o := range oi.Offers {
        resp, err := http.Get("https://pricing.us-east-1.amazonaws.com" + o.CurrentVersionURL)
{{</ highlight >}}

#### And products

The same principles applies for the products, we decode the stream in an object:

{{< highlight go >}}
var pd productDetails
err = json.NewDecoder(resp.Body).Decode(&pd)
{{</ highlight >}}

Now that we have all the informations we are ready to store them in the database.

## Storing the informations

As usual with any AWS access, you need to create a `session` and a `service` object:

{{< highlight go >}}
sess, err := session.NewSession()
svc := dynamodb.New(sess)
{{</ highlight >}}

The [session](http://docs.aws.amazon.com/sdk-for-go/api/aws/session/) will take care of the credentials by reading the appropriate files or environment variables.

the `svc` object is used to interact with the DynamoDB service. To store an object we will use the method [PutItem](http://docs.aws.amazon.com/sdk-for-go/api/service/dynamodb/#DynamoDB.PutItem) which takes as argument a reference to [PutItemInput](http://docs.aws.amazon.com/sdk-for-go/api/service/dynamodb/#PutItemInput).

_Note_ All of the AWS service have the same logic and work the same way: Action takes as a parameter a reference to a type ActionInput and returns a type ActionOutput.

Let's see how to create a `PutItemInput` element from a `Product` type.

#### the Dynamodb Item

The two mandatory fields I will use for the `PutItemInput` are:

* `TableName` (which is Product in my case)
* `Item` (which obviously hold what to store)

Other fields exists, but to be honest, I don't know whether I need them by now.

The `Item` expects a map whose key is the field name (In our case it can be "SKU", "ProductFamily" or anything) and whose value is a reference to the special type [AttributeValue](http://docs.aws.amazon.com/sdk-for-go/api/service/dynamodb/#AttributeValue).

From the documentation the definition is:

_AttributeValue Represents the data for an attribute. You can set one, and only one, of the elements._

The AttributeValue is _typed_ (The types are described [here](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_AttributeValue.html))
Therefore our informations (remember the `map[string]inteface{}`) must be "convrted" to a dynamodb format.
This task has been made easy by using the package [dynamodbattribute](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_AttributeValue.html) which does it for us:

To fill the item I need to loop for every product in the object `pd` and create an item:

{{< highlight go >}}
for k, v := range pd.Products {
      item["SKU"], err = dynamodbattribute.Marshal(k)
      item["ProductFamily"], err = dynamodbattribute.Marshal(v.ProductFamily)
      item["Attributes"], err = dynamodbattribute.Marshal(v.Attributes)
{{</ highlight >}}

Once I have an Item, I can create the parameters and send the request to the DB:

{{< highlight go >}}
Item:      item,
      TableName: aws.String(config.TableName),
}
// Now put the item, discarding the result
_ , err = svc.PutItem(params)
{{</ highlight >}}

# Execution and conclusion

Once compiled I can run the program that will take a couple of minute to execute (it can easily be optimized simply by processing each offer in a separate goroutine).
Then I can find the informations in my DB:
![Result](/assets/images/bigdata/blog-dynamo-result.png)

Now that I have the informations, on the same principle I can grab the prices and put a little web service in front of it.
And I could even code a little fronted for the service.

I am aware that if you are not an average go programmer the code may seem tricky, but I can assure you that it is not (the whole example is less than 100 lines long including the comments).
The AWS API seems strange and not idiomatic, but it has the huge advantage to be efficient and coherent.

Regarding the inventory model. it can be used for any product or even any stock and prices. It is a cheap (and yet efficient) way to manage an inventory.

# Full code

The full code of the example can be found on my [gist](https://gist.github.com/owulveryck/f9665470e8334e8609434feeeddc6071)
+++
date = "2015-10-26T10:41:57Z"
draft = false
title = "Developping \"Google Apps\" on my Chromebook"
tags = [
    "chromebook",
    "configuration"
]
+++

It is a week now that I'm playing with my chromebook.
I really enjoy this little internet Terminal.

I "geeked" it a little bit and I installed my favorites dev tools eg:

* [The solarized theme for the terminal](https://gist.github.com/johnbender/5018685)
* `zsh` with [Oh-my-zsh](https://github.com/robbyrussell/oh-my-zsh)
* `tmux` (stared with `tmux -2` to get 256 colors)
* `git`
* `vim`
* a `Go` compiler
* The [`HUGO`](http://gohugo.io/overview/quickstart/) tool to write this blog.


All of it has been installed thanks to the "brew" package manager and following [those instructions](https://github.com/Homebrew/linuxbrew/wiki/Chromebook-Install-Instructions).

## Google Development Environment

I've installed the Google Development Environement as described [here](https://cloud.google.com/appengine/docs/go/gettingstarted/devenvironment).

Python 2.7 is a requirements so I `brewed it` without any noticeable issue.

When I wanted to serve locally my very first Google App developement, I ran into the following error:

```
~ go app serve $GOPATH/src/myapp
...
ImportError: No module named _sqlite3
error while running dev_appserver.py: exit status 1
```

Too bad. I've read that this module should be built with python, but a even a `find /` (I know it's evil) didn't return me any occurence.

So, I have:

* Googled 
* reinstalled sqlite with `brew reinstall sqlite`
* reinstalled python with `brew reinstall python`
* played with brew link, unlink and so
* ...

Still no luck!

I've also tried the compilation with a `verbose` option, and I the log file, there is an explicit message:

```
Python build finished, but the necessary bits to build these modules were not found:
_bsddb  _sqlite3_tkinter
...
To find the necessary bits, look in setup.py in detect_modules() for the modules name.
```


That's where I am now, stuck with a stupid python error. I'd like the folks at google to provide a pure go developement enrironement that would avoid the bootstraping problems.

I'll post an update as soon as I have solved this issue !

*EDIT*:

I've had a look in the `setup.py` file. To compile the sqlite extension, it looks into the following paths:

```
...
sqlite_incdir = sqlite_libdir = None
sqlite_inc_paths = [ '/usr/include',
                     '/usr/include/sqlite',
                     '/usr/include/sqlite3',
                     '/usr/local/include',
                     '/usr/local/include/sqlite',
                     '/usr/local/include/sqlite3',
                   ]
...
```

But in my configuration, the libraries are present in `/usr/local/linuxbrew/*`. Hence, simply linking the include and libs did the trick

I'm now facing another error when I try to run the `goapp serve` command:

```
...
AttributeError: 'module' object has no attribute 'poll'
error while running dev_appserver.py: exit status 1
```

Google told me, that on OSX the poll system call is broken and has been disabled.
As brew is mainly developped on MacOS, that may be the reason

I've recompiled the python with the `--with-poll` option and that did the trick.

## Finally

Here are my options for compiling python:

```
~ brew reinstall python --with-brewed-openssl --with-brewed-sqlite --with-poll 
...
Warning: The given option --with-poll enables a somewhat broken poll() on OS X (https://bugs.python.org/issue5154)  Formula git:(master)).
...
```

And the `goapp serve` is finally working on my Chromebook:

```
~ goapp serve /home/chronos/user/GOPROJECTS/src/github.com/owulveryck/google-app-example/
INFO     2015-10-26 15:48:04,840 devappserver2.py:763] Skipping SDK update check.
INFO     2015-10-26 15:48:04,935 api_server.py:205] Starting API server at: http://localhost:54116
INFO     2015-10-26 15:48:06,092 dispatcher.py:197] Starting module "default" running at: http://localhost:8080
INFO     2015-10-26 15:48:06,096 admin_server.py:116] Starting admin server at: http://localhost:8000
INFO     2015-10-26 15:48:16,700 shutdown.py:45] Shutting down.
INFO     2015-10-26 15:48:16,701 api_server.py:648] Applying all pending transactions and saving the datastore
INFO     2015-10-26 15:48:16,701 api_server.py:651] Saving search indexes
```
---
categories:
- category
date: 2017-05-04T19:15:12+02:00
description: ""
draft: true
images:
- /2016/10/image.jpg
tags:
- tag1
- tag2
title: What I learned at the Google Next 2017 in London
---

I have rencetly attended the google Next event in London.

# The infrastructure level

## Compute engine: a pure IaaS Solution

## App engine: Severless before it was cool

### Traditional app-engine

### Flex app-engine

## Cloud functions 

# The machine learning 

## The tools provided

### Tensorflow

#### The Shakespear demo 

### The ML services and APIs

#### Computer vision

#### Translate

#### NLP

### Why machine learning

# About databases

# Methods at Google

## Mindset

## SRE

## Change management

# Conclusion

---
categories:
date: 2017-06-01T22:07:56+02:00
description: "A very brief article about my first call to the new service of Google Cloud Plateform: Video Intelligence. Caution: The video used in this example is #NSFW"
draft: false  
images:
- https://cloud.google.com/images/products/video-intelligence/analysis.png
tags:
title: Analyzing a parodic trailer (NSFW) with Google Cloud Video Intelligence
---

Google has recently announced its new service called "[Google Cloud Video Intelligence](https://cloud.google.com/video-intelligence/)".
The purpose of this service is to offer tagging and annotations of digital videos.

I will try this service on a trailer of a French parody. This movie is made of several scenes taken from erotic movies of the seventies.

Why this parody?

* because it is fun
* because it is composed of a lot of different scenes
* because it is short (so it won't cost me a lot)
* because, as it is related to erotic of the seventies, I am curious about the result!

_Caution_: this video **is not a porn video**, but is indeed **not safe for work** (_#nsfw_)

# What information can the service find?

## Shot change detection

This feature will detect the different scenes and display their time range. There is no further analysis of the scene. That is to say that it won't tell, by now, that the first scene is about a sports competition. But indeed it will describe that the first scene occurs from the first microsecond until the xxx microsecond and so on.

## Label detection

The more interesting feature is the label detection.

With this operation, the service will display tags of any element found in the video, as well as the time range of the video where they can be seen.

For example, it may tell you that there is a dog in the video between x and y micro-seconds as well as between w and z micro-seconds.

# Preparing the video

I have downloaded the video, thanks to [youtube-dl](https://rg3.github.io/youtube-dl/) and I have uploaded it to [Google Cloud Storage](https://cloud.google.com/products/storage/) as the API expects the video to be here. There may be a way to post the video encoded in base64 directly, but that would have been less convenient for my tests.

![screnshot](/assets/video-intelligence/gs-trailer.png)

# Querying Google Cloud Video Intelligence 

This test is made with the simple REST API with `curl`.

## Preparing the request

To actually use the API, we need to perform a POST request. 
The payload is a simple JSON file where we specify:

* the URI of the video file to process
* an array of features to use among: Shot change detection and/or label detection

Here is my payload. I want both features for my test:

{{< highlight js >}}
{
    "inputUri": "gs://video-test-blog/trailer.mp4",
    "features": ["SHOT_CHANGE_DETECTION","LABEL_DETECTION"]
}
{{</ highlight >}}

## Launching the request

### Authorization

To actually use the service, I need an authorization token. This token is linked to a service account.
Then with the token, we can trigger the analysis by using this `curl` command:

{{< highlight shell >}}
curl -s -k -H 'Content-Type: application/json' \
      -H 'Authorization: Bearer MYTOKEN' \
      'https://videointelligence.googleapis.com/v1beta1/videos:annotate' \
      -d @demo.json
{{</ highlight >}}

The action replies with a JSON containing an `operation name`. Actually, the operation is long and asynchronous. This `operation name` can be used to get the processing status.

{{< highlight js >}}
{
   "name": "us-east1.16784866925473582660"
}
{{</ highlight >}}

### Getting the status

To request the status, we need to query the service to get the status of the operation:

{{< highlight shell >}}
curl -s -k -H 'Content-Type: application/json' \
      -H 'Authorization: Bearer MYTOKEN' \
      'https://videointelligence.googleapis.com/v1/operations/us-east1.16784866925473582660'
{{</ highlight >}}

It returns a result in `json` that in which we can find three important fields:

* `done`: a boolean that tells whether the processing of the video is complete or not
* `shotAnnotations`: an array of the shot annotations as described earlier
* `labelAnnotations`: an array of label annotations

Here is a sample output: (the full result is [here](/assets/video-intelligence/video-analysis-a-la-recherche.json))
{{< highlight js >}}
{
  "response": {
    "annotationResults": [
      {
        "shotAnnotations": [
          // ...
          {
            "endTimeOffset": "109479985",
            "startTimeOffset": "106479974"
          }
        ],
        "labelAnnotations": [
          // ... 
          {
            "locations": [
              {
                "level": "SHOT_LEVEL",
                "confidence": 0.8738658,
                "segment": {
                  "endTimeOffset": "85080015",
                  "startTimeOffset": "83840048"
                }
              }
            ],
            "languageCode": "en-us",
            "description": "Acrobatics"
          },
        ],
        "inputUri": "/video-test-blog/trailer.mp4"
      }
    ],
    //...
  },
  "done": true,
  //...
}
{{</ highlight >}}

# Interpreting the results

## Tag cloud

I will only look at the label annotations.
The API has found a lot of label described under the `description` fields and 1 to N location where such a description is found.

What I can do is to manipulate the data to list all the label with their frequency.

You can find [here](https://gist.github.com/owulveryck/70d97e1e73d664c1c927c253a862ac17) a little go code that will display labels as many times as they occur.

For example:

```
Abdomen Abdomen Abdomen Acrobatics Action figure Advertising Advertising ...
```

This allows me to generate a tag cloud with the help of [this website](https://www.jasondavies.com/wordcloud/):

So here is the visual result of what the service has found in the video:

![tag cloud](/assets/video-intelligence/wordcloud.png)

## Annotated video

To find out where the labels are, I made a little javascript that display the elements alongside of the youtube video.
Just click on the video and the tags will be displayed below.

<button id="launchyt">It is safe to watch the video, please show the result!</button>

<div id="player"></div>

<ul id="labels"></ul>

<ul>
    <li id="result1"></li>
    <li id="result2" style="color: #8A8A8A;"></li>
    <li id="result3" style="color: #9E9E9E;"></li>
    <li id="result4" style="color: #B2B2B2;"></li>
    <li id="result5" style="color: #C6C6C6;"></li>
</ul>

<script type="text/javascript" async src="/assets/video-intelligence/app.js"></script>

# Conclusion

There is a lot more to do than simply displaying the tags.
For example, We could locate an interesting tag, take a snapshot of the video, and use the photo API to find websites related to this part of the video.

For example, in this video, it can be possible to find the original movies were people are dancing for example.

I will postpone this for another geek-time.

_P.S._ The javascript has been made with _gopherjs_. It is not optimize at all (I should avoid the encoding/json package for example). If you are curious about the implementation, the code is [here](/assets/video-intelligence/main.go), [here](/assets/video-intelligence/structure.go) and [here](/assets/video-intelligence/data.go).
---
author: Olivier Wulveryck
date: 2016-10-19T07:24:51+02:00
description: How to write a Single Page Application MVC without blowing your mind with Javascript and a Framework.
draft: false
keywords:
tags:
- javascript
- jQuery
- Gopherjs
- SPA
- WebUI
- Cordova
title: SPA with Gopherjs
topics:
- Wed UI
type: post
---

# Introduction

Single page application (SPA) are a standard when dealing with mobile apps.
Unfortunately, by now, JavaScript is the only programming language supported by a browser.

Therefore, to code web UI it remains a must.

## Life of an ex sysadmin who wants to code a web app: getting depressed

To make the development easier, your friend, who is "web developer" will recommend you to use a marvelous framework.
Depending on the orientation of the wind, the moon or its reading, he will encourage you to use `reactjs`, `angular`, `ember` or whatever exotic
tool.

With some recommendation from my real friends and from Google, I've started an app based on [ionic](http://ionicframework.com/) which is based on [angular](https://angularjs.org/).
As I did not know anything about angular, I've watched a (very good) [introduction](https://www.youtube.com/watch?v=i9MHigUZKEM) and followed the ionic tutorial.

So far so good...

Then I implemented a SSO with Facebook. I wrote a backend in `go` to handle the token generation and the used database connection.
I started to code it by hand, until a friend tells me about the angular module [Satellizer](https://github.com/sahat/satellizer) that was suppose to handle the logic for me.
And it did.... It was suddenly automagic:

Everything was working on my browser. I was happy, So I decided to deploy my app on my iPhone and enjoy the power of Cordova.

That's when the headache started: There was something wrong on the mobile phone version. A bug!

I tried to debug it, with Xcode, with Safari... The more I was searching, the more I had to dive into the framework. Too many magic in it for 
something that was, in fine, not a bug [^1].

I asked some help from a friend and his first reply was: "which version of angular? Because in version 2 they have changed a lot of concepts"

That was too much.
I considered that this world made of JavaScript, frameworks, grunt, bower, gulp, npm or whatever fancy tool was definitely not for me.
Too many work to learn something already outdated.

On top of that, I've never been a callback man, I hate them since my X11/Motif programming course. I do like CSP!

<center>
![Matt Holt's Tweet](/assets/images/not_my_type.png)
</center>
## Out of the depression!

Ok, I abandoned those tools. But I still want to code my app, and I'm not the kind of guy that easily give up.

Let's resume my needs:

* I need a MVC, because it's the most natural way to code web ui today
* MVC is not framework dependent
* A SPA is the good choice for a mobile app and Cordova makes things easy
* Javascript is mandatory

I've digge a little bit and I've found this blog post: [Do you really want an SPA framework?](https://mmikowski.github.io/no-frameworks/) which leads me to "the solution": 

I will code my model/view/controller from scratch.
But as I want to preserve my health and stay away from Javacript, I will code it with something fun: go.

At the last doGo.eu [Dmitri Shuralyov](https://twitter.com/shurcool) gave a very good introduction about [gopherjs](https://github.com/gopherjs/gopherjs). Gopherjs is a [transpiler](https://en.wikipedia.org/wiki/Source-to-source_compiler) that converts go code into javascript.

You can code all your logic in go and transpile it in javascript, or you can use is to access you DOM or other javascript libraries.

A bunch of bindings to famous javascript libraries such as jQuery already exists on gihtub,

Let's see an example and implement a very basic routing mechanism relying on a pure js library.

# Examples

I will code a little page based on bootstrap.

## A basic go code

The dynamic part will be coded in pure GO in a file called `main.go` and transpiled into javscript code with the help of the `gopherjs` command:

```bash
gopherjs build main.go -o js/app.js -m
```

To make things easier, I can add some directives in my go code in the form of a comment:

```go
package main
//go:generate gopherjs build main.go -o js/app.js -m
// +build ignore

import (
  "log"
)

func main() {
    log.Println("Hello World")
}
```

Therefore I will be able to generate my code directly with a simple call to `go generate` and it will produce the `js/app.js` file for me.

## The page

The structure of the main page is taken from bootstrap's [starter template](http://getbootstrap.com/examples/starter-template/#) 

I simply add my javascript file generated with gopherjs :

```html
<script src="js/app.js"></script>
```

If I launch my page, I will have a "hello world" written in the javascript browser of my console.

# the SPA

Now, I will implement a very basic SPA.
It will display three tabs accessible by their names (for demo purpose):

* [/#](/#)
* [/#about](/#about)
* [/#contact](/#contact)

I want to trigger a javascript code that could change the content of the body by clicking on the links.


## Routing

A good SPA needs a good routing system to deal with anchor refs.

There are several implementation of gopherjs based routing mechanism.

But, for the purpose of this blog post, I will use a pure Javascript routing library: [Director.js](https://github.com/flatiron/director#routing-table). It's the router used in the todoMVC example and it will allow me to show how to interact with global javascript objects.

The first thing to do is to include the js file at the end of the `index.html`

```html
<script src="js/director.min.js"></script>
<script src="js/app.js"></script>
```

Then I will create a GO type _Router_ that will correspond to the Router objet in javascript.
To do so, the Router type must be a [*js.Object](https://godoc.org/github.com/gopherjs/gopherjs/js#Object)

```go
import "github.com/gopherjs/gopherjs/js"

type Router struct {
    *js.Object
}
```

Then I define a constructor, that simply get the router object from the global scope of the javascript engine:
```go
func NewRouter() *Router {
    return &Router{js.Global.Get("Router").New()}
}
```

Then, to actually implement my [adhoc-routing](https://github.com/flatiron/director#adhoc-routing) as described in the doc of director.js,
I must implement the `on` and the `init` bindings.

Once done, I add the routes in my `main` func:

{{< gist owulveryck 3256d582ad2241eeeaf118d5bf9c1cd0 "router.go" >}}

If I launch the page, I can now click on the links and it will diplay hello in my console.

You can check the full code on [this gist](https://gist.github.com/owulveryck/3256d582ad2241eeeaf118d5bf9c1cd0)

You see that I've let the function as `notImplementedYet`, but replacing it with a jQuery call is trivial:

```go
import "github.com/gopherjs/jquery"

//convenience:
var jQuery = jquery.NewJQuery

func content() {
    jQuery("#main").SetText("Welcome to GopherJS")
}
```

# Conclusion

Gopherjs is not trivial, but it has the ability to make the web development more structured. 
I've started a web ui from scratch and reach the same goal as the one I reached in javacript in only 2 days (compared to 3 weeks).

Of course, a javascript-master-of-the-world would argue that he would implement it in 2 hours, but that's not the point here.
The point is that I can use all the "benefits" of the go principles easily to write a web ui.

You can check the development of the [Nhite fronted](https://github.com/nhite/frontend) to watch the progress I will make (or not) with this technology.

----
[^1]:
1 - Actually, I figured out what the "bug" was later, when I finished the implementation in go and there was no magic anymore in the code.
The oauth2 flow I use is "[Authorization code](https://tools.ietf.org/html/rfc6749#section-1.3.1)". In this flow, you query the authorization server (here facebook) and send it the client identifier and _a redirection URI_.
In my dev environment this redirection URI is set to "http://localhost". Once the user is logged in (on the Facebook page), the navigation window redirects him in the application at localhost.
When running on iOS with _cordova_ the files are served locally (file://,,,) and there is no way to specify a redirect URI that point to file://, therefore the redirect URI must point somewhere else... but in this case, getting the code from the application becomes tricky because of the security policies. I could do a complete blog post about this.

---
categories:
- category
date: 2017-03-28T21:03:52+02:00
description: "Now that I have played with GraphQL, Let's see how to render the data thanks to its perfect companion: React"
draft: false
images:
- /assets/images/graphql-react-logos.png
tags:
- react
- graphql
title: From GraphQL to a table view with React and Apollo
---

In the last post I have played with _GraphQL_. 
The next step is to actually query the data and display it.

In this post I will use _[react](https://facebook.github.io/react/)_ (from Facebook) and the _[Apollo](http://dev.apollodata.com/)_ GraphQL client.

# Tooling

## React

I won't give in here an introduction of the language because you may find on the web thousands of very good tutorials and advocacy articles.
Anyway I will explain briefly why I have chosen React.

It ain't no secret to anyone: I am not a UI guy. Actually I hate Javascript... Well I hated what was javascript until recently.

I have heard a lot about React. As I am curious, I decided to give it a try.

The JSX syntax (the core syntax of react) is very clear. On top of that, the adoption of _Ecmascript 6_ make it an elegant and robust language.

React is component based, object oriented and relatively easy to learn.

On top of that, I have always planed to develop a mobile app (at least for fun). The "React Native" project would be a perfect fit to do so. But I will keep that for another moment when I am more Skilled with react.

### react-table

A lot of react libraries are available to render tables.

I did some research, and I found that the project [react-table](https://github.com/tannerlinsley/react-table) was the more documented and easy to use.

## Apollo

When it comes to GraphQL integration we have the choice between two major projects:

* [Relay](https://facebook.github.io/relay/) which is self described as _A Javascript framework for building data-driven react applications_
* [Apollo](http://dev.apollodata.com/) described as _The flexible, production ready GraphQL client for React and native apps_ 

Of course, as I knew nothing, I googled for some help about which framework I should use.

I found an excellent article on _codazen_: [Choosing a GraphQL Client: Apollo vs. Relay](https://www.codazen.com/choosing-graphql-client-apollo-vs-relay/). To be honest I didn't understand half of the concepts that were compared, but one of those was enough to help me in my choice.

The documentation of the Apollo project is exposed as "fantastic (with pictures)!". And as a newbie, I will need to refer to a strong documentation. That doesn't mean that I will not change later nor that this framework is better. That only means that I will use it for my first experiment.

## VIM

![https://xkcd.com/378/](https://imgs.xkcd.com/comics/real_programmers.png)

When I was at university I was an emacs guy... but when I started to work as a sys-admin I fell into vi/m. And by now I can hardly imagine using something else.
So vim will be my choice to edit react.

To use it decently, I have installed two plugins via [vundle:

{{< highlight shell >}}
# from my ~/.vimrc
Plugin 'pangloss/vim-javascript'
Plugin 'mxw/vim-jsx'     
{{</ highlight >}}

I have also installed [eslint](http://eslint.org/) which is a pluggable linting utility compatible with [syntastic](https://github.com/vim-syntastic/syntastic).

{{< highlight shell >}}
sudo npm install -g eslint
sudo npm install -g babel-eslint
sudo npm install -g eslint-plugin-react              
{{</ highlight >}}

I have also configured [syntastic](https://github.com/vim-syntastic/syntastic) to understand correctly the javascript and the react syntax by using 

{{< highlight shell >}}
# from my ~/.vimrc
let g:syntastic_javascript_checkers = ['eslint'] 
{{</ highlight >}}

# Let's code

## Installing react
 
 The installation of react is pretty straightforward:

{{< highlight shell >}}
sudo npm install -g react_
sudo npm install -g create-react-app
{{</ highlight >}}

The create-react-app is a helper program that pre-configure a lot of stuff for you (such as babel or webpack) to focus on the code instead of the tooling.

### Creating the basic apps

{{< highlight shell >}}
create-react-app blog-test
{{</ highlight >}}

This will create a directory `blog-test`, download all the base packages that are necessary for a hello-world app and create an application skeleton.

At the end, the output should be something like:

Success! Created blog-test at /home/olivier/blog-test
Inside that directory, you can run several commands:

<pre>
Success! Created blog-test at /home/olivier/blog-test
Inside that directory, you can run several commands:

  npm start
    Starts the development server.

  npm run build
    Bundles the app into static files for production.

  npm test
    Starts the test runner.

  npm run eject
    Removes this tool and copies build dependencies, configuration files
    and scripts into the app directory. If you do this, you can't go back!

We suggest that you begin by typing:

  cd blog-test
  npm start
</pre>

So if do what's suggested, you can open you browser and point to `localhost:3000` and get sonething that looks like that:

<iframe src="/assets/react/hello/index.html" width="100%" height="300">
  <p>Your browser does not support iframes.</p>
</iframe>

### Installing the dependencies

As explained, I will used the Apollo and react-table dependencies. Let's install them first:

{{< highlight shell >}}
npm install react-apollo --save
npm install react-table
{{</ highlight >}}

## Let's write the code of the application

The code of the application is located in the file `src/App.js`. It is a class that inherit from the React.Component.

### Setting up Apollo and the graphql connection

We will use Apollo to setup the connection to the GraphQL server we coded last time in `go`.
First, we need to import the `Apollo` dependencies:

{{< highlight js >}}
import { ApolloClient, ApolloProvider, createNetworkInterface } from 'react-apollo';
{{</ highlight >}}

Then we will instanciate the connection in the Constructor of the App component as written in Apollo's documentation:

{{< highlight js >}}
class App extends Component {
   constructor(props) {
     super(props);
     const networkInterface = createNetworkInterface({
       uri: 'http://localhost:8080/graphql'
     })
 
     this.client = new ApolloClient({
       networkInterface: networkInterface
     });
   }
 
   render() {
   ...
{{</ highlight >}}

_Note_ : The version of the server on github has been tweaked to handle the CORS Preflight request.

Then for now, we Setup the ApolloProvider component to do nothing in the render method:

{{< highlight js >}}
...
render() {
  return (
    <div className="App">
      <ApolloProvider client={this.client}>
      </ApolloProvider>
...
{{</ highlight >}}

### Defining the table

To use the react-table, we need to import the component:

{{< highlight js >}}
import ReactTable from 'react-table'
import 'react-table/react-table.css'
{{</ highlight >}}

_Note_ : I also import the CSS

Then we create the columns headers as expected by the `react-table` component (it is well described in the documentation):

Here we want to display the product SKU, its location, the instance type and the operatingSystem.
{{< highlight js >}}
const columns = [{
    header: 'SKU',
    accessor: 'sku' // String-based value accessors!
  }, {
    header: 'Location',
    accessor: 'location',
    sortable: true,
  }, {
    header: 'Instance Type',
    accessor: 'instanceType'
  }, {
    header: 'Operating System',
    accessor: 'operatingSystem'
}]
{{</ highlight >}}

Then we create a component `ProductList` that will render the table:

{{< highlight js >}}
function ProductList({ loading, products }) {
   if (loading) {
     return <div>Loading</div>;
   } else {
     return (
       <div className="App">
       <ReactTable className="-striped -highlight"
         data={products}
         columns={columns}     
       />
       </div>
     );
  } 
} 
{{</ highlight >}}

Then we change the render function to use the ProductList instead of the default view:

{{< highlight js >}}
...
render() {
  return (
    <div className="App">
      <ApolloProvider client={this.client}>
        <ProductList />
      </ApolloProvider>
    </div>
  );
}
{{</ highlight >}}

If you did everything correctly, you shoud see this:

<iframe src="/assets/react/table1/index.html" width="100%" height="300">
  <p>Your browser does not support iframes.</p>
</iframe>

### Now let's query:

To use the Graphql components of Apollo, we need to import them:

{{< highlight js >}}
import { gql, graphql } from 'react-apollo';
{{</ highlight >}}

Then, let's create the query as a constant in the exact same manner as when we did it with GraphiQL (cf last post):

{{< highlight js >}}
const allProducts = gql`
query products {
  products{
    sku
    location
    instanceType
    operatingSystem
  }                                                                                                                                                                                                             
}
`
{{</ highlight >}}

Now the "tricky" part: We must change the Component ProductList to use our Data.
This is documented on the Apollo Website under the section [Requesting Data](http://dev.apollodata.com/react/initialization.html):

_The graphql() container is the recommended approach for fetching data or making mutations. It is a React Higher Order Component, and interacts with the wrapped component via props._

As written in the doc, let's create a component `ProductListWithData`:

{{< highlight js >}}
const ProductListWithData = graphql(allProducts, {
  props: ({data: { loading, products }}) => ({
    loading,
    products,
  }),
})(ProductList);
{{</ highlight >}}

And use it within the Apollo provider instead of the ProductList Component

{{< highlight js >}}
...
render() {
  return (
    <div className="App">
      <ApolloProvider client={this.client}>
        <ProductListWithData />
      </ApolloProvider>
    </div>
  );
}
{{</ highlight >}}

# Conclusion

Et Voil... If you:

* start the graphql-test server from the last post
* start the dev environment with `npm start` inside the blog-test folder
* go to http://localhost:3000


You should see something like:

![Screenshot](/assets/images/React-table.png)

This was a very quick introduction. A lot of stuff may be incomplete but I hope that none of them are inaccurate. Again: this is my personnal experience and I am not a UI developper so any comment welcome.
---
categories:
- category
date: 2017-03-22T09:15:35+01:00
description: "My first experience with GraphQL. I will try to see how it fits the pricing model of AWS as described in an earlier post."
draft: false
images:
- /assets/images/graphqllogo.png
tags:
- golang
- aws
- graphql
title: Playing with Facebook's GraphQL (applied to AWS products and offers management)
---

# About GraphQL

GraphQL has been invented by Facebook for the purpose of refactoring their mobile application. Facebook had reached the limits of the standard REST API mainly because:

* Getting that much information was requiring a huge amount of API endpoints
* The versioning of the API was counter-productive regarding Facebook's frequents deployements.

But graphql is not only a query language related to Facebook. GraphQL is not only applicable to social data. 

Of course it is about graphs and graphs represents relationships. But you can represent relationships in all of your business objects.

Actually, GraphQL is all about your application data.

In this post I will try to take a concrete use case. I will first describe the business objects as a graph, then I will try to implement a schema with GraphQL. At the very end I will develop a small GraphQL endpoint to test the use case.

__Caution__ _I am discovering GraphQL on my own. This post reflects my own work and some stuff may be inaccurate or not idiomatic._

## The use case: AWS billing

Let's take a concrete example of a graph representation. Let's imagine that we are selling products related to Infrastructre as a Service (_IaaS_). 

For the purpose of this post, I will use the AWS data model because it is publicly available and I have already blogged about it.
We are dealing with products families, products, offers and prices.

In (a relative) proper english, let's write down a description of the relationships:

* Products
  * A product family is composed of several products
  * A product belongs to a product family
  * A product owns a set of attributes (for example its location, its operating system type, its type...)
  * A product and all its attributes are identified by a stock keeping unit (SKU)
  * A SKU has a set of offers
* Offers
  * An offer represents a selling contract
  * An offer is specific to a SKU
  * An offer is characterized by the term of the offer
  * A term is typed as either "Reserved" or "OnDemand"
  * A term has attributes
* Prices
  * An offer has at least one price dimension
  * A price dimension is characterized by its currency, its unit of measure, its price per unit, its description and eventually per a range of application (start and end)

Regarding those elements, I have extracted and represented a "t2.micro/linux in virginia" with 3 of its offers and all the prices associated.

Here is the graphical representation generated thanks to [graphviz' fdp](http://www.graphviz.org/)
![Graph Representation](/assets/graphql/graph.svg)


The goal of GraphQL is to extract a subtree of this graph to get part or all information.
As an example, here is a tree representation of the same graph:

![Graph Representation](/assets/graphql/graph_tree.svg)

_Note_: I wrote a very quick'n'dirty parser to get the information which can be found [here](https://gist.github.com/owulveryck/bac700e2f5e5b1af0fffda4e7adb9eed). I wrote an idiomatic one but it is the property of the company I made it for.

# Defining the GraphQL schema

The first thing that needs to be done is to write the [schema](http://graphql.org/learn/schema/) that will define the _query_ type.

I will not go into deep details in here. I will simple refer to this excellent document which is a _resume_ of the language:
[Graphql shorthand notation cheat sheet](https://github.com/sogko/graphql-schema-language-cheat-sheet/raw/master/graphql-shorthand-notation-cheat-sheet.png)

We can define a product that must contains a list of offers this way and a product family like this:

{{< highlight graphql >}}
# Product definition
type Product {
  offers: [Offer]!
  location: String
  instanceType: String
  sku: String!
  operatingSystem: String
}

# Definition of the product family
type ProductFamily {
  products: [Product]!
}
{{</ highlight >}}

One offer is composed of a mandatory price list. An offer must be of a pre-defined type: _OnDemand_ or _Reserved_.
Let's define this:
{{< highlight graphql >}}
# Definition of an offer
type Offer {
  type: OFFER_TYPE!
  code: String!
  LeaseContractLength: String
  PurchaseOption: String
  OfferingClass: String
  prices: [Price]!
}

# All possible offer types
enum OFFER_TYPE {
  OnDemand
  Reserved
}

# Definition of a price
type Price {
  description: String
  unit: String
  currency: String
  price: Float
}
{{</ highlight >}}

At the very end we define the _queries_ 
Let's start by defining a single query. To make it simple for the purpose of the post, Let's assume that we will try to get a whole _product family_.
If we query the entire product family, we will be able to display all informations of all product in the family. But let's also consider that we want to limit the family and extract only a certain product identified by its SKU.

The Query definition is therefore:
{{< highlight graphql >}}
# root Query type
type Query {
    products(sku: String): [Product]
}
{{</ highlight >}}

We will query products (`{products}`) and it will return a ProductFamily.

## Query

Let's see now how a typical query would look like. To understand the structure of a query, I advise you to read this excellent blog post: [The Anatomy of a GraphQL Query](https://dev-blog.apollodata.com/the-anatomy-of-a-graphql-query-6dffa9e9e747#.jbklz6h17).

{{< highlight graphql >}}
{
  ProductFamily {
    products {
      location
      type
    }
  }
}
{{</ highlight >}}

This query should normally return all the products of the family and display their location and their type.
Let's try to implement this

# Geek time: let's go!

I will use the `go` implementation of GraphQL which is a "simple" translation in go of the [javascript's reference implementation](https://github.com/graphql/graphql-js).

To use it: 

{{< highlight go >}}
import "github.com/graphql-go/graphql"
{{</ highlight >}}

To keep it simple, I will load all the products and offers in memory. In the real life, we should implement an access to whatever database. But that is a strength of the GraphQL model: The flexibility. The backend can be changed later without breaking the model or the API.

## First pass: Only the products

### Defining the schema and the query in go

Most of the work has already been done and documented in a series of blog posts [here](http://mycodesmells.com/post/building-graphql-api-in-go)

First we must define a couple of things:

* A _Schema_ as returned by the function `graphql.NewSchema` that takes as argument a `graphql.SchemaConfig`
* The `graphql.SchemaConfig` is a structure composed of a `Query`, a `Mutation` and other alike fields which are pointers to `graphql.Object`
* The rootQuery is created by the structure `graphql.ObjectConfig` in which we pass an object of type `graphql.Fields` (which is a `map[string]*Field`)

The code to create the schema is the following:
{{< highlight go >}}
fields := graphql.Fields{}
rootQuery := graphql.ObjectConfig{Name: "RootQuery", Fields: fields}
schemaConfig := graphql.SchemaConfig{
       Query: graphql.NewObject(rootQuery),
}
schema, err := graphql.NewSchema(schemaConfig)
{{</ highlight >}}

### Defining the fields

Our shema is created but nearly empty because we did not filled the "fields" variable.
the fields variable will contain what the user can request.

As seen before, fields is a map of `*Field`. The key of the map is the root query. In our definition of the Query, we declared that the query would be "products". So "products" is the key of the map.
The graphql.Field that is returned is a list type composed of productTypes.

{{< highlight go >}}
fields := graphql.Fields{
        "products": &graphql.Field{
             Type: graphql.NewList(productType),
        ...
{{</ highlight >}}

We will see in a minute how to define the _productType_. Before, we must provide a way to seek for the product in the database.
This is done by implementing the `Resolve` function:

{{< highlight go >}}
fields := graphql.Fields{
        "products": &graphql.Field{
             Type: graphql.NewList(productType),
             Resolve: func(p graphql.ResolveParams) (interface{}, error) {
        ...
{{</ highlight >}}

The resolv function will return all the products in our database.

But wait... In the Query definition, we said that we wanted to be able to limit the product by setting a sku in the query.

To inform our schema that it can handle a we add the `Args` field to the `graphql.Field` structure:


{{< highlight go >}}
fields := graphql.Fields{
        "products": &graphql.Field{
              Type: graphql.NewList(productType),
              Args: graphql.FieldConfigArgument{
                      "sku": &graphql.ArgumentConfig{
                                Type: graphql.String,
                      },
              },
              Resolve: func(p graphql.ResolveParams) (interface{}, error) {
        ...
{{</ highlight >}}

as the argument is not mandatory, we will use an if statement in the Resolve function to check whether we have a sku or not:

{{< highlight go >}}
if sku, skuok := p.Args["sku"].(string); skuok {
{{</ highlight >}}

### Defining the _productType_

To be able to query display the information of the product (and query the fields), we must define the productType as a graphql object.
This is done like this:

{{< highlight go >}}
var productType = graphql.NewObject(graphql.ObjectConfig{
        Name: "Product",
        Fields: graphql.Fields{
                "location": &graphql.Field{
                        Type: graphql.String,
                },
                "sku": &graphql.Field{
                        Type: graphql.String,
                },
                "operatingSystem": &graphql.Field{
                        Type: graphql.String,
                },
                "instanceType": &graphql.Field{
                        Type: graphql.String,
                },
        },
})
{{</ highlight >}}

A productType is a graphql object composed of the 4 fields. Those fields will be returned as string in the graphql.

### Querying

I will not implement a webservice to query my schema by now. This can easily be done with some handlers that are part of the project.
I will use the same technique as found on internet: I will put the query as argument to my cli.

Assuming that `query` actually holds my my graphql request, I can query my schema by doing:

{{< highlight go >}}
params := graphql.Params{Schema: schema, RequestString: query}
r := graphql.Do(params)
if r.HasErrors() {
    log.Fatalf("Failed due to errors: %v\n", r.Errors)
}
{{</ highlight >}}

### A couple of tests...
    ./pricing -db bla -query "{products(sku:\"HZC9FAP4F9Y8JW67\"){location}}" | jq "."
{{< highlight json >}}
{
  "data": {
    "products": [
      {
        "location": "US East (N. Virginia)"
      }
    ]
  }
}
{{</ highlight >}}
     
    ./pricing -db bla -query "{products(sku:\"HZC9FAP4F9Y8JW67\"){location,instanceType}}" | jq "."
{{< highlight json >}}
{
  "data": {
    "products": [
      {
        "location": "US East (N. Virginia)",
        "instanceType": "t2.micro"
      }
    ]
  }
}
{{</ highlight >}}

    ./pricing -db bla -query "{products{location}}" | jq "." | head -15
{{< highlight json >}}
{
  "data": {
    "products": [
      {
        "location": "US East (Ohio)"
      },
      {
        "location": "EU (Frankfurt)"
      },
      {
        "location": "EU (Frankfurt)"
      },
      {
        "location": "Asia Pacific (Sydney)"
      },

{{</ highlight >}}

    ./pricing -db bla -query "{products{location,operatingSystem}}" | jq "." | head -20
{{< highlight json >}}
{
  "data": {
    "products": [
      {
        "operatingSystem": "Windows",
        "location": "Asia Pacific (Sydney)"
      },
      {
        "operatingSystem": "Windows",
        "location": "AWS GovCloud (US)"
      },
      {
        "operatingSystem": "Windows",
        "location": "Asia Pacific (Mumbai)"
      },
      {
        "operatingSystem": "SUSE",
        "location": "US East (N. Virginia)"
      },
{{</ highlight >}}

## Adding the Offers

To add the offer, we should first define a new offerType

{{< highlight go >}}
var offerType = graphql.NewObject(graphql.ObjectConfig{
        Name: "Offer",
        Fields: graphql.Fields{
                "type": &graphql.Field{
                        Type: graphql.String,
                },
                "code": &graphql.Field{
                        Type: graphql.String,
                },
                "LeaseContractLenght": &graphql.Field{
                        Type: graphql.String,
                },
                "PurchaseOption": &graphql.Field{
                        Type: graphql.String,
                },
                "OfferingClass": &graphql.Field{
                        Type: graphql.String,
                },
        },
})
{{</ highlight >}}

And then make the productType aware of this new type:

{{< highlight go >}}
var productType = graphql.NewObject(graphql.ObjectConfig{
        Name: "Product",
        Fields: graphql.Fields{
                "location": &graphql.Field{
                        Type: graphql.String,
                },
                "sku": &graphql.Field{
                        Type: graphql.String,
                },
                "operatingSystem": &graphql.Field{
                        Type: graphql.String,
                },
                "instanceType": &graphql.Field{
                        Type: graphql.String,
                },
                "offers": &graphql.Field{
                        Type: graphql.NewList(offerType),
                },
        },
})
{{</ highlight >}}

Then, make sure that the resolv function is able to fill the structure of the product with the correct offer.

### Testing:

    ./pricing -db bla -query "{products(sku:\"HZC9FAP4F9Y8JW67\"){location,instanceType,offers{type,code}}}" | jq "."
{{< highlight json >}}
{
  "data": {
    "products": [
      {
        "offers": [
          {
            "type": "OnDemand",
            "code": "JRTCKXETXF"
          }
        ],
        "location": "US East (N. Virginia)",
        "instanceType": "t2.micro"
      }
    ]
  }
}
{{</ highlight >}}

This is it!

# Conclusion

I didn't document the prices, but it can be done following the same principles.

Graphql seems really powerful. Now that I have this little utility, I may try (once more) to develop a little react frontend or a _GraphiQL_ UI.
What I like most is that it has forced me to think in graph instead of the traditional relational model.

The piece of code is on [github](https://github.com/owulveryck/graphql-test)

**edit**: I have included a graphiql interpreter for testing. It works great. Everything is on github:

![GraphiQL](/assets/images/graphiql.png)
+++
images = ["/2016/10/image.jpg"]
description = ""
categories = ["category"]
tags = ["tag1", "tag2"]
draft = true
+++

---
categories:
date: 2017-09-12T13:28:36+02:00
description: "This is a second part of the last article. I now really dig into Terraform. This article will explain how to use the Terraform sub-packages in order to create a brand new binary that acts as a gRPC server instead of a cli."
draft: false
images:
- https://nhite.github.io/images/logo.png
tags:
title: Terraform is hip... Introducing Nhite
---

In a previous post, I did some experiments with gRPC, protocol buffer and Terraform.
The idea was to transform the "Terraform" cli tool into a micro-service thanks to gRPC.

This post is the second part of the experiment. I will go deeper in the code and see if it is possible
to create a brand new utility, without hacking Terraform. The idea is to import some packages that compose the binary
and create my own service based on gRPC.

# The Terraform structure

Terraform is a binary utility written in `go`.
The `main` package resides in the root directory of the `terraform` directory.
As usual with go projects, all other subdirectories are different modules.

The whole business logic of Terraform is coded into the subpackages. The "`main`" package is simply an enveloppe for kick-starting the utility (env variables, etc.) and to initiate the command line.

### The cli implementation

The command line flags are instantiated by Mitchell Hashimoto's cli package.
As explained in the previous post, this cli package is calling a specific function for every action.

### The _command_ package

Every single action is fulfilling the `cli.Command` interface and is implemented in the [`command`](https://godoc.org/github.com/hashicorp/terraform/command) subpackage.
Therefore, every "action" of Terraform has a definition in the command package and the logic is coded into a `Run(args []string) int` method (see the [doc of the Command interface for a complete definition](https://godoc.org/github.com/mitchellh/cli#Command).

# Creating a new binary

The idea is not to hack any of the packages of Terraform to allow an easier maintenance of my code. 
In order to create a custom service, I will instead implement a new utility; therefore a new `main` package.
This package will implement a gRPC server. This server will implement wrappers around the functions declared in the `terraform.Command` package.

For the purpose of my POC, I will only implement three actions of Terraform:

* `terraform init`
* `terraform plan`
* `terraform apply`

## The gRPC contract

In order to create a gRPC server, we need a service definition.
To keep it simple, let's consider the contract defined in the previous post ([cf the section: Creating the protobuf contract](https://blog.owulveryck.info/2017/09/02/from-command-line-tools-to-microservices---the-example-of-hashicorp-tools-terraform-and-grpc.html#creating-the-protobuf-contract)).
I simply add the missing procedure calls:

{{< highlight protobuf >}}
syntax = "proto3";

package pbnhite;

service Terraform {
    rpc Init (Arg) returns (Output) {}
    rpc Plan (Arg) returns (Output) {}
    rpc Apply (Arg) returns (Output) {}
}

message Arg {
    repeated string args = 2;
}

message Output {
    int32 retcode = 1;
    bytes  stdout = 2;
    bytes stderr = 3;
}
{{</ highlight >}}

## Fulfilling the contract

As described previously, I am creating a `grpcCommand` structure that will have the required methods to fulfill the contract:

{{< highlight go >}}
type grpcCommands struct {}

func (g *grpcCommands) Init(ctx context.Context, in *pb.Arg) (*pb.Output, error) {
    ....
}
func (g *grpcCommands) Plan(ctx context.Context, in *pb.Arg) (*pb.Output, error) {
    ....
}
func (g *grpcCommands) Apply(ctx context.Context, in *pb.Arg) (*pb.Output, error) {
    ....
}
{{</ highlight >}}

In the previous post, I have filled the `grpcCommand` structure with a `map` filled with the command definition.
The idea was to keep the same CLI interface.
As we are now building a completely new binary with only a gRPC interface, we don't need that anymore.
Indeed, there is still a need to execute the `Run` method of every Terraform command.

Let's take the example of the Init command. 

Let's see the definition of the command by looking at the [godoc](https://godoc.org/github.com/hashicorp/terraform/command#InitCommand):

{{< highlight go >}}
type InitCommand struct {
    Meta
    // contains filtered or unexported fields
}
{{</ highlight >}}

This command holds a substructure called `Meta`. `Meta` is defined [here](https://godoc.org/github.com/hashicorp/terraform/command#Meta) and holds _the meta-options that are available on all or most commands_. Obviously we need a Meta definition in the command to make it work properly.

For now, let's add it to the `grpcCommand` globally, and we will see later how to implement it.

Here is the gRPC implementation of the contract:

{{< highlight go >}}
type grpcCommands struct {
    meta command.Meta
}

func (g *grpcCommands) Init(ctx context.Context, in *pb.Arg) (*pb.Output, error) {
    // ...
    tfCommand := &command.InitCommand{
        Meta: g.meta,
    }
    ret := int32(tfCommand.Run(in.Args))
    return &pb.Output{ret, stdout, stderr}, err
}
{{</ highlight >}}

## How to initialize the _grpcCommand_  object

Now that we have a proper `grpcCommand` than can be registered to the grpc server, let's see how to create an instance.
As the grpcCommand only contains one field, we simply need to create a `meta` object.

Let's simply copy/paste the code done in Terraform's main package and we now have:

{{< highlight go >}}
var PluginOverrides command.PluginOverrides
meta := command.Meta{
    Color:            false,
    GlobalPluginDirs: globalPluginDirs(),
    PluginOverrides:  &PluginOverrides,
    Ui:               &grpcUI{},
}
pb.RegisterTerraformServer(grpcServer, &grpcCommands{meta: meta})
{{</ highlight >}}

According to the comments in the code, the `globalPluginDirs()` _returns directories that should be searched for
globally-installed plugins (not specific to the current configuration)_.
I will simply copy the function into my main package

## About the UI

In the example CLI that I developed in the previous post, what I did was to redirect stdout and stderr to an array of bytes, in order to capture it and send it back to a gRPC client.
I noticed that this was not working with Terraform.
This is because of the UI!
UI is an interface whose purpose is to get the output stream and write it down to a specific io.Writer.

Our tool will need a custom UI.

### A custom UI

As UI is an interface ([see the doc here](https://godoc.org/github.com/mitchellh/cli#Ui)), it is easy to implement our own. Let's define a structure that holds two array of bytes called `stdout` and `stderr`. Then let's implement the methods that will write into these elements:

{{< highlight go >}}
type grpcUI struct {
    stdout []byte
    stderr []byte
}

func (g *grpcUI) Output(msg string) {
    g.stdout = append(g.stdout, []byte(msg)...)
}
{{</ highlight>}}

_Note 1_: I omit the methods `Info`, `Warn`, and `Error` for brevity.

_Note 2_: For now, I do not implement any logic into the `Ask` and `AskSecret` methods. Therefore, my client will not be able to ask something. But as gRPC is bidirectional, it would be possible to implement such an interaction.

Now, we can instantiate this UI for every call, and assign it to the meta-options of the command:

{{< highlight go >}}
var stdout []byte
var stderr []byte
myUI := &grpcUI{
    stdout: stdout,
    stderr: stderr,
}
tfCommand.Meta.Ui = myUI
{{</ highlight >}}

So far, so good: we now have a new Terraform binary, that is working via gRPC with a very little code.

# What did we miss?

## Multi-stack
It is fun but not usable for real purpose because the server needs to be launched from the directory where the _tf_ files are... 
Therefore the service can only be used for one single Terraform stack... Come on!

Let's change that and pass as a parameter of the RPC call the directory where the server needs to work. It is as simple as adding an extra argument to the `message Arg`:

{{< highlight protobuf >}}
message Arg {
    string workingDir = 1;
    repeated string args = 2;
}
{{</ highlight >}}

and then, simply do a `change directory` in the implementation of the command:

{{< highlight go >}}
func (g *grpcCommands) Init(ctx context.Context, in *pb.Arg) (*pb.Output, error) {
    err := os.Chdir(in.WorkingDir)
    if err != nil {
        return &pb.Output{int32(0), nil, nil}, err
    }
    tfCommand := &command.InitCommand{
        Meta: g.meta,
    }
    var stdout []byte
    var stderr []byte
    myUI := &grpcUI{
        stdout: stdout,
        stderr: stderr,
    }
    ret := int32(tfCommand.Run(in.Args))
    return &pb.Output{ret, stdout, stderr}, err
}
{{</ highlight >}}

## Implementing a new _push_ command

I have a Terraform service. Alright.
Can an "Operator" use it?

The service we have deployed is working exactly like Terraform. I have only changed the user interface.
Therefore, in order to deploy a stack, the 'tf' files must be present locally on the host.

Obviously we do not want to give access to the server that hosts Terraform. This is not how micro-services work.

Terraform has a push command that Hashicorp has implemented to communicate with Terraform enterprise.
This command is linked with their close-source product called "Atlas" and is therefore useless for us.

Let's take the same principle and implement our own _push_ command.

### Principle

The push command will zip all the `tf` files of the current directory in memory, and transfer the zip via a specific message to the server.
The server will then decompress the zip into a unique temporary directory and send back the ID of that directory.
Then every other Terraform command can use the id of the directory and use the stack (as before).

Let's implement a protobuf contract:

{{< highlight protobuf >}}
service Terraform {
    // ...
    rpc Push(stream Body) returns (Id) {}
}

message Body {
    bytes zipfile = 1;
}

message Id {
    string tmpdir = 1;
} 
{{</ highlight >}}

_Note_: By now I assume that the whole zip can fit into a single message. I will probably have to implement chunking later

Then instantiate the definition into the code of the server:

{{< highlight go >}}
func (g *grpcCommands) Push(stream pb.Terraform_PushServer) error {
    workdir, err := ioutil.TempDir("", ".terraformgrpc")
    if err != nil {
    return err
    }
    err = os.Chdir(workdir)
    if err != nil {
    return err
    }

    body, err := stream.Recv()
    if err == io.EOF || err == nil {
        // We have all the file
        // Now let's extract the zipfile
        // ...
    }
    if err != nil {
        return err
    }
    return stream.SendAndClose(&pb.Id{
            Tmpdir: workdir,
    })
}
{{</ highlight >}}

# going further...

The problem with this architecture is that it's stateful, and therefore easily scalable.

A solution would be to store the zip file in a third party service, identify it with a unique id.
And then call the Terraform commands with this ID as a parameter. 
The Terraform engine would then grab the zip file from the third party service if needed and process the file

## Implementing a micro-service of backend

I want to keep the same logic, therefore the storage service can be a gRPC microservice.
We can then have different services (such as s3, google storage, dynamodb, NAS, ...) written in different languages.

The Terraform service will act as a client of this "backend" service (take care, it is not the same backend as the one defined within Terraform).

Our Terraform-service can then be configured in runtime to call the host/port of the correct backend-service. We can even imagine the backend address being served via consul.

This is a work in progress and may be part of another blog post.

# Hip[^1] is _cooler than cool_: Introducing _Nhite_

[^1]: [hip definition on wikipedia](https://en.wikipedia.org/wiki/Hip_(slang))

I have talked to some people about all this stuff and I feel that people are interested.
Therefore, I have set up a GitHub organisation and a GitHub project to centralize what I will do around that.

The project is called Nhite.

* The GitHub organization is called [nhite](https://github.com/nhite)
* The web page is [https://nhite.github.io](https://nhite.github.io)

There is still a lot to do, but I really think that this could make sense to create a community. It may give a product by the end, or go in my attic of dead projects.
Anyway, so far I've had a lot of fun!
---
author: Olivier Wulveryck
date: 2016-07-29T09:51:12+02:00
description: description
draft: true
keywords:
- key
- words
tags:
- one
- two
title: hmm explained
topics:
- topic 1
type: post
---

It's been about a year that I started to read the __tutorial on Hidden Markov Models and selected applications in speech recognition__ from Lawrence R. Rabiner.

This paper is a reference in term of HMM application. The problem was that even with a lot of effort and concentration, I was lacking a bunch of
skills to fully understand it.
As I've now studied a bit, with the support of Coursera and the lectures of Andrew Ng, I will try to dig into it again.

This post may not be published, but I think it may be useful to me.
Actually, rephrasing, explaining and trying to argument the publication from Rabiner will be, according to me, the best way to understand it.



---
author: Olivier Wulveryck
date: 2016-10-17T20:50:18+02:00
description: How UDT (UDP-based Data Transfer Protocol) can be used with HTTP to tranfer files between two AWS instances hosted in different regions.
draft: false
keywords:
- udt
- http
- aws
tags:
- udt
- http
- golang
title: HTTP over UDT for inter-region file transfer
topics:
- UDT
type: post
---

# Introduction

Transferring files between server is no big deal with nowadays network equipments.
You use `rsync`, `scp` or even `http` to get a file from A to B.

Of course, you rely on the TCP stack so you have a decent reliability in the transport.

But TCP has its drawback, especially when it needs to go through a lot of equipments. Typically in the cloud, or over a VPN.

To prevent the drawbacks of the TCP protocol, there are several solutions:

* Use UDP, but UDP by itself is not "reliable"
* Develop another layer 4 protocol, but it cannot be done in a pure user space. You need to develop a system driver. It cannot be easily done on a large scale.
* Use UDP and another framework on top of UDP.

## Layer 4: UDP? 

Yes, UDP, but with an "extra" layer. I've had the opportunity to try three of them.

* Quic by Google
* FASP by ASPERA
* UDT by Dr GU.

### Quic


First Google, along with its [quic](https://en.wikipedia.org/wiki/QUIC) protocol, tries to enhance the user experience. Actually, a quic implementation is already present in chrome and within google web servers. I've heard about quic at the [dotGo](https://dotgo.eu); [Lucas Clemente](https://github.com/lucas-clemente) has presented its work in progress of a quic implementation in GO.

I've tried it, but it lacks a client part by now, and the [quic tools](https://www.chromium.org/quic/playing-with-quic) from chromium are far from being usable in a production environment.

### Aspera's FASP

Aspera has its own protocol. It is based on UDP. I've seen it running, and yes, it simply works!
The problem is that it is not open source and a bit expensive.

### The UDT protocol

The UDT protocol is described by ASPERA as its main competitor [here](http://asperasoft.com/fileadmin/media/Asperasoft.com/Resources/White_Papers/fasp_Critical_Technology_Comparison_AsperaWP.pdf).
It's open source and worth the try.
It's the one I will use for my tests.
The code is distributed as a C++ library, but it exists GO bindings.

## The Layer 7: HTTP

To actually transfer a file, I can use the `udtcat` tool provided in the github of go-udtwrapper. 
It is ok for a test, but I won't be able to serve multiple files, to resume a transfer etc... So I need a layer 7 protocol.
HTTP is, according to me, a good choice.

# The implementation in GO

Implementing a simple client/server http-over-udt in go is relatively easy. The HTTP is interfaced in a way that the transport can be easily changed.
Therefore, no need to reimplement a complete HTTP stack; GO has all I need in its standard library.

<center>
![/assets/images/save-princess-go.jpg](/assets/images/save-princess-go.jpg)

https://toggl.com/programming-princess
</center>

I will use this fork of [go-udtwrapper](github.com/Lupus/go-udtwrapper) which seems to be the most up-to-date.

## The server

Implementing a basic http server over UDT is very easy.

The [Serve function](https://golang.org/pkg/net/http/#Serve) from the http package takes a `net.Listener` as argument.
The `udt.Listen` function implements the [net.Listener](https://golang.org/pkg/net/#Listener) interface.

Therefore we can simply use this code to serve HTTP content via the DefaultMuxer over udt:

```go
ln, _ := udt.Listen("udt", config.Addr)
http.Serve(ln, nil)
```

A full implementation that serves local file is simply done by:

{{< gist owulveryck 6a44885c2b3527159f496c21071ab8df "server.go" >}}

## The client

The `http.Client`'s DefautTransport relies on TCP.
Therefore we must completely rewrite a Transport to use UDT.

The Transport entry of the Client implements the RoundTripper interface.

The key point is to write a client transport for UDT that implements the RoundTripper interface.

### The http.RoundTripper interface

Here is an example of an implementation:

```go 
// UdtClient implements the http.RoundTripper interface
type udtClient struct{}

func (c udtClient) RoundTrip(r *http.Request) (*http.Response, error) {
      d := udt.Dialer{}
      conn, err := d.Dial("udt", r.URL.Host)
      if err != nil {
          return nil, err
      }
      err = r.Write(conn)
      if err != nil {
          return nil, err
      }
      return http.ReadResponse(bufio.NewReader(conn), r)
}
```

### The full client code

A simple client that will perform a GET operation on our server would be:

{{< gist owulveryck 6a44885c2b3527159f496c21071ab8df "client.go" >}}

### Building the tools
As we rely on CGO, to do a static compilation, we must use the extra flags: `go build --ldflags '-extldflags "-static"'`.

# Conclusion

This is a very basic implementation of http over UDT.
I have developed a more complete tool for my client, but it cannot be published in open source.

Among the things that I have done there are:

* Gzip compression
* Partial content for resuming a broken download (with http.ServeContent)
* SHA256 checking at the end of the transport
* an HTTP middleware (Rest API) to query the transfer states, rates and efficiency via the PerfMon interface

What's not done yet:

* TLS and mutual authentication
* good benchmarks to actually measure the performances of UDT.
* Downloading chunks to optimize the speed of transfer and the bandwith usage
* maybe a POST method to upload a file in multipart
---
author: Olivier Wulveryck
date: 2016-06-23T15:32:54+02:00
description: Playing with websocket for a dynamic presentation.
draft: false
tags:
- websocket
- bootstrap
- golang
- revealjs 
- Javascript
- D3js
title: Websockets, Reveal.js, D3 and GO for a dynamic keynote
topics:
- topic 1
type: post
---

# the goal
As all my peers, I have the opportunity to talk about different technological aspects.
As all my peers, I'm asked to present a bunch of slides (powerpoint or keynote, or whatever).

In this post I won't dig into what's good or not to put in a presentation, and if that's what interest you, I 
recommend you to take a look at [Garr Reynold's tips and tricks](http://www.presentationzen.com/).

_Steve Jobs_ said:

> People who knows what they're talking about don't need PowerPoint

(actually it has been quoted in Walter Isaacson's biography see [this reference](http://blog.jgc.org/2011/11/people-who-know-what-theyre-talking.html)).

As an attendee I tend to agree; usually PowerPoints are boring and they hardly give any interest besides for the writer to say "hey look, I've worked for this presentation".

Indeed, they are a must. So for my next presentation I thought: 

wouldn't it be nice to use this wide display area to make the presentation more interactive.
One of the key point in communication is to federate people. So what if people could get represented for real in the presentation.

## how to: the architecture 

Obviously I cannot use conventional tools, such as PowerPoint, Keynote, Impress, google slides and so.
I need something that I can program; something that can interact with a server, and something that is not a console so I can get
fancy and eye-candy animations.

### The basic

[reveal.js](http://lab.hakim.se/reveal-js/) is an almost perfect candidate:

* it is a framework written in JavaScript therefore, I can easily ass code
* it's well designed
* it can be used alongside with any other JavaScript framework

### Graphs, animations, etc...

A good presentation has animations, graphs, diagrams, and stuffs that cannot be expressed simply with words.
I will interact with the audience. I will explain how later, but anyway they will send me some data.
I could process them in whatever server-side application (php, go-template base, python) but I have the feeling that's not 
the idiomatic way of doing modern web content. Actually, I would need anyway to deal with device (mobile, desktop), screen size,
browser... So what's best, I think, is to get the data on the client side and process it via Javascript.

[Data Driver Documents](https://d3js.org/) is the framework I will use to process and display the data I will get from the client.

It actually uses SVG to represent the graphs; I would have liked to use a full HTML5 to be more... 2016, but the D3 is actually very very good 
framework I wanted to use for a while.

### The attendees 

If I want the attendees to participate they need a device, to act as a client.
About all people I will talk to have a smartphone; that is what I will use. 

It has two advantages:

* it is their own device, I looks more realistic and unexpected: therefore I would get a better reception of the message I'm trying to pass.
* it usually has a Webkit based web browser with a decent Javascript engine.

I won't develop a native app, instead I will a webpage mobile compliant based on the [bootstrap](http://getbootstrap.com/) framework.

### The HUB

The point now, is how to make my clients and my presentation to exchange data.
As I said before, I would not be an easy task to setup a pure browser based peer-to-peer communication, so I will fall 
back to the traditional web server based hub.

the first idea is to use a RESTfull mechanism, but this has the major disadvantage of not being real-timed.
What I would like is a communication HUB that would broadcast events as soon as they are reveived.

I've implemented a server in go to do so. The clients will talk to the server over websockets which are now natively present in every
modern browsers.

#### the server

I've used the [Implementation from gorilla](https://github.com/gorilla/websocket) because it seemed to be the best as of today.
It implements all the RFC and the development is up-to-date.

The code heavily relies on channels to broadcast the messages between the different peers.
 I've taken the chat example present in the gorilla's package.

At first I did code all the mechanism is a simple go package. After a bunch of code, I've decided to split the code into two different
projects: The main presentation and the [gowmb](http://github.com/owulveryck/gowmb). The gowmb package is usable in others projects.

# Conclusion.

I don't go into the implementation details in this post, instead I will refer to the [github](https://github.com/owulveryck/topology-presentation)
repository where the presentation is hosted.

By now I have a good animated slideshow, and the ability to join the slides with a mobile phone.
I can also draw a topology of the attendees via D3 and interact with them.
---
date: 2017-08-14T09:42:12+02:00
description: "This is the story of a lambda person that had his iPhone broken after an update... And then the story of a geek, who has been told that he needed to pay 350 for a replacement, based on assumptions and lies."
draft: true
images:
- /assets/images/iphone-blog-logo.png
title: When I apply a 350 update on the iPhone
---

I am an apple client for years now. I was very pleased with the apple customer care until the day I really needed something from them.

This post is the "why". But as you are on a technical blog the idea behind this post is:

* to explain the reason of wrath that made me dig into the deep understanding of my phone and
* to give technical facts and hints to any geek that would like to investigate an iPhone issue based on what I learned (this is, according to me, the most interesting part)

# Part I:  The facts

I have add an iPhones, iPads and MacBook. I made this choice for different reasons amongst:

* the quality of the products
* the very user friendly interface
* the security of the phone
* the implicit contract that apple provides: "nevemind, apple will take care"

Of course, those reasons are personals and can be argued (and subject to _troll_), but this is not the point of this article.

## Updating the iOS

Apple releases updates of its iOS several times a year. As a geek, I usually read the changelog and the forums to see if it's worth applying the update as soon as it is out.
On July the 19th, Apple released iOS 10.3.3 and the changelog mentioned several security issues that were corrected with this version.
An after all, I was on holiday, and it was definitely not a good moment to check, analyse and do an update.

When I came home, I planned an update campaign at home. As usual, I start by my own phone, then I update my wife's iPhone and iPad.
This is a trick I use to avoid any inflexion in the [Wife Acceptance Factor](https://en.wikipedia.org/wiki/Wife_acceptance_factor). 
If I notice something wrong on my phone, the update campaign is stopped...

Anyway, when I updated successfuly my own phone, I proceed as planed.
I triggered the update of my wife's iPhone 6 and I left home to go to work.

At 9:30 I received a mail:

```
From: Darling
To: me 
Subject: Phone's not working

Contact me via email
```

I triggered `hangout` so we could chat and asked what was going on. She told me that the phone was searching for the carrier.
I asked her to do simple manipulations:

<center>
![IT helpdesk](/assets/images/have-you-tried.jpg)
</center>

Nothing worked.

In the evening I tried to reinstall the iOS. Then I tried to downgrade. I also tried to install a beta version of iOS11... Nothing worked.

I asked "google". I noticed that this problem was common. Moreover, it could happen after any version update. 
So I went on twitter, I found two people with the exact same problem and I asked them how they solved it:

<center>
{{< tweet 893320425534619648 >}}
</center>

Ok, so I contacted the support via twitter, and they gave me a phone appointment with an advisor. So far, so good...
We did a couple of manipulations such as _turning it off and on again_ 

They made me reinstall the software (again) and nothing worked. They say that I should go to an Apple store for a further diagnostic. I thought I could just walk into an Apple Store with my phone but NO! You need an appointment to meet an advisor in what they call "_The genius bar_". The genius bar... I was so excited to finally meet a genius!

<center>
i![genius](/assets/images/genius.jpg)
</center>

# Part II: Apple (does not) Care

I took the appointment over lunch time. I arrived on time, but due to a misunderstanding, they told me that I was 20 minutes too early... Indeed I have waited and an advisor finally arrived with a macbook. He told me:

"What I will do is to reset your phone and install the ios again"

I told him that has already been done, but he replied: this is the procedure, and it will only take a few minutes here.
10 minutes later, the iphone is restored and the problem was still there.

Then he simply told me:

"_hardware problem. Your phone is out of warranty, but I have a solution for you. For only 350, you can have a new phone that will have a 3 months warranty!_"

I replied that this was ridiculous because my phone was working perfectly before the update. Therefore I refuse to pay this fee. So he told me that I should probably talk to a supervisor. 

15 minutes later a supervisor came... (it was already 1:30pm, and I was late for work).
He told me that AppleCare did not order a phone replacement and that there was nothing he could do. So I ask:

"_Can I call AppleCare back, so they may tell you to give me a new phone?_" (you know, I was still at the time convinced that Apple was customer centric...)

So he gave me a phone with a direct line to a supervisor at AppleCare.  This gentleman told me: 

"_Your phone is broken, and the hardware problem cannot be linked with the software update in any way. Nobody has ever seen a software that could break a hardware_"

I replied that, in my experience, it is something that can happen, and I took as the example of an [EPROM](https://en.wikipedia.org/wiki/EPROM). A bad update simply breaks it. And, even if I didn't know the iPhone architecture, I thought that similar updates were done in some components of the phone.
But anyway, nobody has given me any proof of a hardware problem. The problem can be software related.

Well, we argued for 35 minutes. At the very end he killed me by saying:

_Anyway, I won't do any exception and I will not replace your phone... But even if I wanted to (it is 3 clicks for me), by now you are in an Apple Store and only a supervisor of the store can take such a decision_

So who is lying... Actually, I didn't care, I asked to talk to another supervisor. That was more than 2 hours that I was in the shop. All I wanted was a solution or at least a real diagnose of my phone that could tell me which piece was broken and why...

I went back home, disappointed... I looked over the internet for a solution because I was not in the exception list and Apple would not do me a courtesy. 

In fact, both people from Apple Care and from the Apple Store told me that they could be exceptions. Of course, they did not want to tell me the reasons.
In a similar situation, some people reported that their [iPhone has been changed for free](https://discussions.apple.com/message/31836225#message31836225). Taking different treatment depending on who is asking heavily sounds like discrimination to me: 

<center>
![discrimination](/assets/images/discrimination.png)
</center>

Anyway, let's move on and "seek for the truth". And check whether there is really a broken part in my phone.

# Part III: Geek (at last!)

__Disclaimer__: By now, I am only playing with official tools. I am waiting for an answer from Apple. If they still refuse to change the phone, I may jailbreak it to do further investigations. I am blogging this because what took me hours to understand may help someone with a similar problem. As of today, I do not have any solution nor I have found the problem.

[Baseband device](https://www.theiphonewiki.com/wiki/Baseband_Device)

The qualcomm modem is a [MDM9625](https://www.theiphonewiki.com/wiki/MDM9635)

[https://developer.apple.com/bug-reporting/profiles-and-logs/](https://developer.apple.com/bug-reporting/profiles-and-logs/)


## Is the SIM card ok

`iPhone CommCenter[75] <Notice>: #N SIM initialization complete; all essential information available #sim-ready`

`misd[31] <Notice>: carrier service is available`


`iPhone CommCenter[75] <Notice>: #I Unbrick device was successful `
The params are displayed with the profile update
` #I Received activation info: <private>       `

`16873.516 [I] evt: Firing event 'recalculateConnectionAvailability': with params= 0, Postponement status change  `
---
author: Olivier Wulveryck
date: 2016-10-10T07:43:05+02:00
description: A very basic implementation of a household Butler as a support for an automation speech.
draft: true
title: J.A.R.V.I.S., show them how go, javascript, and aws ec2 can change the way we do IaaS (and more) (and more)
type: post
---

# introduction

As you may know, if you know me personally of if you read my blog, I'm a big fan of automation.

Automation alongside of machine learning could heavily improve the usage of IT and increase functional perspectives.
Indeed, automation is not a reserved principle of continuous integration; automation is not only targeted to ease the life of operation engineers
or a tool to lower the costs.

Automation (and machine learning) may also be used for everyday life.

Few days ago I had to present some of my work to a team of automation convicted people. People that develop and integrate continuous integration pipes;
other people that does the same with an orchestration engine and people who are the targeted people of the automation process. Those who will really take advantage of a fully automation integration and management pipeline.

Anyway, I wanted to introduce myself simply by showing them my dream of automation.

# The principle

I wanted a demo related to my experience as an operation and infrastructure manager. Therefore the best example would be to ask my "household Butler" to create a virtual machine in the cloud for me, and to setup a DNS to ease the external access.

# The architecture of the Butler

## The ear

## The voice

## The brain

## The "nervous impulsions messages"

# The implementation

## The nervous system

## The ear

## The voice

## The brain

# Conclusion
---
categories:
date: 2017-01-06T21:59:25+01:00
description: "In this post I will explain how to setup a lambda function written in go that will be triggered when an object is dropped on a bucket on s3. The function will read the object and copy it into another bucket iin another location. It will use AWS' golang sdk."
draft: true
images:
- /assets/images/default-post.png
tags:
- aws
- lambda
- s3
- golang
title: lambda inter s3
---

In this post I will explain how to setup a lambda function written in go that will be triggered when an object is dropped on a bucket on s3. The function will read the object and copy it into another bucket iin another location. It will use AWS' golang sdk.

# The Lambda skeleton

AWS Lambda does not support the go programming language natively. Anyway, it as a go program is compiled and packed as a simgle static binary, it it easy to execute a go program whitin a wrapper (node.js or python).
You can find a lot of tutorials on the Internet to do that. That is definitly not my point here.

For my example, I will use a slighlty different approch, based on a new feature of go 1.8: the plugins. I will use this repo to code my lambda function in order to benefit from the "context" object (so I will be able to handle the timeout gracefully).

https://github.com/eawsy/aws-lambda-go-shim

---
author: Olivier Wulveryck
date: 2016-05-20T12:50:59+02:00
description: A post about machine learning and an application on a simple case I've met in my job. 
  Here is the use case \:regarding four different technical solutions, 
  after the evaluation by group of human of different features, can a Bot "think" on its own and evaluate which one offers then best ratio cost/features.
  And therefore, can it tell any manager which solution to choose.
draft: false
tags:
- machine learning
- octave
- linear regression
title: Which solution should I choose? Don't think too much and ask a bot!
topics:
- topic 1
type: post
---

# Let me tell you a story: the why!

A year ago, one of those Sunday morning where spring starts to warm up the souls, I went, as usual to my favorite bakery.
The family tradition is to come back with a bunch of "Pains au Chocolat" (which, are, you can trust me, simply excellent).

- _hello sir, I'd like 4 of your excellent "pains au chocolat" please_
- _I'm sorry, I don't have any "pains au chocolat" nor any "croissant" anymore_
- _what? How is it possible ?_
- _everything has been sold._
- _too bad..._

I think to myself: _why didn't you made more?_. He may have read my thought and told me

- _I wish I could have foreseen_
 
When I left his shop, his words were echoing... I wish I could have foreseen... We have self-driving cars, we have the Internet, 
we are a civilization that is technology advanced. 
Facebook recognize your face among billions as soon as you post a photo... It must be possible to foresee...

This is how I started to gain interest in machine learning

At first I started to read some papers, then I learn (a very little bit) about graph theory, Bayesian networks, Markov chains.
But I was not accurate and I felt I was missing some basic theory.

That's the main reason why, 8 weeks ago, I signed in a course about ["Machine learning" on Coursera](https://www.coursera.org/learn/machine-learning). 
This course is given by [Andrew Ng](http://www.andrewng.org/) from [Stanford University](https://www.stanford.edu/).

It is an excellent introduction that gives me all the tools I need to go deeper in this science. The course is based on real examples
and uses powerful mathematics without going too deeply in the proofs.

# So what?

The course is not finished yet, but after about 8 weeks, I've learned a lot about what we call "machine learning".

The main idea of the machine learning is:

* to feed some code with a bunch of data (who said big data was useless)
* to code or encode some mathematical formulas that could represent the data
* to implement any algorithm that optimize the formulas by minimizing the error made by the machine on the evolving data sets.

To make it simple: machine learning is feeding a "robot" with data and teach him how to analyse the errors so it can make decisions on its own.

Scary isn't it? But so exciting... As usual I won't go into ethical debate on this blog, and I will stick to science and on the benefit
of the science. 

But indeed, always remind Franois Rabelais:

> Science sans conscience n'est que ruine de l'&acirc;me (_Science without conscience is but the ruin of the soul_)

## A use case

### Defining the problem

I have 4 technical solutions providing a similar goal: deliver cloud services.
Actually, none of them is fulfilling all the requirements of my business.
As usual, one is good in a certain area, while another one is weak, etc.

A team of people has evaluated more than 100 criteria, and gave two quotations per criteria and per product:

* the first quotation is in the range 0/3 and indicated whether the product is fulfilling the current feature
* the second quotation may be {0,1,3,9} and points the effort needed to reach a 3 for the feature

Therefore, for each solution, I have a table looking like this :

| feature  name | feature evaluation  | effort |
|---------------|---------------------|--------|
| feature 1     |                   0 |      9 |
| feature 2     |                   3 |      0 |
| feature 3     |                   2 |      1 |
| feature 4     |                   0 |      3 |
| .......       |                ...  |    ... |
| feature 100   |                   2 |      3 |

I've been asked to evaluate the product and to produce a comparison.

To do an analytic, I must look for an element of comparison. So I've turned the problem into this :

__I would like to know which product is the cheapest to fulfill my requirement.__

(I've uploaded my samples here):

* [solution 1](/assets/ml/solution1.csv)
* [solution 2](/assets/ml/solution2.csv)
* [solution 3](/assets/ml/solution3.csv)
* [solution 4](/assets/ml/solution4.csv)

### Finding a solution

In the machine learning, we notice two different fields of application:

* regression
* classification

The classification mechanism would be used to answer a yes/no question; for example: _should I keep solution 1_ ?
The regression mechanism helps us for "predicting". Actually, the goal is to _automatically_ find a mathematical formulae that turns
a set of feature into a result. 

what is a feature, and what's the result?
Let's go back to my _petits pains_ example.

Consider that the baker has made statistics on its production for sunday, and it has taken some events into consideration:

* sunday the 1st: it was raining, I sold only 100 petits Pains
* sunday the 8th: it was sunny, I sold 250 petits Pains
* sunday the 16th: it was sunny, and it was a special day (eg: mother's day): 300 petits Pains
* sunday the 24th: it was cloudy: 150 petits Pains

Here, the baker thinks that its production must be a function of the weather and the calendar; therefore those are the two features.
What ML propose is to tell the baker how many "petits pains" he should make __knowing__ that it is a special day (father's day) and that it 
is partially sunny... 

Back in the context of this post, the goal of the regression would be to find a mathematical function that will tell me the effort needed
for any value, and doing this on the simple basis of the training set I have.

#### The actual score of all the solutions

The first thing to find it the total score of all the 4 solutions.
If I consider $m$ features, the total score of the solution is defined by:

$ score = \frac{1}{m} . \sum_{k=1}^{m} feature_k $ 

What I need now, is to evaluate the effort needed to reach a score of 3 for each solution.
Let's do that.

#### Representing the training set

First, let's plot the training set.
<center>
<img class="img-responsive" src="/assets/images/ml/trainingset.jpg" alt="Training set"/> 
</center>

__note__ the representation is not accurate because there may be several bunk points 

I will use in this post what's called "supervised learning". That means that I will express a skeleton of function and let the machine 
adjust it. (actually this is a very basic and week implementation; a lot more complex examples may be implemented but that's not the purpose of this post)

When I look at the training set representation, I can imagine a line passing by the middle of the plots.
This line may look like this:

<center>
<img class="img-responsive" src="/assets/images/ml/x-1_5.jpg" alt="x^(-1/5)"/> 
</center>

This is actually a representation of the function $ x^{\frac{1}{5}} $

Let's assume that this function may basically fit my example, my goal will be to adapt the function.
assume this equation with two parameters $\theta_0$ and $\theta_1$ that will influence the curve:

$ f(x) = \theta_0 + \theta_1 . x^{\frac{1}{5}} $

Therefore, my goal will be to code something so that the machine will figure out what $\theta_0$ and $\theta_1$  are.

I will use an implementation of an algorithm called [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) for linear regression.
I won't go into the details of this algorithm, as it takes a complete course to be explained.

The implementation is made with [GNU octave](https://www.gnu.org/software/octave/) and the code is available on my [github](https://github.com/owulveryck/linear-regression-example)

## The computation and the result

Here is a figure representing the function for one particular solution:
<center>
<img class="img-responsive" src="/assets/images/ml/trainingset_plot.jpg" alt="Training set with the function"/> 
</center>

We can see that the curve is "under fitting" the data. 
Anyway, let's continue and get the result I want (I will explain later how to perform better):

Here are the computational results:
<pre>
octave:10> compute
Analysing solution1.csv:0.67/3
Running gradient descent...
Theta found by gradient descent: 5.397050 -4.315835 
Prediction for x=0.669291 ; 1.414256
Prediction for x=3 ; 0.020681
Effort (scaled to 10): 2.582105

Analysing solution2.csv:0.96/3
Running gradient descent...
Theta found by gradient descent: 3.178478 -2.451611 
Prediction for x=0.960630 ; 0.746482
Prediction for x=3 ; 0.124430
Effort (scaled to 10): 1.957075

Analysing solution3.csv:0.67/3
Running gradient descent...
Theta found by gradient descent: 2.557847 -2.015334 
Prediction for x=0.669291 ; 0.698031
Prediction for x=3 ; 0.047283
Effort (scaled to 10): 2.544122

Analysing solution4.csv:0.86/3
Running gradient descent...
Theta found by gradient descent: 3.104868 -2.422627 
Prediction for x=0.858268 ; 0.755175
Prediction for x=3 ; 0.086926
Effort (scaled to 10): 2.152261
</pre>

For each solution, I have:

* the score (the first line /3)
* the parameters $\theta$
* a prediction for the actual score, and for a score of 3
* the effort (scale on 10) needed to pass from the actual score to 3

#### Final result 
Here is the final classification of my four solutions:

| Solution   | score | effort |
|------------|-------|--------|
| Solution 2 |  0.96 |   1.95 |
| Solution 4 |  0.86 |   2.15 |
| Solution 3 |  0.67 |   2.54 |
| Solution 1 |  0.67 |   2.58 |

Solution 2 is the cheapest. It's possible to go into further analysis to determine why it's the cheapest, and how the other ones 
may catch up and go back in the race, but again, that is not the purpose of my post.

# Conclusion

This is a simple approach.
Some axis of optimization could be to use a more complex polynomial (eg: $\theta_0+\theta_1.x^\frac{1}{3}+\theta_2.x^\frac{1}{5}$)
or to use a [support vector machine](https://en.wikipedia.org/wiki/Support_vector_machine) with a gaussian kernel for example.

One other optimization would be to add some more features, such as, for example, a score on the importance of a feature (a functional feature).

Machine learning is a wide mathematical and IT area. It is now in everyone's life.
Nowadays we are talking about plateform fully automated, self-healing applications, smart deployements, smart monitoring.
There are already some good implementations of algorithms on the market, but there is a huge place for integration of those tools into
the life of the IT specialist.

Automation has already helped and took the boring job of the IT specialist. Smart automation will go a step further.

---
author: Olivier Wulveryck
date: 2016-02-29T20:55:01+01:00
description: I am convinced that the description of the topology of an application IS the way to handle it.
  It can be used to deploy the application, to control it, and even to cure it.
  I'm now interrested in the ability of the application to be self-aware.
  In this post, I'm trying to organize my ideas about the Markov Model and how it can be
  applied to the concept I'm describing.
  It's not meant to be a tutorial (at all). The primary goal is to organize my own ideas and learn.
  The secondary goal is, to find any other interseted person who'd like to discuss about this idea.
draft: false
tags:
- R
- Markov model
- IA
- Machine learning
- eigenvectors
title: Is there a Markov model hidden in the choreography?
topics:
- topic 1
type: post
---

# Introduction

In my last post I introduced the notion of choreography as a way to deploy an manage application.
It could be possible to implement self-healing, elasticity and in a certain extent
self awareness.

To do so, we must not rely on the _certainty_ and the _determinism_ of the automated tasks.
_Mark Burgess_ explains in his book [in search of certainty](http://http://www.amazon.com/gp/product/1491923075/ref=pd_lpo_sbs_dp_ss_1?pf_rd_p=1944687522&pf_rd_s=lpo-top-stripe-1&pf_rd_t=201&pf_rd_i=1492389161&pf_rd_m=ATVPDKIKX0DER&pf_rd_r=1BRFTEAZ2RRQ8M77MZ0C) that none should consider the command and control anymore.

Actually we grew up with the idea that a computer will do whatever we told him to.
The truth is that it simply don't. If that sounds astonishing to you, just consider the famous bug.
A bug is a little insect that will avoid any programmed behaviour to act as it should.

In a lot of wide spread software, we find _if-then-else_ or _try-catch_ statements.
Of course one could argue that the purpose of this conditional executionis is to deal with different scenarii, which is true, but indeed,
the keyword is _try_...

## Back to the choreography

In the choreography principle, the automation is performed by a set of dancer that acts on their own. Actually, the most logical way
to program it, is to let them know about the execution plan, and assume that everything will run as expected.

What I would like to study is simply that deployement without knowing the deployement plan.
The nodes would try to perform the task, and depending on the event they receive, they learn and enhance their probability of success.

### First problem


First, I'm considering a single node $A$  which can be in three states $\alpha$, $\beta$ and $\gamma$.
Let's $S$ be the set of states such as $S = \left\\{\alpha, \beta, \gamma\right\\}$

#### Actually knowing what's expected

The expected execution is: $ \alpha \mapsto \beta \mapsto \gamma$

therefore, the transition matrix should be:

$$
P=\\begin\{pmatrix\}
0 & 1 & 0 \\\\
0 & 0 & 1 \\\\
0 & 0 & 0
\\end\{pmatrix\}
$$

Let's represent it with GNU-R (see [this blog post](http://www.r-bloggers.com/getting-started-with-markov-chains/) 
for an introduction of markov reprentation with this software)

```r
> library(expm)
> library(markovchain)
> library(diagram)
> library(pracma)
> stateNames <- c("Alpha","Beta","Gamma")
> ExecutionPlan <- matrix(c(0,1,0,0,0,1,0,0,0),nrow=3, byrow=TRUE)
> row.names(ExecutionPlan) <- stateNames; colnames(ExecutionPlan) <- stateNames
> ExecutionPlan
      Alpha Beta Gamma
      Alpha     0    1     0
      Beta      0    0     1
      Gamma     0    0     0
> svg("ExecutionPlan.svg")
> plotmat(ExecutionPlan,pos = c(1,2), 
         lwd = 1, box.lwd = 2, 
         cex.txt = 0.8, 
         box.size = 0.1, 
         box.type = "circle", 
         box.prop = 0.5,
         box.col = "light yellow",
         arr.length=.1,
         arr.width=.1,
         self.cex = .4,
         self.shifty = -.01,
         self.shiftx = .13,
         main = "")
> dev.off()
```
which is represented by:

![Representation](/assets/images/ExecutionPlan.svg)

#### Knowing part of the plan...


Now let's consider a different scenario. I assume now that the only known hypothesis is that we cannot go
from $\alpha$ to $\gamma$ and vice-versa, but for the rest, the execution may refer to this transition matrix:

$
P=\\begin\{pmatrix\}
\frac{1}{2} & \frac{1}{2} & 0 \\\\
\frac{1}{3} & \frac{1}{3} & \frac{1}{3}  \\\\
0 & \frac{1}{2} & \frac{1}{2} 
\\end\{pmatrix\}
$
which is represented this way ![Representation](/assets/images/ExecutionPlan2.svg)

The transition matrix is regular - we can see, for example that $P^2$ contains all non nil numbers:

```r
> ExecutionPlan %^% 2
                Alpha     Beta      Gamma
          Alpha 0.4166667 0.4166667 0.1666667
          Beta  0.2777778 0.4444444 0.2777778
          Gamma 0.1666667 0.4166667 0.4166667
```

Therefore, Makov theorem says that:

* as n approaches infinity, $P^n = S$ where $S$ is a matrix of the form $[\mathbf{v}, \mathbf{v},...,\mathbf{v}]$, where $\mathbf{v}$ being a constant vector
* let $X$ be any state vector, then we have $\lim_{n\to \infty}P^nX = \mathbf{v}$ where $\mathbf{v}$ is a fixed probability vector (the sum of its entries = 1), all whose entries are positives

So we can look for vector $\mathbf{v}$ (also known as the **steady-state vector of the system**) to see if there is a good chance that our _finite state machine_ would converged to the desired state $\gamma$.

### Evaluation of the steady-state vector

Now since $P^{n+1}=P*P^n$ and that both $P^{n+1}$ and $P^n$  approach $S$, we have $S=P*S$. 

Note that any column of this matrix equation gives $P\mathbf{v}=\mathbf{v}$. Therefore, the steady-state vector of a regular Markov chain with transition matrix $P$ is the unique probability vector $\mathbf{v}$ satisfying $P\mathbf{v}=\mathbf{v}$.

To find the steady state vector, we must solve the equation: $P\mathbf{v}=\mathbf{v}$. $\mathbf{v}$ is actually an eigenvector for an eigenvalue $\lambda = 1$.

_Note from [wikipedia](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors)_

> In linear algebra, an eigenvector or characteristic vector of a square matrix is a vector that does not change its direction under the associated linear transformation. 
> In other words: if $v$ is a vector that is not zero, then it is an eigenvector of a square matrix $A$ if $Av$ is a scalar multiple of $v$. i
> This condition could be written as the equation: $ Av = \lambda v$, where $\lambda$ is a scalar known as the eigenvalue or characteristic 
> value associated with the eigenvector $v$

To compute the eigenvector, we should find the solution to the equation $det(A-\lambda I)=0$ where $I$ is the identity matrix. Actually
I don't know how to do it anymore, and I will simply use _R_'s _eigen_ function:

```r
> eigen(ExecutionPlan)
$values
[1]  1.0000000  0.5000000 -0.1666667

$vectors
          [,1]          [,2]       [,3]
          [1,] 0.5773503  7.071068e-01  0.5144958
          [2,] 0.5773503  1.107461e-16 -0.6859943
          [3,] 0.5773503 -7.071068e-01  0.5144958

> ExecutionPlan %^% 15
        Alpha      Beta     Gamma
Alpha 0.2857295 0.4285714 0.2856990
Beta  0.2857143 0.4285714 0.2857143
Gamma 0.2856990 0.4285714 0.2857295
```

Wait, it has found 3 eigenvalues, and one of those equals 1 which is coherent.
But the eigen vector is not coherent at all with the evaluation of the matrix at step 15.

According to [stackoverflow](http://stackoverflow.com/questions/14912279/how-to-obtain-right-eigenvectors-of-matrix-in-r) 
that's because it computes the _right_ eigenvector and what I need is the _left_ eigenvector.

Here is how to evaluate it.

```r
> lefteigen  <-  function(A){
       return(t(eigen(t(A))$vectors))
}
> lefteigen(ExecutionPlan)
               [,1]          [,2]       [,3]
          [1,] 0.4850713  7.276069e-01  0.4850713
          [2,] 0.7071068 -3.016795e-16 -0.7071068
          [3,] 0.4082483 -8.164966e-01  0.4082483
```

We now have the steady vector : $\mathbf{v} = \\begin\{pmatrix\}0.48 \\\\ 0.70 \\\\ 0.40\\end{pmatrix}$

which simply means that according to our theory, our finite state machin will most likely end in state $\beta$.

### Analysis

What did I learn ?
Not that much actually. I've learned that given a transition matrix (a model) I could easily compute the probability of success.
If I consider the finte state machine as the whole automator of deploiement, given the pobability of failure, I can predict
if it's worth continuing the deploiement or not.

Cool, but far away from my goal: I want a distributed application to learn how to deploy, cure, and take care of itself with a single information: 
its topology.

Back to real life, the model I've described in this post could be the observable states of the application (eg: $\alpha = initial$,$\beta = configured$, $\gamma=started$...)

Hence, the states of the components of the application are hidden from the model (and they must remain hidden, as I don't care observing them)

And this is the proper definition of a __hidden markov model (HMM)__.
So yes, there is a Markov model hidden in the choreography!

I shall continue the study and learn how the signals sent from the compenent gives _evidences_ and do influence the Markov Model of my application.

It's a matter of inference, I-maps, Bayesian networks, HMM.... It's about machine learning which is fascinating !



---
author: Olivier Wulveryck
date: 2016-03-22T07:33:42+01:00
description: Second part of the study about machine learning and its application to the choreography
draft: true
keywords:
tags:
- one
- two
title: markov2
topics:
- topic 1
type: post
---

What we do here is online learning because it is not possible to provide a set of data.
Actually, if the expectation is part of predetermined vector of states ($\{start, stop, confgure,...\}) for any topology,
the way to achieve the goal and reach the state __is__ different for every topology (the deployement plan is different).

# the definition of the universe

We have to define the universe in which our machine evolve in order to bound the events that may interact with it.
+++
date = "2015-11-10T08:56:36+01:00"
draft = false
title = "IaaS-like RESTfull API based on microservices"
tags = [
    "Development",
    "ruby",
    "go",
    "API",
    "REST",
    "msgpack",
    "simple-iaas"
]
+++

# Abstracts

Recently, I've been looking at the principles of a middleware layer and especially on how a RESTFULL API could glue different IT services together.

I am reading more and more about the "API economy"

I've also seen this excellent video made by [Mat Ryer](https://www.youtube.com/watch?v=tIm8UkSf6RA&list=PLDWZ5uzn69ezRJYeWxYNRMYebvf8DerHd) about how to code an API in GO and why go would be the perfect language to code such a portal.

The problem I'm facing is that in the organization I'm working for, the developments are heterogeneous and therefore you can find *ruby* teams as well as *python* teams and myself as a *go* team (That will change in the future anyway)
The key point is that I would like my middleware to serve as an entry point to the services provided by the department.

We (as an "ops" team) would then be able to present the interface via, for example, a [swagger](http://swagger.io) like interface, take care of the API and do whatever RPC to any submodule.

# An example: a IAAS like interface

Let's consider a node compute lifecycle.

What I'd like to be able to do is:

* to create a node
* to update a node (maybe)
* to delete a node
* to get the status of the node

## The backend

The backend is whatever service, able to create a node, such as openstack, vmware vcac, juju, ... 
Thoses services usually provide RESTfull API.

I've seen in my experience, that usually, the API are given with a library in a so called "modern language". 
This aim to simplify the development of the clients.
Sometimes this library may also be developed by an internal team that will take care of the maintenance.

## The library

In my example, we will consider that the library is a simple _gem_ file developed in ruby. 
Therefore, our service will be a simple server that will get RPC calls, call the good method in the _gemfile_ 
and that will, _in fine_ transfer it to the backend.

## The RestFull API.

I will use the example described [here](http://thenewstack.io/make-a-restful-json-api-go/) as a basis for this post.
Of course there are many other examples and excellent go packages that may be used, but according to Mat Ryer, I will stick to the idiomatic approach.

## The glue: MSGPACK-RPC

There are several methods for RPC-ing between different languages. Ages ago, there was xml-rpc; then there has been json-rpc; 
I will use [msgpack-rpc](https://github.com/msgpack-rpc/msgpack-rpc) which is a binary, json base codec.
The communication between the Go client and the ruby server will be done over TCP via HTTP for example.

Later on, outside of the scope of this post, I may use ZMQ (as I have already blogged about 0MQ communication between those languages).

# The implementation of the Client (the go part)

I will describe here the node creation via a POST method, and consider that the other methods could be implemented in a similar way.

## The signature of the node creation

Here is the expected signature for creating a compute element:

```json
{
    "kind":"linux",
    "size":"S",
    "disksize":20,
    "leasedays":1,
    "environment_type":"dev",
    "description":"my_description",
}
```

The corresponding GO structure is:

```go
type NodeRequest struct {
    Kind string `json:"kind"` // Node kind (eg linux)
    Size string `json:"size"` // size
    Disksize         int    `json:"disksize"`
    Leasedays        int    `json:"leasedays"`
    EnvironmentType  string `json:"environment_type"`
    Description      string `json:"description"`
}
```

## The route

The Middleware is using the [gorilla mux package](http://www.gorillatoolkit.org/pkg/mux). 
According the description, I will add an entry in the routes array (into the _routes.go_ file):

```go
Route{
    "NodeCreate",
    "POST",
    "/v1/nodes",
    NodeCreate,
},
```

*Note* : I am using a prefix `/v1` for my API, for exploitation purpose.

I will then create the corresponding handler in the file with this signature

```go
func NodeCreate(w http.ResponseWriter, r *http.Request){
    var nodeRequest NodeRequest
    body, err := ioutil.ReadAll(io.LimitReader(r.Body, 1048576))
    if err != nil {
        panic(err)
    }
    if err := r.Body.Close(); err != nil {
        panic(err)
    }
    if err := json.Unmarshal(body, &nodeRequest); err != nil {
        w.Header().Set("Content-Type", "application/json; charset=UTF-8")
        w.WriteHeader(http.StatusBadRequest) // unprocessable entity
        if err := json.NewEncoder(w).Encode(err); err != nil {
            panic(err)
        }
    }    
}
```

That's in this function that will be implemented RPC (client part). To keep it simple at the beginning, 
I will instantiate a TCP connection on every call.
Don't throw things at me, that will be changed later following the advice of Mat Ryer.

## The implementation of the handler

### The effective remote procedure call

To use _msgpack_ I need to import the go implementation `github.com/msgpack-rpc/msgpack-rpc-go/rpc`.
This library will take care of the encoding/decoding of the messages.

Let's dial the RPC server and call the `NodeCreate` method with, as argument, the information we had from the JSON input

```go
    conn, err := net.Dial("tcp", "127.0.0.1:18800")
    if err != nil {
        fmt.Println("fail to connect to server.")
        return
    }
    client := rpc.NewSession(conn, true)
    retval, err := client.Send("NodeCreate", nodeRequest.Kind, nodeRequest.Size, nodeRequest.Disksize, nodeRequest.Leasedays, nodeRequest.EnvironmentType, nodeRequest.Description)
    if err != nil {
        fmt.Println(err)
        return
    }
    fmt.Println(retval)
```
# The RPC server (the ruby part)

This part is written in ruby, and will take care of the effective node creation.
At first, we should install the GEM file with the command `gem install msgpack-rpc`.

```ruby
require 'msgpack/rpc'
class MyHandler
    def NodeCreate(kind, size, disksize, leasedays, environmenttype, description) 
        print "Creating the node with parameters: ",kind, size, disksize, leasedays, environmenttype, description
        return "ok"
    end
end
svr = MessagePack::RPC::Server.new
svr.listen('0.0.0.0', 18800, MyHandler.new)
svr.run
```

# let's test it

Launch the RPC server:
`ruby server.rb`

Then launch the API rest server

`go run *go`

Then perform a POST request

```shell
curl -X POST -H 'Content-Type:application/json' -H 'Accept:application/json' -d '{"kind":"linux","size":"S","disksize":20,"leasedays":1,"environment_type":"dev","description":"my_description"}' -k http://localhost:8080/v1/nodes
```

It should write something like this: 
```
2015/11/10 13:56:51 POST        /v1/nodes       NodeCreate      2.520673ms
ok
```

And something like this in the output of the ruby code:
```
Creating the node with parameters: linux S 20 1 dev my_description
```

That's all folks! What's left:

* To implement the other methods to be "[CRUD](https://en.wikipedia.org/wiki/Create,_read,_update_and_delete)" compliant
* To implement an authentication and accreditation mechanism (JWT, Oauth, ?)
* To change the implementation of the RPC client to use a pool instead of a single connection
* To implement the swagger interface and documentation of the API
* Whatever fancy stuff you may want from a production ready interface.

You can find all the codes in the github repository [here](https://github.com/owulveryck/example-iaas) in the branch `iaas-like-restfull-api-based-on-microservices`
---
author: author
date: 2016-01-30T11:35:36+01:00
description: description
draft: true
keywords:
- key
- words
tags:
- one
- two
title: nas for macos on freebsd
topics:
- topic 1
type: post
---

Creating the jail:

```shell
iocage create tag=nas resolver="nameserver 192.168.1.1" hostname="nas" ip4_addr="lo1|192.168.1.100/24"
```

## mDNSresponder
---
author: Olivier Wulveryck
date: 2015-12-07T08:48:20Z
description: |
    In a previous post I have described (an implemented) and orchestrator that takes a digraph
    as input (via its adjacency matrix).
    In this post I will implement an API, so the orchestrator will be transformed into a web service.
    Say hello to OaaS
draft: true
tags:
- golang
- orchestrator
title: OaaS orchestrator as a service - part 1
topics:
- OaaS
type: post
---

In a [previous post](http://blog.owulveryck.info/2015/12/02/orchestrate-a-digraph-with-goroutine-a-concurrent-orchestrator/) I have setup and orchestrator that takes a digraph
as input (via its adjacency matrix).

In this post I will implement an API, so the orchestrator will be transformed into a web service.

# The documentation of the API

The API will be self documented with swagger (see [this post](http://blog.owulveryck.info/2015/11/11/simple-iaas-api-documentation-with-swagger/) for an "introduction").

## The verbs 

I will use only 3 verbs by now:

* POST : to send the digraph representation to the orchestrator engine
* GET : to get the actual status of the execution workflow
* DELETE : delete to obviously delete an execution task

## The JSON objects






## The Swagger interface


# The implementation


---
author: Olivier Wulveryck
date: 2015-11-17T10:05:42Z
description: Playing with openstack keystone
draft: false
keywords:
- openstack
- keystone
- authentication
tags:
- openstack
- keystone
- authentication
- REST
title: Playing with (Openstack) Keystone
topics:
- topic 1
type: post
---

In the cloud computing, alongside of the hosting monsters such as amazon or google, there is the [Openstack Platform](https://www.openstack.org).

Openstack is not a single software, it is more a galaxy of components aim to control the infrastructure, such as hardware pools, storage, network.
The management can then be done via a Web based interface or via a bunch of RESTful API.

I would like to evaluate its identity service named [keystone](http://docs.openstack.org/developer/keystone/) and use it as a AuthN and AuthZ backend for my simple_iaas example.

_Note_ : I will consider that the openstack keystone is installed (The release I'm using is _liberty_). As I don't want to rewrite an installation procedure as many exists already on the web. For my tests, I'm using an keystone installation from sources in a Ubuntu VM

# My goal

My goal is to have a webservice that will protect the scopes of my IAAS. 
I may declare two users:

- One may list the nodes via a GET request
- The other one may also create and destroy nodes via POST and DELETE request

This post is about setting up keystone so I can use it as a lab to understand the concepts and maybe achieve my goal with it.

# Let's go 

I won't use any external web server. Instead I will rely on the builtin Eventlet based web server.

The documentation says it is deprecated, indeed I will use it for testing purpose, so that will do the job.

## The WSGI pipeline configuration

To be honest, I don't know anything about the python ecosystem. And as it is my blog, I will write anything I've learned from this experience... 

So:

- WSGI is a gateway interface for python, and my understanding is that it's like the good old CGI we used in the beginning of this century;
- Is is configured by a ini file based on [Paste](http://pythonpaste.org/) and especially _Paste Deploy_ which is a system made for loading and configuring WSGI components.

The WSGI interface is configured by a ini file as written in the [Openstack keystone documentation](http://docs.openstack.org/developer/keystone/configuration.html).
This file is called `keystone-paste.ini`. I won't touch it and use the provided one. It sounds ok and when I start the service with `keystone-all` I can see in the logs:

```logs
2015-11-17 10:05:04.918 7068 INFO oslo_service.service [-] Starting 2 workers
2015-11-17 10:05:04.920 7068 INFO oslo_service.service [-] Started child 7082
2015-11-17 10:05:04.922 7068 INFO oslo_service.service [-] Started child 7083
2015-11-17 10:05:04.925 7082 INFO eventlet.wsgi.server [-] (7082) wsgi starting up on http://0.0.0.0:35357/
2015-11-17 10:05:04.927 7068 INFO keystone.common.environment.eventlet_server [-] Starting /usr/bin/keystone-all on 0.0.0.0:5000
2015-11-17 10:05:04.927 7068 INFO oslo_service.service [-] Starting 2 workers
2015-11-17 10:05:04.930 7068 INFO oslo_service.service [-] Started child 7084
2015-11-17 10:05:04.934 7083 INFO eventlet.wsgi.server [-] (7083) wsgi starting up on http://0.0.0.0:35357/
2015-11-17 10:05:04.936 7068 INFO oslo_service.service [-] Started child 7085
2015-11-17 10:05:04.940 7085 INFO eventlet.wsgi.server [-] (7085) wsgi starting up on http://0.0.0.0:5000/
2015-11-17 10:05:04.941 7084 INFO eventlet.wsgi.server [-] (7084) wsgi starting up on http://0.0.0.0:5000/
2015-11-17 10:17:01.005 7085 INFO keystone.common.wsgi [-] GET http://localhost:5000/
```

which sounds ok and a `curl` call to the endpoint reply at least something:

```json
$ curl -s http://localhost:5000/v3 | jsonformat
{
  "version": {
    "id": "v3.4",
    "links": [
      {
        "href": "http://localhost:5000/v3/",
        "rel": "self"
      }
    ],
    "media-types": [
      {
        "base": "application/json",
        "type": "application/vnd.openstack.identity-v3+json"
      }
    ],
    "status": "stable",
    "updated": "2015-03-30T00:00:00Z"
  }
}
```

## The keystone configuration

The proper keystone configuration is done in the file `keystone.conf`. This configuration file is decomposed into different sections as explained in the documentation.

### The general configuration (Default section)
I will only set the `admin token` randomly as it will be used to create the users, roles and so on.

Let's generate a token with `openssl rand -hex 10` and report it to my configuration:
```
[DEFAULT]
admin_token = 8a0b4eacc6a81c3bc5aa
```

The rest will use all the default values for the General configuration (the [DEFAULT] section). This means that this section may be empty or full of comments.


### The assignment configuration
In this section, we choose the driver for the assignment service.
This purpose of this service is

> [to] provide data about roules and role assignments 
> to the entities managed by the Identity and Resource services

(source [Keystone architecture](http://docs.openstack.org/developer/keystone/architecture.html))

I plan to use a SQL backend instead of a LDAP for my tests, so the configuration may be:
```
[assignment]
driver = sql
```

### The authentication plugin configuration
Keystone supports authentication plugins; those plugins are specified in the `[auth]` section.
In my test, the `password` plugin will be used.

```
[auth]
methods = password
```

### The credentials
The credentials are stored in a sql database as well:

```
[credential]
driver = sql
```

### The DB configuration
For my tests I will use a sqlite database as configured in this section:
```
[database]
sqlite_db = oslo.sqlite
sqlite_synchronous = true
backend = sqlalchemy
connection = sqlite:////var/lib/keystone/keystone.db

```

## Restart the keystone server and play 


```
# service keystone restart
# service keystone status
 keystone.service - OpenStack Identity service
   Loaded: loaded (/lib/systemd/system/keystone.service; enabled; vendor preset: enabled)
   Active: active (running) since Tue 2015-11-17 14:47:06 GMT; 3s ago
  Process: 15505 ExecStartPre=/bin/chown keystone:keystone /var/lock/keystone /var/log/keystone /var/lib/keystone (code=exited, status=0/SUCCESS)
  Process: 15502 ExecStartPre=/bin/mkdir -p /var/lock/keystone /var/log/keystone /var/lib/keystone (code=exited, status=0/SUCCESS)
 Main PID: 15508 (keystone-all)
   CGroup: /system.slice/keystone.service
           15508 /usr/bin/python /usr/bin/keystone-all --config-file=/etc/keystone/keystone.conf --log-file=/var/log/keystone/keystone.log
           15523 /usr/bin/python /usr/bin/keystone-all --config-file=/etc/keystone/keystone.conf --log-file=/var/log/keystone/keystone.log
           15524 /usr/bin/python /usr/bin/keystone-all --config-file=/etc/keystone/keystone.conf --log-file=/var/log/keystone/keystone.log
           15525 /usr/bin/python /usr/bin/keystone-all --config-file=/etc/keystone/keystone.conf --log-file=/var/log/keystone/keystone.log
           15526 /usr/bin/python /usr/bin/keystone-all --config-file=/etc/keystone/keystone.conf --log-file=/var/log/keystone/keystone.log

Nov 17 14:47:08 UBUNTU keystone[15508]: 2015-11-17 14:47:08.479 15508 INFO oslo_service.service [-] Started child 15523
Nov 17 14:47:08 UBUNTU keystone[15508]: 2015-11-17 14:47:08.482 15508 INFO oslo_service.service [-] Started child 15524
Nov 17 14:47:08 UBUNTU keystone[15508]: 2015-11-17 14:47:08.486 15508 INFO keystone.common.environment.eventlet_server [-] Starting /usr/bin/keystone-all on 0.0.0.0:5000
Nov 17 14:47:08 UBUNTU keystone[15508]: 2015-11-17 14:47:08.490 15508 INFO oslo_service.service [-] Starting 2 workers
Nov 17 14:47:08 UBUNTU keystone[15508]: 2015-11-17 14:47:08.491 15523 INFO eventlet.wsgi.server [-] (15523) wsgi starting up on http://0.0.0.0:35357/
Nov 17 14:47:08 UBUNTU keystone[15508]: 2015-11-17 14:47:08.493 15508 INFO oslo_service.service [-] Started child 15525
Nov 17 14:47:08 UBUNTU keystone[15508]: 2015-11-17 14:47:08.499 15524 INFO eventlet.wsgi.server [-] (15524) wsgi starting up on http://0.0.0.0:35357/
Nov 17 14:47:08 UBUNTU keystone[15508]: 2015-11-17 14:47:08.502 15508 INFO oslo_service.service [-] Started child 15526
Nov 17 14:47:08 UBUNTU keystone[15508]: 2015-11-17 14:47:08.506 15525 INFO eventlet.wsgi.server [-] (15525) wsgi starting up on http://0.0.0.0:5000/
Nov 17 14:47:08 UBUNTU keystone[15508]: 2015-11-17 14:47:08.510 15526 INFO eventlet.wsgi.server [-] (15526) wsgi starting up on http://0.0.0.0:5000/
```

so far so good... let's check if the DB is here now:

```
# sqlite3 /var/lib/keystone/keystone.db
SQLite version 3.8.11.1 2015-07-29 20:00:57
Enter ".help" for usage hints.
sqlite> .tables
access_token            identity_provider       revocation_event
assignment              idp_remote_ids          role
config_register         mapping                 sensitive_config
consumer                migrate_version         service
credential              policy                  service_provider
domain                  policy_association      token
endpoint                project                 trust
endpoint_group          project_endpoint        trust_role
federation_protocol     project_endpoint_group  user
group                   region                  user_group_membership
id_mapping              request_token           whitelisted_config
sqlite> .quit
```

## Interacting with openstack

A tools called [python-openstackclient](http://docs.openstack.org/developer/python-openstackclient/command-list.html) is available in my ubuntu release and will be used for testing purpose.

The binary provided is `openstack` (`dpkg-query -L python-openstackclient | grep bin`)

### Creating a user

We need to define a couple of environment variables to be able to connect to the keystone server with the `root` power:

I will create a simple file that I will source when I need to interact as admin of keystone
```
cat << EOF > admin.sh
# The value of admin_token defined in the keystone.conf
export OS_TOKEN=8a0b4eacc6a81c3bc5aa 
# This is the default value if not overridden by the directive admin_endpoint
export OS_URL=http://localhost:35357/v2.0 
export OS_IDENTITY_API_VERSION=3
EOF
```

and another file to unset those variables:
```
cat << EOF > noadmin.sh 
unset OS_TOKEN
unset OS_URL
unset OS_IDENTITY_API_VERSION
EOF
```

Then we create the user: 
```
source admin.sh
openstack user create olivier
'links'
```

Then set its password:
```
source admin.sh
openstack user set --password-prompt olivier
User Password:
Repeat User Password:
'users'
```

And see if it's actually here:
```
source admin.sh
openstack user list
+----------------------------------+---------+
| ID                               | Name    |
+----------------------------------+---------+
| c80f5244c7d3486fbf4059b7197b4770 | olivier |
+----------------------------------+---------+
```

### Creating a project

`openstack project create --description 'demo project' demo`

### Assigning the admin role

Let's first get the role list
```
source admin.sh
openstack role list
+----------------------------------+----------+
| ID                               | Name     |
+----------------------------------+----------+
| 5f772b617b5d4758badb7746934124e8 | admin    |
| 9fe2ff9ee4384b1894a90878d3e92bab | _member_ |
+----------------------------------+----------+
```

And add the admin right to the user `olivier` for the project `demo`

```
source admin.sh
openstack role add --user olivier --project 0e07a734d54e4f3799a31768b13a38c2 admin
```

## Getting a token

### With the openstack tool 

I've a default domain, I've setup a demo project, and assigne the my user the admin role for testing purpose.
I may now be able to generate an access token

Let's try:

```
source noadmin.sh
openstack --os-auth-url http://localhost:5000/v3 --os-username olivier --os-password olivier --os-auth-type=password --os-project-name demo token issue
Expecting to find domain in project - the server could not comply with the request since it is either malformed or otherwise incorrect. The client is assumed to be in error. (HTTP 400) (Request-ID: req-09cad46b-9a5f-4b0f-8f2b-82b4442ed999)
```

Ok, now add the domain:
```
source noadmin.sh
openstack --os-auth-url http://localhost:5000/v3 --os-username olivier --os-password olivier --os-auth-type=password --os-project-name demo --os-domain-name default token issue
Authentication cannot be scoped to multiple targets. Pick one of: project, domain, trust or unscoped
```

Too bad, remove the project...
```
source noadmin.sh
openstack --os-auth-url http://localhost:5000/v3 --os-username olivier --os-password olivier --os-auth-type=password --os-domain-name default token issue
The request you have made requires authentication. (HTTP 401) (Request-ID: req-59c39895-8e96-42c4-b5c5-1477001da618)
```

Still no luck... Google gave me a lot of answers, but I couldn't figure whether it was:

* a bug
* a misconfiguration of the service
* a bad usage of the tools
* a totally bad apprehension of the product

I may continue to experiment, but I'm far from my goal actually, and I hate the idea of being lost. no help from Google, so DIY method:

* openstack client in debug mode with --debug
* keystone in debug with a `debug=true` directive in `keystone.conf`

The message is now clear:
```
2015-11-18 10:37:33.337 7164 WARNING keystone.common.wsgi [req-27dadee6-51d9-475d-a426-99e3b4f77f4a - - - - -] Authorization failed. User c80f5244c7d3486fbf4059b7197b4770 has no access to domain default
```
So let's re-set the password, just in case, as done in the previous section of this post and try again:

```
source noadmin.sh
openstack --os-auth-url http://localhost:5000/v3 --os-username olivier --os-password olivier --os-auth-type=password --os-project-name demo --os-domain-name default token issue
...
Unauthorized: User c80f5244c7d3486fbf4059b7197b4770 has no access to domain default (Disable debug mode to suppress these details.)
```

Ok, let's add `olivier` as admin of the Default domain:

```
source admin.sh
openstack role add --user olivier --domain Default admin
```

And try again:

```
source noadmin.sh
openstack --os-auth-url http://localhost:5000/v3 --os-username olivier --os-password olivier --os-auth-type=password --os-domain-name Default token issue
+-----------+----------------------------------+
| Field     | Value                            |
+-----------+----------------------------------+
| domain_id | default                          |
| expires   | 2015-11-18T11:44:23.325817Z      |
| id        | 0525e008619748848735d9122f8f2e81 |
| user_id   | c80f5244c7d3486fbf4059b7197b4770 |
+-----------+----------------------------------+
```

Bingo!!! 

# Conclusion

That's enough information for this first post about OpenStack keystone.
By now, I will continue the investigations and use this installation as a _lab_ to understand the full concepts of this identity management software.

I leave my test with a bitter taste.

I may not have understood all the concepts behind the scene, but I can say that this product, at least in the current release, is by far too complex and has too much contributors to evaluate it in a simple way.
Therefore, the quite complete but messy documentation, a mix in the releases and some major incompatibilities in the tools using V2.0 and V3 gave me a bad impression.
The tool may be useful, but the TCO sounds high and the entry ticket is not negligible.
---
author: Olivier Wulveryck
date: 2015-12-02T14:24:21Z
description: |
    A directed graph may be represented by its adjacency matrix.
    Considering each vertice as a runable element and any edge as a dependency,
    I will describe a method to "run" the graph in a concurrent way using goalang's goroutine
draft: false
tags:
- golang
- digraph
title: Orchestrate a digraph with goroutine, a concurrent orchestrator
type: post
---

I've read a lot about graph theory recently.
They have changed the world a lot. From the simple representation to Bayesian network via Markov chains, the applications are numerous.

Today I would like to imagine a graph as a workflow of execution. Every node would be considered as runnable. And every  edge would be a dependency.

It is an important framework that may be used to as an orchestrator for any model, and of course I am a lot thinkingabout __TOSCA__

# The use case 
If we consider this very simple graph (example taken from the french wikipedia page)

<img class="img-responsive" src="/assets/images/digraph1.png" alt="digraph example"/> 

its corresponding adjacency matrix is:

<img class="img-responsive" src="/assets/images/matrix1.png" alt="Adjacency matrix"/>

its dimension is 8x8

For the lab, I will consider that each node has to do a simple task which is to wait for a random number of millisecond (such as Rob Pike's _boring_ function, see references)

# Let's GO

## How will it work

Every node will be run in a `goroutine`. That is a point. But how do I deal with concurrency ?

Every single goroutine will be initially launched and then wait for an information.

It will have an input communication channel, and a _conductor_ will feed this channel with enough information for the goroutine to decides whether it should run or not. 
This information is simply the adjacency matrix up-to-date. That means that is a node is done, its value is set to zero.

Every goroutine will then check in the adjacency matrix, whether it has predecessor (that means if the corresponding vector is null, or every digit in column N is 0) and therefore will execute the step or not.

Once the execution of task is over, the goroutine will then feed another channel to tell the conductor that its job is done. and then the conductor will broadcast the information.

### Example

In our example, nodes _3_, _5_, and _7_ do not have any predecessor, so they will be able to run first.

* __(1)__ The conductors feed the nodes with the matrix

<a href="/assets/orchestrate-a-digraph-with-goroutine/digraph_step1.dot"><img class="img-responsive img-thumbnail" src="/assets/images/digraph_step1.png" alt="digraph example"/></a>

* __(2)__ Every node get the data and analyse the matrix

<a href="/assets/orchestrate-a-digraph-with-goroutine/digraph_step2.dot"><img class="img-responsive img-thumbnail" src="/assets/images/digraph_step2.png" alt="digraph example"/> </a>

* __(3)__ Nodes 3, 5 and 7 have no predecessor (their column in the matrix sums to zero): they can run

<a href="/assets/orchestrate-a-digraph-with-goroutine/digraph_step3.dot"><img class="img-responsive img-thumbnail" src="/assets/images/digraph_step3.png" alt="digraph example"/> </a>

* __(4)__ Nodes 3 and 5 are done, they informs the conductor

<a href="/assets/orchestrate-a-digraph-with-goroutine/digraph_step4.dot"><img class="img-responsive img-thumbnail" src="/assets/images/digraph_step4.png" alt="digraph example"/> </a>

* __(5)__ conductor update the matrix. It fills the rows 3 and 5 with zeros (actually rows 4 and 6, because our first node is 0)

<a href="/assets/orchestrate-a-digraph-with-goroutine/digraph_step5.dot"><img class="img-responsive img-thumbnail" src="/assets/images/digraph_step5.png" alt="digraph example"/> </a>

* __(6)__ The conductor feeds the nodes with the matrix

<a href="/assets/orchestrate-a-digraph-with-goroutine/digraph_step6.dot"><img class="img-responsive img-thumbnail" src="/assets/images/digraph_step6.png" alt="digraph example"/> </a>

* __(7)__ The nodes analyse the matrix

<a href="/assets/orchestrate-a-digraph-with-goroutine/digraph_step7.dot"><img class="img-responsive img-thumbnail" src="/assets/images/digraph_step7.png" alt="digraph example"/> </a>

* __(8)__ Node 2 can run...

<a href="/assets/orchestrate-a-digraph-with-goroutine/digraph_step8.dot"><img class="img-responsive img-thumbnail" src="/assets/images/digraph_step8.png" alt="digraph example"/> </a>

## The representation of the use case in go

### Data representation
to keep it simple, I won't use a `list` or a `slice` to represent the matrix, but instead I will rely on the [package mat64](https://godoc.org/github.com/gonum/matrix/mat64).

A slice may be more efficient, but by now it is not an issue. 

On top of that, I may need later to transpose or look for eigenvalues, and this package does implement the correct method to do so.
For clarity of the description, I didn't use a `float64` array to initialize the matrix.

```golang
// Allocate a zeroed array of size 88
m := mat64.NewDense(8, 8, nil)
m.Set(0, 1, 1); m.Set(0, 4, 1) // First row
m.Set(1, 6, 1); m.Set(1, 6, 1) // second row
m.Set(3, 2, 1); m.Set(3, 6, 1) // fourth row
m.Set(5, 0, 1); m.Set(5, 1, 1); m.Set(5, 2, 1) // fifth row
m.Set(7, 6, 1) // seventh row
fa := mat64.Formatted(m, mat64.Prefix("    "))
fmt.Printf("\nm = %v\n\n", fa)
```

### The node execution function (_run_)
The node execution is performed by a `run` function that takes two arguments: 

* The ID of the node
* The duration of the sleep it performs...

This function returns a channel that will be used to exchange a `Message`
```golang
func run(id int, duration time.Duration) <-chan Message { }
```

A `Message` is a structure that will holds:

* the id of the node who have issued the message
* a boolean which act as a flag that says whether it has already run
* a wait channel which take a matrix as argument. This channel acts as the communication back mechanism from the conductor to the node


```golang
type Message struct {
	id   int
	run  bool
	wait chan mat64.Dense
}
```

The run function will launch a goroutine which will remain active thanks to a loop.
It allows the run function to finish an returns the channel as soon as possible to it can be used by the conductor.

### The conductor

The conductor will be executed inside the main function in our example.

The first step is to launch as many `run` function as needed.

There is no need to launch them in separate goroutines, because, as explained before, 
the run function will returns the channel immediately because the intelligence is living in a goroutine already.

```golang
for i := 0; i < n; i++ { // n is the dimension of the matrix
    cs[i] = run(i, time.Duration(rand.Intn(1e3))*time.Millisecond)
    ...
```

Then, as we have launched our workers, and as the communication channel exists, we should launch `n` "angel" goroutines, that will take care of
sending back the matrix to all the workers.

```golang
    ...
	node := <-cs[i]
	go func() {
		for {
			node.wait <- *m
		}
	}()
}
```

Then we shall collect all the messages sent back by the goroutines to treat them and update the matrix as soon as a goroutine has finished.
I will use the `fanIn` function as described by _Rob Pike_ in the IO Takl of 2012 (see references) and then go in a `for loop` to get the results
as soon as they arrived:

```golang
c := fanIn(cs...)
timeout := time.After(5 * time.Second)
for {
    select {
    case node := <-c:
        if node.run == true {
            fmt.Printf("%v has finished\n", node.id)
            // 0 its row in the matrix
            for c := 0; c < n; c++ {
                m.Set(node.id, c, 0)
            }
        }
    case <-timeout:
        fmt.Println("Timeout")
        return
    default:
        if mat64.Sum(m) == 0 {
            fmt.Println("All done!")
            return
        }
    }
}
fmt.Println("This is the end!")
```

__Note__ I have set up a timeout, just in case ([reference](https://talks.golang.org/2012/concurrency.slide#36))...
__Note2__ I do not talk about the fanIn funtion which is described [here](https://talks.golang.org/2012/concurrency.slide#28)

## The test

Here is what I got when I launch the test:

```
go run orchestrator.go 
I am 7, and I am running
I am 3, and I am running
I am 5, and I am running
3 has finished
5 has finished
I am 2, and I am running
I am 0, and I am running
0 has finished
I am 1, and I am running
I am 4, and I am running
4 has finished
7 has finished
2 has finished
1 has finished
All done!
```

Pretty cool

The complete source can be found [here](https://github.com/owulveryck/gorchestrator).

If you want to play: download go, setup a directory and a `$GOPATH` then simply

```
go get github.com/owulveryck/gorchestrator
cd $GOPATH/src/github.com/owulveryck/gorchestrator
go run orchestrator.go
```

# Conclusions

I'm really happy about this implementation. It is clear and concise, and no too far to be idiomatic go.

What I would like to do now:

* Read a TOSCA file (again) and pass the adjacency matrix to the orchestrator. That would do a complete orchestrator for cheap.
* Re-use an old implemenation of the [toscaviewer](https://github.com/owulveryck/toscaviewer).
The idea is to implement a web server that serves the matrix as a json stream. This json will be used to update the SVG (via jquery),
and then we would be able to see the progession in a graphical way.


__STAY TUNED!!!__

### References

* [Go Concurrency Patterns (Rob Pike)](https://talks.golang.org/2012/concurrency.slide)
---
author: Olivier Wulveryck
date: 2017-01-06T11:06:06+01:00
description: It ain't no secret to anyone I do not like Javascript.  
draft: true
keywords:
- key
- words
tags:
- one
- two
title: Accessing AWS deep learning services from iOS.
topics:
- topic 1
type: post
---


> Only fools never change their minds - _Talleyrand_

![https://www.flickr.com/photos/8047705@N02/5366637592](/assets/images/edison.jpg)
---
author: Olivier Wulveryck
date: 2016-12-16T14:51:18+01:00
description: Very quick post to present a piece of code that grabs an image from a webcam and send it to amazon's image recognition service.
draft: false
tags:
- webcam
- golang
- aws
title: Image reKognition with a webcam, go and AWS.
topics:
- topic 1
type: post
---

It's been a while since I last posted something. I will fill the gap with a quick post about _rekognition_.

[rekognition](https://aws.amazon.com/rekognition/?nc1=h_ls) is a service from AWS that is described as:

> Deep learning-based image recognition

> Search, verify, and organize millions of images

In this light post, I will present a simple method to grab a picture from my webcam, send it to rekognition and display the result.

The part of the result I will focus on is the emotion. In other word, I will ask amazon: "An I happy?".

# Getting the picture from the webcam

I am using the package [github.com/blackjack/webcam](github.com/blackjack/webcam) to grab the picture.

## Capabilities of the webcam and image format

My webcam is handling the MJPEG format.
Therefore, after the creation of a _cam_ object and set the correct settings to grab mjpeg, I can read a frame in JPEG:

```go
// ...
cam, err := webcam.Open("/dev/video0") // Open webcam
// ...
// Setting the format:
_,_,_, err := cam.SetImageFormat(format, uint32(size.MaxWidth), uint32(size.MaxHeight))
```

<code>format</code> is of type `uint32` and computable thanks to the informations present in [/usr/include/linux/videodev2.h](http://lxr.free-electrons.com/source/include/uapi/linux/videodev2.h)

MJPEG is: 1196444237

_Note_: To be honest, I did not evaluate the FOURCC method; I have requested the supported format of my webcam with their descriptions :)

## Grabbing the picture

In a endless `for` loop, a frame is read with a call to `ReadFrame`:

```go
for {
    timeout := uint32(5) //5 seconds
    err = cam.WaitForFrame(timeout)
    frame, err := cam.ReadFrame()
}
```

# AWS

The API to import to use the service is `github.com/aws/aws-sdk-go/service/rekognition` and is documented here: [http://docs.aws.amazon.com/sdk-for-go/api/service/rekognition/](http://docs.aws.amazon.com/sdk-for-go/api/service/rekognition/)

The operation that I am using to detect the emotion is [DetectFaces](http://docs.aws.amazon.com/sdk-for-go/api/service/rekognition/#Rekognition.DetectFaces) that takes an pointer to [DetectFacesInput](http://docs.aws.amazon.com/sdk-for-go/api/service/rekognition/#DetectFacesInput) with is composed of a pointer to an [Image](http://docs.aws.amazon.com/sdk-for-go/api/service/rekognition/#Image).

## Creating the input

The first thing that needs to be created is the [Image](http://docs.aws.amazon.com/sdk-for-go/api/service/rekognition/#Image) object from our `frame`:

```go
if len(frame) != 0 {
    image := &rekognition.Image{ // Required
        Bytes: frame,
    }
```

Then we create the DetectFacesInput:

```go
params := &rekognition.DetectFacesInput{
        Image: image,
        Attributes: []*string{
                aws.String("ALL"), 
        },
}
```

The `ALL` attributes is present, otherwise AWS does not return the complete description of what it has found.

## Sending the query

### Pricing notice and __warning__
The price of the service as of today is 1 dollar per 1000 request. That sounds cheap, but at 25 FPS, this may cost a lot.
Therefore, I have set up a read request that only process a picture if we press _enter_ 

```go
bufio.NewReader(os.Stdin).ReadBytes('\n')
```

### Session

As usual, to query AWS we need to create a session:

```go
var err error
sess, err = session.NewSession(&aws.Config{Region: aws.String("us-east-1")})
if err != nil {
    fmt.Println("failed to create session,", err)
    return
}
svc = rekognition.New(sess)
```

_Note_: The `session` library will take care of connections informations such as environment variables like:

* `AWS_ACCESS_KEY_ID`
* `AWS_SECRET_ ACCESS_KEY_ID`

### Query and result

Simply send the query 
```go
esp, err := svc.DetectFaces(params)

if err != nil {
        fmt.Println(err.Error())
        return
}
```

The result is of type [DetectFacesOutput](http://docs.aws.amazon.com/sdk-for-go/api/service/rekognition/#DetectFacesOutput).
This type is composed of a array of FaceDetails because obviously there can me more than one person per image.
So we will loop and display the emotion for each face detected:

```go
for i, fd := range resp.FaceDetails {
        fmt.Printf("The person %v is ", i)
        for _, e := range fd.Emotions {
                fmt.Printf("%v, ", *e.Type)
        }
        fmt.Printf("\n")
}
```

# Run:

<pre>
Resulting image format: MJPEG (320x240)
Press enter to process 
The person 0 is HAPPY, CONFUSED, CALM, 
</pre>

# Conclusion

That's all folks. The full code can be found [here](https://gist.github.com/owulveryck/33753125afa6284cd5dbbb1bd4d1eb54).

In the test I made, I was always happy. I've tried to be angry or sad, without success... Maybe I have a happy face.
I should try with someone else maybe.

The service is nice and opens the door to a lot of applications:
For example to monitor my home and sends an alert if someone is in my place and __not from my family__ (or not the cat :).
---
author: Olivier Wulveryck
date: 2016-09-09T13:39:50+02:00
description: People who know what they are talking about don't need Powerpoint said Steve Jobs. Indeed, it may be useful from time to time to have a single slide to display key words. But what I understand from Jobs's sentence is that relying on the slides is a bad idea. Good stuff that machine learning can pilot the slides for us!
draft: false
tags:
- chrome
- speech recognition
- Javascript
- revealjs
- slides
title: Being a better public speaker with a little help of Speech Recognition, Javascript and Chrome
topics:
- topic 1
type: post
---

# Introduction

I usually don't like slidewares.

Actually as IT engineer working, by now, exclusively in France, I'm facing the PowerPoint problem:

* Too many boring slides,
* too much information per slide,
* a presenter dedicated to read their content.

Therefore, the audience is watching its watch while waiting for a coffee break.

I won't redo the introduction I already did in a [previous post](/2016/06/23/websockets-reveal.js-d3-and-go-for-a-dynamic-keynote/index.html) but indeed slides can,
from time to time, be a value-add to a presentation.

Is the previous post, I introduced reveal.js, this excellent javascript framework. And I already did an interactive presentation.

In this post, I will go a little bit further with the integration of Speech Recognition.

# Speech Recognition by Google

## The Google Cloud Speech API

It ain't no secret now, I'm a big fan of machine learning.
Machines learn faster than people, and they can now assist us in a lot of boring tasks.

On the base of a neuron network, Google provides an [API for speech recognition](https://cloud.google.com/speech/).
It is fairly complete and multi lingual.

## Chrome speech recognition
They "_eat their own dog food_" and use their engine for android and... Chrome.
Best of all, they provide a userland access via a Javascript API to this functionality in Chrome.

This means that you can develop a web page that will access you microphone, sends what you say to the Google cloud, get the result back and process it in your page.

You can see an introduction [here](https://developers.google.com/web/updates/2013/01/Voice-Driven-Web-Apps-Introduction-to-the-Web-Speech-API)

# What can I use that for: A case study?

I had to do a presentation recently.
This presentation was about _Agility_ and _Devops_. The main idea was to give my client a feedback about experiences I had regarding those principles in digital transformation.

I didn't want to loose my audience with slides. But I wanted to keep the key concepts alive and visible.

So what I did was a one slide presentation only with the keywords I wanted to talk about.

The day before, I though:

> "How nice it would be if as far as I speak, the key concepts would appear on screen..."

You may think: _"that's easy, learn your script and click on the right moment"_.

Ok, but there are drawbacks:

* You have to learn the script
* You cannot be spontaneous anymore
* It's a one shot, one displayed, you cannot interact with the points anymore.

What I need is "an assistant" that will _listen to me_ and _act as soon as he/she/it hear a buzz word_.
It's 2016, this assistant is a machine, and I can teach it to act correctly.

Here is a little demo of the end product (don't pay to much attention to the content, I said nonsense for the demo)

<iframe width="560" height="315" src="https://www.youtube.com/embed/MOmmBufEwPo" frameborder="0" allowfullscreen></iframe>

And another one in French.

<iframe width="560" height="315" src="https://www.youtube.com/embed/3Uyr0G0add4" frameborder="0" allowfullscreen></iframe>

# The implementation

## Annyang

I have used the [annyang](https://github.com/TalAter/annyang) which is a javascript wrapper for the Chrome speech recognition functionnality.

Instead of matching a sentence as explained in the example of annyang, I made it listen to my whole speech in a loop.
Then I passed every sentence detected by the framework to a javascript function that was applying regexp to look for keywords.

### Displaying words

For every keyword I did a match to an object of my DOM and simply changed its <code>visibility</code> style from <code>hidden</code> to <code>visible</code>

Here is the javascript code:

```javascript
if (annyang) {
  annyang.debug(true);
  annyang.setLanguage('fr-FR');
  annyang.addCallback('result', function(phrases) {
  for (s of phrases) {
     str = s.toLowerCase();
     switch (str) {
        case (str.match(/communication/) || {}).input:nnyang.start();                                                                                              
        document.getElementById('communication').style.visibility = 'visible';
        ...
    // Start listening. You can call this here, or attach this call to an event, button, etc.
    annyang.start();       
    ...
```

And the corresponding html section for the communication keyword:

```html
<div class="reveal">
  <div class="slides">
    <section>
      <h3 style="visibility: visible" id="agile">Agile</h3>
      <p> <span style="visibility: hidden;" id="communication">communication</span> </p>            
```

The speech recognition engine detects the sentence and gives a confidence note about its recognition.
All the potential results are stored in an array (here <code>phrases</code>). I've used them all so I was more confident not to miss a word.

### Making them blink

As I was not fully confident in the solution (it was late in the night and the show was the next morning), Therefore I made a fall-back solution.
All the words were displayed, and I used a little CSS tweak to make them blink when they were pronounced. 
This was done by adding and removing a css class to the concerned node.
The logic remains the same.

```css 
/* Chrome & Safari */
@-webkit-keyframes blink {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}

.blink {
  -webkit-animation: blink 750ms 2;
}
```

```Javascript
case (str.match(/communication/) || {}).input:                                                                                       
   document.getElementById("b_communication").classList.toggle('blink');
   setTimeout(function () {
     document.getElementById("b_communication").classList.remove('blink');
   }, 1500);
   break;
```

# Conclusion and TODO

The speech recognition engine is efficient and usable.
What I need to do is to code a tiny javascript library in order to get a JSON associative array between the list of spoken words that would trigger an action for an element for example:

```json
{
  "tag": "communication",
  "words": ["communication", "communicate"],
  "fuction": "blink"
}
```

Anyway, as a quick and dirty solution, the goal is achieved.

Another Idea would be to plug this with a NLP engine to perform stemming or lemmatization to do a better decoding and be even more machine learning compliant. This could be done with the help of [MITIE](https://github.com/mit-nlp/MITIE)


---
author: Olivier Wulveryck
date: 2016-08-29T21:58:17+02:00
description: A first geek interaction between my  raspberry pi 3 and my weather station
draft: false
tags:
- libusb
- udev
- linux
- weather station
- oregon RMS300
- golang
- raspberry pi
title: Getting weather data from the station to the raspberry
topics:
- topic 1
type: post
---

# Introduction

A bunch of friends/colleagues offered me a <a type=amzn>raspberry pi 3</a>.
It may become my VPN gateway, or my firewall, or the brain of my CCTV, or maybe the center of an alarm.... Maybe a spotify player...

Anyway, I have installed raspbian and I'm now playing with it.

Yesterday evening, as I was about to go to bed, I've had a very bad idea... I've linked together my <a type=amzn>rpi</a> and my <a type="amzn">Oregon</a> Weather Station.
3 hours later, I was still geeking...

As usual in the blog I will explain what I did, what did work, and what did not.

# Attaching the devices

I've plugged the device, ok! Now what does the system tells me about it:

What `dmesg` tells me is simply

<pre>
[ 2256.877522] usb 1-1.4: new low-speed USB device number 5 using dwc_otg
[ 2256.984860] usb 1-1.4: New USB device found, idVendor=0fde, idProduct=ca01
[ 2256.984881] usb 1-1.4: New USB device strings: Mfr=0, Product=1, SerialNumber=0
[ 2256.984894] usb 1-1.4: Product:  
[ 2256.992719] hid-generic 0003:0FDE:CA01.0002: hiddev0,hidraw0: USB HID v1.10 Device [ ] on usb-3f980000.usb-1.4/input0
</pre>

## Finding the device

`lsusb` gives me the list of the usb devices on my <a type=amzn>rpi</a>:

<pre>
# lsusb 
Bus 001 Device 004: ID 0fde:ca01  
Bus 001 Device 003: ID 0424:ec00 Standard Microsystems Corp. SMSC9512/9514 Fast Ethernet Adapter
Bus 001 Device 002: ID 0424:9514 Standard Microsystems Corp. 
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
</pre>

The first one correspond to my weather station but it belongs to root:

<pre>
# ls -lrt /dev/bus/usb/001/004
crw------- 1 root root 189, 3 Aug 30 12:45 /dev/bus/usb/001/004
</pre>

## Giving access: `udev`

The first thing to do is to allow access to my usb device so I won't need to run any program as root.
By default the `pi` user belongs to a bunch of groups. One of those is called `plugdev`.
It is the one I will use for my experiment.

### Get information about my Device

<pre>
# udevadm info /dev/bus/usb/001/004

P: /devices/platform/soc/3f980000.usb/usb1/1-1/1-1.3
N: bus/usb/001/012
E: BUSNUM=001
E: DEVNAME=/dev/bus/usb/001/012
E: DEVNUM=012
E: DEVPATH=/devices/platform/soc/3f980000.usb/usb1/1-1/1-1.3
E: DEVTYPE=usb_device
E: DRIVER=usb
E: ID_BUS=usb
E: ID_MODEL_ENC=\x20
E: ID_MODEL_FROM_DATABASE=WMRS200 weather station
E: ID_MODEL_ID=ca01
E: ID_REVISION=0302
E: ID_SERIAL=0fde_
E: ID_USB_INTERFACES=:030000:
E: ID_VENDOR=0fde
E: ID_VENDOR_ENC=0fde
E: ID_VENDOR_FROM_DATABASE=Oregon Scientific
E: ID_VENDOR_ID=0fde
E: MAJOR=189
E: MINOR=11
E: PRODUCT=fde/ca01/302
E: SUBSYSTEM=__usb__
E: TYPE=0/0/0
E: USEC_INITIALIZED=5929384
</pre>

I will note the vendor ID and the product ID.
Funny stuff is that it presents itself as a WMRS200 and the model I have is a RMS300, but never mind.

Let's create the udev rule file using the previous informations about the idVendor and the idProduct and create a special file `/dev/weather-station`.
This will make the code more easy as I will be able to hard code the name, and leave the boring task of finding the device aside.

```shell
cat << EOF > /etc/udev/rules.d/50-weather-station.rules
# Weather Station
SUBSYSTEM=="usb", ATTRS{idVendor}=="0fde", ATTRS{idProduct}=="ca01", MODE="0660", GROUP="plugdev", SYMLINK+="weather-station"
EOF
```

Once done, I can restart udev with `sudo /etc/init.d/udev restart` or reload and trigger the rules with `udevadm`

IF something goes wrong, you can check the logs by turning the log level to info, reload the rules and look into the syslog file
```
# udevadm control -l info
# udevadm control -R
# # grep -i udev /var/log/syslog 
# 
```

```
# ls -lrt /dev/weather-station                                                                                                               
lrwxrwxrwx 1 root root 15 Aug 29 21:32 /dev/weather-station -> bus/usb/001/007
# ls -lrt /dev/bus/usb/001/007                                                                                                   
crw-rw-r-- 1 root plugdev 189, 6 Aug 29 21:32 /dev/bus/usb/001/007
```

So far so good...

# Accessing the data

## The libusb
Linux has a low level library "libusb" that make the development of modules easy: [libusb-1.0](http://www.libusb.org/wiki/libusb-1.0).
On my <a type=amzn>rpi</a>, I can install the development version with a simple `sudo apt-get install libusb-1.0-0-dev`.

## Using GO: The `gousb` library

A binding for the libusb is available through the [gousb](https://github.com/truveris/gousb)

There is also a __lsusb__ version that is available as an example.
Let's grab it with a simple
`go get -v github.com/kylelemons/gousb/lsusb`

and test it 

    # ~GOPATH/bin/lsusb
    
    001.004 0fde:ca01 WMRS200 weather station (Oregon Scientific)
      Protocol: (Defined at Interface level)
      Config 01:
        --------------
        Interface 00 Setup 00
          Human Interface Device (No Subclass) None
          Endpoint 1 IN  interrupt - unsynchronized data [8 0]
        --------------
    001.003 0424:ec00 SMSC9512/9514 Fast Ethernet Adapter (Standard Microsystems Corp.)
      Protocol: Vendor Specific Class
      Config 01:
        --------------
        Interface 00 Setup 00
          Vendor Specific Class
          Endpoint 1 IN  bulk - unsynchronized data [512 0]
          Endpoint 2 OUT bulk - unsynchronized data [512 0]
          Endpoint 3 IN  interrupt - unsynchronized data [16 0]
        --------------
    001.002 0424:9514 SMC9514 Hub (Standard Microsystems Corp.)
      Protocol: Hub (Unused) TT per port
      Config 01:
        --------------
        Interface 00 Setup 00
          Hub (Unused) Single TT
          Endpoint 1 IN  interrupt - unsynchronized data [1 0]
        Interface 00 Setup 01
          Hub (Unused) TT per port
          Endpoint 1 IN  interrupt - unsynchronized data [1 0]
        --------------
    001.001 1d6b:0002 2.0 root hub (Linux Foundation)
      Protocol: Hub (Unused) Single TT
      Config 01:
        --------------
        Interface 00 Setup 00
          Hub (Unused) Full speed (or root) hub
          Endpoint 1 IN  interrupt - unsynchronized data [4 0]
      --------------

## Rawread

I want to read the raw data from the device.
The gousb package comes along with an example named "rawread". I'm using it:

<pre>
# rawread git:(master) # go run main.go -device "0fde:ca01"
2016/08/30 14:00:01 Scanning for device "0fde:ca01"...
  Protocol: (Defined at Interface level)
  Config 01:
    --------------
    Interface 00 Setup 00
      Human Interface Device (No Subclass) None
      Endpoint 1 IN  interrupt - unsynchronized data [8 0]
    --------------
2016/08/30 14:00:01 Connecting to endpoint...
2016/08/30 14:00:01 - &usb.Descriptor{Bus:0x1, Address:0x4, Spec:0x110, Device:0x302, Vendor:0xfde, Product:0xca01, Class:0x0, SubClass:0x0, Protocol:0x0, Configs:[]usb.ConfigInfo{usb.ConfigInfo{Config:0x1, Attributes:0x80, MaxPower:0x32, Interfaces:[]usb.InterfaceInfo{usb.InterfaceInfo{Number:0x0, Setups:[]usb.InterfaceSetup{usb.InterfaceSetup{Number:0x0, Alternate:0x0, IfClass:0x3, IfSubClass:0x0, IfProtocol:0x0, Endpoints:[]usb.EndpointInfo{usb.EndpointInfo{Address:0x81, Attributes:0x3, MaxPacketSize:0x8, MaxIsoPacket:0x0, PollInterval:0xa, RefreshRate:0x0, SynchAddress:0x0}}}}}}}}}
2016/08/30 14:00:01 open: usb: claim: libusb: device or resource busy [code -6]
</pre>

After digging into the documentation and forums about the libusb, it looks like the device is locked by a generic kernel driver.
So I need to detach it first.

The API call used to detach a kernel driver is `libusb_detach_kernel_driver`. Sadly it has not be bound to the golang's library.
Indeed [Joseph Poirier](https://github.com/jpoirier) maintain an active fork from the gousb library and he does implement the call.
It's a private method that is called implicitly by another call, so no need to modify the code from rawread to use it.

I've switched to his version:

<pre>
# go get github.com/jpoirier/gousb/rawread
./main -device "0fde:ca01"
2016/08/30 14:12:28 Scanning for device "0fde:ca01"...
  Protocol: (Defined at Interface level)
  Config 01:
    --------------
    Interface 00 Setup 00
      Human Interface Device (No Subclass) None
      Endpoint 1 IN  interrupt - unsynchronized data [8 0]
    --------------
2016/08/30 14:12:28 Connecting to endpoint...
2016/08/30 14:12:28 - &usb.Descriptor{Bus:0x1, Address:0x4, Spec:0x110, Device:0x302, Vendor:0xfde, Product:0xca01, Class:0x0, SubClass:0x0, Protocol:0x0, Configs:[]usb.ConfigInfo{usb.ConfigInfo{Config:0x1, Attributes:0x80, MaxPower:0x32, Interfaces:[]usb.InterfaceInfo{usb.InterfaceInfo{Number:0x0, Setups:[]usb.InterfaceSetup{usb.InterfaceSetup{Number:0x0, Alternate:0x0, IfClass:0x3, IfSubClass:0x0, IfProtocol:0x0, Endpoints:[]usb.EndpointInfo{usb.EndpointInfo{Address:0x81, Attributes:0x3, MaxPacketSize:0x8, MaxIsoPacket:0x0, PollInterval:0xa, RefreshRate:0x0, SynchAddress:0x0}}}}}}}}}
</pre>

Nothing more because the code ends by 

```go
  ep, err := dev.OpenEndpoint(uint8(*config), uint8(*iface), uint8(*setup), uint8(*endpoint)|uint8(usb.ENDPOINT_DIR_IN))
  if err != nil {
      log.Fatalf("open: %s", err)
  }
  _ = ep 
```

Cool... Now let's add some code to read from the endpoint (which is an interface and that implements a Read method as described [here](https://godoc.org/github.com/jpoirier/gousb/usb#Endpoint))

```go
  b := make([]byte, 16)
  _, err = ep.Read(b)
  if err != nil {
      log.Fatalf("read: %s", err)
  }
  log.Printf("%v", b)
  _ = ep 
```

And run the code:

<pre>
go run main.go -device "0fde:ca01"
2016/08/30 14:25:58 Scanning for device "0fde:ca01"...
  Protocol: (Defined at Interface level)
    Config 01:
    --------------
    Interface 00 Setup 00
      Human Interface Device (No Subclass) None
      Endpoint 1 IN  interrupt - unsynchronized data [8 0]
    --------------
2016/08/30 14:25:58 Connecting to endpoint...
2016/08/30 14:25:58 - &usb.Descriptor{Bus:0x1, Address:0x4, Spec:0x110, Device:0x302, Vendor:0xfde, Product:0xca01, Class:0x0, SubClass:0x0, Protocol:0x0, Configs:[]usb.ConfigInfo{usb.ConfigInfo{Config:0x1, Attributes:0x80, MaxPower:0x32, Interfaces:[]usb.InterfaceInfo{usb.InterfaceInfo{Number:0x0, Setups:[]usb.InterfaceSetup{usb.InterfaceSetup{Number:0x0, Alternate:0x0, IfClass:0x3, IfSubClass:0x0, IfProtocol:0x0, Endpoints:[]usb.EndpointInfo{usb.EndpointInfo{Address:0x81, Attributes:0x3, MaxPacketSize:0x8, MaxIsoPacket:0x0, PollInterval:0xa, RefreshRate:0x0, SynchAddress:0x0}}}}}}}}}
2016/08/30 14:25:59 [7 0 48 0 48 53 1 255 7 255 0 66 129 239 0 32]
</pre>

OK! Here are the data, now what I need to figure out, is how to interpret them!

## Decoding the Protocol

Internet is a great tool: I've found a description of the protocol [here](http://www.bashewa.com/wmr200-protocol.php)

I've read that it was mandatory to send a heartbeat sequence every 30 seconds.
I will implement the heartbeat later. For now I will send it initially to request data from the station:

```go
// This is a hearbeat request (9 bytes array)
h := []byte{0x00, 0x01, 0xd0, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00}
log.Println("Sending heartbeat")
i, err := ep.Write(h)
if err != nil {
    log.Fatal("Cannot send heartbeat", err)
}
log.Println("%v bytes sent",i)
```

And then read the stream back. Every data payload is separate from the others by a 0xffff sequence.

### Testing the sequence initialization request

<pre>
 go run main.go -device "0fde:ca01"
2016/08/30 20:02:19 Scanning for device "0fde:ca01"...
Protocol: (Defined at Interface level)
  Config 01:
  --------------
  Interface 00 Setup 00
    Human Interface Device (No Subclass) None
    Endpoint 1 IN  interrupt - unsynchronized data [8 0]
  --------------
  2016/08/30 20:02:19 Connecting to endpoint...
2016/08/30 20:02:19 Sending heartbeat
2016/08/30 20:02:19 heartbeat failed: usb: write: not an OUT endpoint
</pre>

__What did I do wrong?__ 
<center>
![XKCD]( http://imgs.xkcd.com/comics/debugging.png)
</center>

Easy, I didn't RTFM...
Actually, I didn't read the specification of the USB.

As described [here](http://events.linuxfoundation.org/sites/events/files/slides/elc_2014_usb_0.pdf) the USB is a __host-controlled__ bus which means that:

* Nothing on the bus happens without the host first initiating it.
* Devices cannot initiate a transaction.
* The USB is a Polled Bus
* The Host polls each device, requesting data or sending data

The possibles transaction are:

* IN : Device to Host
* OUT: Host to Device

On top of that, a device may handle 1 to N configuration which handles 1 to N endpoints which may be considered IN or OUT.

My weather station has only one endpoint which is IN.
Therefore I will not be able to send information to the station from the host. What I will be able to send is a IN token to get data on the bus.

<pre>
# lsusb -v
...
Endpoint Descriptor:
  bLength                 7
  bDescriptorType         5
  bEndpointAddress     0x81  EP 1 IN
  bmAttributes            3
    Transfer Type            Interrupt
    Synch Type               None
    Usage Type               Data
  wMaxPacketSize     0x0008  1x 8 bytes
  bInterval              10
</pre>

__Note__ I also see that the endpoint is an interrupt

# To be continued...

This blog post is quiet long, and I haven't finished my research yet. Indeed I think that there is enough information for the post to go live.
I will post a part II as soon as I will have time to continue my experiments with the USB device and the <a type=amzn>rpi</a>.


+++
date = "2015-10-22T20:40:36+02:00"
draft = false
title = "Ruby / ZeroMQ / GO" 
description = "My attempt to make a go program talk to a ruby script via a 0MQ message"
tags = [
    "Development",
    "go",
    "ruby",
    "zmq"
]
+++

# Abtract

I really like go as a programming language. It is a good tool to develop web restful API service.

On the other hand, ruby and its framework rails has also been wildly used to achieve the same goal.

Therefore we may be facing a "legacy" ruby developpement that we would like to connect to our brand new go framework.
0MQ may be a perfect choice for intefacing the two languages.

Anyway, it is, at least, a good experience to do a little bit of code to make them communicate.

# ZeroMQ

I will use the ZeroMQ version 4 as it is the latest available one.
On top of that, I can see in the [release notes](http://zeromq.org/docs:changes-4-0-0) that there is an implementation of a strong encryption, and I may use it later on 
# Go

## Installation of the library

As written in the README file, I try a `go get` installation on my chromebook.
```
~ go get github.com/pebbe/zmq4
# pkg-config --cflags libzmq
Package libzmq was not found in the pkg-config search path.
Perhaps you should add the directory containing `libzmq.pc'
to the PKG_CONFIG_PATH environment variable
No package 'libzmq' found
pkg-config: exit status 1
```

The go binding is not a pure go implementation, and it still needs the C library of zmq.

Let's _brew installing_ it:

```
~  brew install zmq
==> Downloading http://download.zeromq.org/zeromq-4.1.3.tar.gz
######################################################################## 100.0%
==> ./configure --prefix=/usr/local/linuxbrew/Cellar/zeromq/4.1.3 --without-libsodium
==> make
==> make install
/usr/local/linuxbrew/Cellar/zeromq/4.1.3: 63 files, 3.5M, built in 73 seconds
```

Let's do the go-get again:

```
~ go get github.com/pebbe/zmq4
```

so far so good. Now let's test the installation with a "hello world" example.

_Note_: the [examples directory](https://github.com/pebbe/zmq4/blob/master/examples) contains a go implementation of all the example of the ZMQ book
I will use the [hello world client](https://github.com/pebbe/zmq4/blob/master/examples/hwclient.go) and the [hello world server](https://github.com/pebbe/zmq4/blob/master/examples/hwserver.go) for my tests

The hello world client/server is implementing a Request-Reply patternt and are communicating via a TCP socket.

* The server is the *replier* and is listening on the TCP port 5555
```go
...
func main() {
    //  Socket to talk to clients
    responder, _ := zmq.NewSocket(zmq.REP)
    defer responder.Close()
    responder.Bind("tcp://*:5555")
    ...
}
```
* The client is the *requester* and is dialing the same TCP port
```go
...
func main() {
    //  Socket to talk to server
    fmt.Println("Connecting to hello world server...")
    requester, _ := zmq.NewSocket(zmq.REQ)
    defer requester.Close()
    requester.Connect("tcp://localhost:5555")
    ...
}
```

Then, the client is sending (requesting) a _hello_ message, and the server is replying a _world_ message.

## Running the example
First, start the server:

```
~ cd $GOPATH/src/github.com/pebbe/zmq4/examples
~ go run hwserver.go
```

Then the client

```
~ cd $GOPATH/src/github.com/pebbe/zmq4/examples
~ go run hwclient.go
Connecting to hello world server...
Sending  Hello 0
Received  World
Sending  Hello 1
Received  World
Sending  Hello 2
...
```

# Ruby

Now let's implement a Ruby client.

## Installation of the library

a _gem install_ is supposed to do the trick:

```
~ gem install zmq
Building native extensions.  This could take a while...
ERROR:  Error installing zmq:
ERROR: Failed to build gem native extension.

/usr/local/linuxbrew/opt/ruby/bin/ruby -r ./siteconf20151022-23021-1ehwusq.rb extconf.rb
    checking for zmq.h... yes
    checking for zmq_init() in -lzmq... yes
    Cool, I found your zmq install...
    creating Makefile

    make "DESTDIR=" clean

    make "DESTDIR="
    compiling rbzmq.c
    rbzmq.c: In function 'socket_getsockopt':
    rbzmq.c:968:7: error: 'ZMQ_RECOVERY_IVL_MSEC' undeclared (first use in this function)
        case ZMQ_RECOVERY_IVL_MSEC:
        ...
```

Arg!, something went wrong. It looks like there is a version mismatch between th libzmq brew installed and the version expected by the gem
The _zmq_ gem seems a bit old and there is a *FFI* ruby extension with a more active developement.

Moreover, I have found []the perfect website for the ruby-and-zmq-ignorant(https://github.com/andrewvc/learn-ruby-zeromq)

As written in the doc, let's install the needed gems via `gem install ffi ffi-rzmq zmqmachine`

## Let's try the lib

Ok, it is now time to run an example

```
require 'rubygems'
require 'ffi-rzmq'
def error_check(rc)
    if ZMQ::Util.resultcode_ok?(rc)
        false
    else
        STDERR.puts "Operation failed, errno [#{ZMQ::Util.errno}] description [#{ZMQ::Util.error_string}]"
        caller(1).each { |callstack| STDERR.puts(callstack)  }
        true
    end
end

ctx = ZMQ::Context.create(1)
STDERR.puts "Failed to create a Context" unless ctx

req_sock = ctx.socket(ZMQ::REQ)
rc = req_sock.connect('tcp://127.0.0.1:5555')
STDERR.puts "Failed to connect REQ socket" unless ZMQ::Util.resultcode_ok?(rc)

2.times do
    rc = req_sock.send_string('Ruby says Hello')
    break if error_check(rc)

    rep = ''
    rc = req_sock.recv_string(rep)
    break if error_check(rc)
    puts "Received reply '#{rep}'"
end
error_check(req_sock.close)

ctx.terminate
```

Running this example with a simple `ruby client.rb` command leads to the following errors:
```
ruby client.rb
Assertion failed: check () (src/msg.cpp:248)
```

But, my GO server is receiving the messages:

```
~ go run hwserver.go
Received  Ruby says Hello
Received  Ruby says Hello
```

# End of show

That's it for now. I think I'm facing a bug in the ruby implementation of the libzmq I'm using. 
Indeed, I've found an [issue](https://github.com/chuckremes/ffi-rzmq/issues/118)... 

I will check again later, or I will try on another environement but the essential is here.
---
author: Olivier Wulveryck
date: 2016-03-31T10:23:02+02:00
description: How to setup RVM on an external drive on a Chromebook
draft: false
keywords:
- ruby
- rvm
- Chromebook
tags:
- Ruby
- Rvm
- Vagrant
- Bundler
- Chromebook
title: RVM from a USB stick on a Chromebook
topics:
- topic 1
type: post
---

# Introduction

#### Opening remarks

I'm not a Ruby developer, and I'm heavily discovering the ecosystem by now.
This are my notes, and if anything seems wrong to you, do not hesitate to send me remarks.

#### The scenario

For testing purpose, I wanted to play with vagrant-aws and more generally with ruby on my Chromebook.

Vagrant does not support _rubygems_ as installation method anymore ([see Mitchell Hashimoto's post](http://mitchellh.com/abandoning-rubygems))
and of course, there is no binary distribution available for the Chromebook.

So I have to install it from the sources.

The [documentation](https://github.com/mitchellh/vagrant/wiki/Installing-Vagrant-from-Source) says:

* Do __NOT__ use the system Ruby - use a Ruby version manager like rvm, chruby, etc

Alright, anyway I don't want to mess with my system and break Homebrew, so using RVM seems to be a good idea.

## Installing RVM

The RVM installation is relatively easy; simply running `curl -sSL https://get.rvm.io | bash` does the trick.
And then those commands make ruby 2.3.0 available via rvm:

```
$ source ~/.rvm/scripts/rvm  
$ rvm install 2.3.0
```

The stupid trick here is that everything is installed in my $HOME directory, and as my Chromebook is short on disk space: FS full !

Too bad.

## Using a USB stick

So my idea is to install the RVM suite onto a USB stick (because with me I don't have any SDHC card available).

### Preparing the stick

At first, the USB stick must be formatted in extendX (ext4) in order to be able to use symlinks, correct ownership etc.

```shell
sudo mkfs.ext4 -L Lexar /dev/sda1
```

__Note__: I've found that avoiding spaces in the volume name was good for rvm.


Once connected on the Chromebook, it's automatically mounted on `/media/removable/Lexar`.
The problem are the options: 

```shell
/dev/sda1 on /media/removable/Lexar type ext4 (rw,nosuid,nodev,noexec,relatime,dirsync,data=ordered)
```

the most problematic is `noexec` because I want to install executables in it.

So what I did was simply:

`sudo mount -o remount /dev/sda1 /media/removable/Lexar`

and that did the trick.

## Installing RVM on the USB

I will install rvm into `/media/removable/Lexar/rvm`. In order to avoid any ownership and permission problem I did:

```shell
mkdir /media/removable/Lexar/rvm
chown chronos:chronos /media/removable/Lexar/rvm
```

And then I created a simple `~/.rvmrc` file as indicated in the documentation with this:

```shell
$ cat ~/.rvmrc                                          
$ export rvm_path=/media/removable/Lexar/rvm
```

I also included this in my `~/.zshrc`

```shell
if [ -s "$HOME/.rvmrc"   ]; then
    source "$HOME/.rvmrc"
fi # to have $rvm_path defined if set
if [ -s "${rvm_path-$HOME/.rvm}/scripts/rvm"   ]; then
    source "${rvm_path-$HOME/.rvm}/scripts/rvm"
fi
```

## Installing rvm

the command I executed were then:

```
$ curl -sSL https://get.rvm.io | bash
$ source /media/removable/Lexar/rvm/scripts/rvm
$ rvm autolibs enable
$ rvm get stable
$ rvm install 2.3.0
```

And that did the trick

```
$ rvm list

rvm rubies

=* ruby-2.3.0 [ x84_64 ]

# => - current
# =* - current && default
#  * - default
```

## Testing with vagrant

### Cloning the vagrant sources

```shell
$ sudo mkdir /media/removable/Lexar/tools
$ sudo chown chronos:chronos /media/removable/Lexar/tools
$ cd /media/removable/Lexar/tools
$ git clone https://github.com/mitchellh/vagrant.git
```

### Preparing the rvm file for vagrant

To use the ruby 2.3.0 (that I've installed before) with vagrant, I need to create a .rvmrc in the vagrant directory:

```
$ cd /media/removable/Lexar/tools/vagrant
$ rvm --rvmrc --create 2.3.0@vagrant
```

### Installing bundler

The bundler version that is supported by vagrant must be <= 1.5.2 as written in the `Gemfile`. So I'm installing version 
1.5.2

```shell
$ cd /media/removable/Lexar/tools/vagrant
$ source .rcmrv
$ gem install bundler -v 1.5.2
```

### Compiling vagrant

Back to the vagrant documentation, what I must do is now to "compile it". To do so, the advice is to run:

```
$ bundle _1.5.2_ install  
```

(just in case several bundler are present )

I faced this error:

```shell
NoMethodError: undefined method `spec' for nil:NilClass
Did you mean?  inspect
An error occurred while installing vagrant (1.8.2.dev), and Bundler cannot continue.
Make sure that `gem install vagrant -v '1.8.2.dev'` succeeds before bundling.
```

According to google, this may be an issue with the version of bundler I'm using.
As I cannot upgrade the bundler because of vagrant, I've decided to take a chance and use
a lower version of Ruby

```shell
$ rvm install 2.2.0
$ rvm --rvmrc --create 2.2.0@vagrant
$ source .rvmrc
# and reinstalling bundler
$ gem install bundler -v 1.5.2            
$ bundle _1.5.2_ install
...
Your bundle is complete!
Use `bundle show [gemname]` to see where a bundled gem is installed.
```

# Voil!

I can now use vagrant installed fully on the USB stick with

```shell
$ bundle _1.5.2_ exec vagrant
Vagrant appears to be running in a Bundler environment. Your 
existing Gemfile will be used. Vagrant will not auto-load any plugins
installed with `vagrant plugin`. Vagrant will autoload any plugins in
the 'plugins' group in your Gemfile. You can force Vagrant to take over
with VAGRANT_FORCE_BUNDLER.

You appear to be running Vagrant outside of the official installers.
Note that the installers are what ensure that Vagrant has all required
dependencies, and Vagrant assumes that these dependencies exist. By
running outside of the installer environment, Vagrant may not function
properly. To remove this warning, install Vagrant using one of the
official packages from vagrantup.com.
...
```

That's it for this post; next I will try to install vagrant-aws and play a little bit with it.

stay tuned.

---
author: Olivier Wulveryck
date: 2015-11-13T09:21:30Z
description: Securing the API with JWT
draft: true
keywords:
tags:
- go
- API
- simple_iaas
title: Securing the API (in GO)
type: post
---

I've created a [couple of posts](http://blog.owulveryck.info/tags/simple-iaas/) about creating an API (mainly in GO).
By now, the API is open. Now I will implement a basic authentication and accreditation mechanism with a little help from [JWT](http://jwt.io)

# What is a Json Web Token

According to the [RFC 7519](https://tools.ietf.org/html/rfc7519), it is a

> compact, URL-safe means of representing
> claims to be transferred between two parties.  The claims in a JWT
> are encoded as a JSON object that is used as the payload of a JSON
> Web Signature (JWS) structure or as the plaintext of a JSON Web
> Encryption (JWE) structure, enabling the claims to be digitally
> signed or integrity protected with a Message Authentication Code
> (MAC) and/or encrypted.

It is a [widely spread mechanism](https://www.google.fr/trends/explore#q=json%20web%20token) to access web services.

_Note:_  JWT is not an authentication framework by itself, and in this post I will assume that the authentication is done via a webservice
that will simply reply `true` or `false` to authorize the user to use the API.

# Defining the user and the authentication API


---
author: Olivier Wulveryck
date: 2015-11-11T14:24:43+01:00
description: Experience with swagger-ui as a documentation tool for the simple iaas api
draft: false
tags:
- swagger
- api
- documentation
- JSON
- REST
- simple-iaas
title: Simple IaaS API documentation with swagger
type: post
---

In a [previous post](http://blog.owulveryck.info/2015/11/10/iaas-like-restfull-api-based-on-microservices/) I have explained how to develop a very simple API server.

Without the associated documentation, the API will be useless. Let's see how we can use [swagger-ui](https://github.com/swagger-api/swagger-ui) 
in this project to generate a beautiful documentation.

*Note* I'm blogging and experimenting, of course, in the "real" life, it's a lot better to code the API interface before implementing the middleware.

# About Swagger

Swagger is a framework. On top of the swagger project is composed of several tools.

The entry point is to write the API interface using the [Swagger Formal Specification](http://swagger.io/specification/). I will the use the [swagger-ui](https://github.com/swagger-api/swagger-ui) to display the documentation.
The swagger-ui can be modified and recompiled, but I won't do it (as I don't want to play with nodejs). Instead I will rely on the "dist" part which can be used "as-is"


# Defining the API interface with Swagger

## Header and specification version:

Swagger comes with an editor which can be used [online](http://editor.swagger.io/#/).

I will use swagger spec 2.0, as I don't see any good reason not to do so. Moreover, I will describe the API using the `YAML` format instead of the JSON format to be human-friendly.

Indeed, in my `YAML` skeleton the header of my specs will then look like this:

```yaml
swagger: '2.0'
info:
  version: 1.0.0
    title: 'Very Simple IAAS'
```

## The node creation: a POST method
Let's document the Node creation (as it is the method that we have implemented before).

The node creation is a `POST` method, that produces a JSON in output with the request ID of the node created.

The responses code may be:

* 202 : if the request has been taken in account
* 400 : when the request is not formatted correctly
* 500 : if any unhanldled exception occurred
* 502 : if the backend is not accessible (either the RPC server or the backend)

So far, the YAML spec will look like:
```yaml
paths:
  /v1/nodes:
    post:
      description: Create a node
      produces:
        - application/json
      responses:
        202:
          description: A request ID.
        400:
          description: |
            When the request is malformated or when mandatory arguments are missing
        500:
          desctiption: Unhandled error
        502:
          description: Execution backend not available
```

So far so good, let's continue with the input payload. The payload will be formatted in JSON, so I add this directive to the model:

```YAML
consumes:
  - application/json
```

I've decided in my previous post that 6 parameters were needed: 

- the kind of os 
- the size of the machine
- the initial disk size allocated
- the lease (in days)
- the environment 
- the description 

All the parameters will compose a payload and therefore will be present in the body of the request.
The YAML representation of the parameters array is:

```YAML
parameters:
  - name: kind
    in: body
    description: "The OS type"
    required: true
  - name: size 
    in: body
    description: "The size of the (virtual) Machine"
    required: true
  - name: disksize
    in: body
    description: "The initial disk capacity allocated"
    required: true
  - name: leasedays
    in: body
    description: "The lease (in days)"
    required: true
  - name: environment_type
    in: body
    description: "The target environment"
  - name: description
    in: body
    description: "The target environment"
```

Sounds ok, but when I test this implementation in the swagger editor for validation, I get this error:

```
Swagger Error
Data does not match any schemas from 'oneOf'
```

_STFWing and RTFMing..._

in the [Specifications](http://swagger.io/specification/#parameterObject), I have found this line:

<html>
If <a href="#parameterIn"><code>in</code></a> is <code>"body"</code>:</p>
<table>
<thead>
<tr>
<th>Field Name</th>
<th style="text-align: center;">Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a name="parameterSchema"></a>schema</td>
<td style="text-align: center;"><a href="#schemaObject">Schema Object</a></td>
<td><stwrong>Required.</strong> The schema defining the type used for the body parameter.</td>
</table>
</html>

Therefore, I should set a schema object for every parameter in order to define its type. In this example, I don't want to go too deeply into the swagger specification, so I won't define any type.

So I have tested the following:
```YAML
parameters:
  - name: kind
    in: body
    description: "The OS type"
    required: true
    schema:
      type: string
  - name: size 
    in: body
    description: "The size of the (virtual) Machine"
    required: true
    schema:
      type: string
    ...
```

And again, I had a validation error from the editor:

<span class="icon"></span> Swagger Error</h5><p class="error-description" ng-bind-html="error.description">Operation cannot have multiple body parameters</p>

_RTFMing..._

> Body - The payload that's appended to the HTTP request. 
> Since there can only be one payload, there can only be one body parameter. 
> The name of the body parameter has no effect on the parameter itself and is used for documentation purposes only. 
> Since Form parameters are also in the payload, body and form parameters cannot exist together for the same operation.

What I must do, is to create a custom type _nodeRequest_ with the input fields as properties and reference it in the body.

Here is the complete structure:

```YAML
parameters:
  - name: nodeRequest 
    in: body
    description: a node request
    required: true
    schema:
        $ref: '#/definitions/NodeRequest'
``` 

And the proper NodeRequest definition in the _definition_ area:

```YAML
definitions:
  NodeRequest:
    description: A Node Request object
    properties:
      kind:
        type: string
        description: The OS type
      size:
        type: string
        description: The size of the (virtual) machine
      disksize:
        type: integer
        format: int32
        description: The initial disk capacity size (in GB)
      leasedays:
        type: integer
        format: int32
        description: The lease
      environment_type:
        type: string
        description: the target environment
      description:
        type: string
```

OK ! The swagger file is valid... Now let's glue it together with swagger-ui and serve it from the GO API server I have developed before

# Integrating swagger-ui

As written in the README in the github of the project, swagger-ui can be used "as-is" using the files in the _dist_ folder. Let's get the files from github:
```shell
/tmp #  git clone https://github.com/swagger-api/swagger-ui.git
Cloning into 'swagger-ui'...
remote: Counting objects: 7292, done.
remote: Compressing objects: 100% (33/33), done.
remote: Total 7292 (delta 8), reused 0 (delta 0), pack-reused 7256
Receiving objects: 100% (7292/7292), 19.20 MiB | 1021.00 KiB/s, done.
Resolving deltas: 100% (3628/3628), done.
Checking connectivity... done.
```

Let's checkout our project:

```shell
/tmp # git clone https://github.com/owulveryck/example-iaas.git 
...
```

and move the `dist` folder into the project:
```
mv /tmp/swagger-ui/dist /tmp/example-iaas
```

## Adding a route to the GO server to serve the static files

I cannot simply add a route in the `routes.go` file for this very simple reason: 
The loop used in the `router.go` is using the `Path` method, and to serve the content of the directory, I need to use the `PathPrefix` method (see [The Gorilla Documentation](http://www.gorillatoolkit.org/pkg/mux#Route.PathPrefix) for more information).

To serve the content, I add this entry to the muxrouter in the `router.go` file:

```go 
router.
       Methods("GET").
       PathPrefix("/apidocs").
       Name("Apidocs").
       Handler(http.StripPrefix("/apidocs", http.FileServer(http.Dir("./dist"))))
```

Then I start the server and point my browser to http://localhost:8080/apidocs...

Wait, nothing is displayed...

# The final test

As I serve the files from the `./dist` directory, what I need to do is to move my `swagger.yaml` spec file into the dist subfolder and tell swagger to read it.

_Et voil!_

<center>
<img class="img-square img-responsive" src="/assets/images/swagger.png" alt="Result"/>
</center>

# Final word

As you can see, there is a "Try out" button, which triggers a `curl` command... Very helpful to enter a test driven development mode.

On top of that swagger is really helpful and may be a great tool to synthesize the need of a client in term of an interface.
Once the API is fully implemented, any client binding may also be generated with the swagger framework.

No not hesitate to clone the source code from [github](https://github.com/owulveryck/example-iaas) and test the swagger.yaml file in the editor to see how the bindings are generated

You can find all the codes in the github repository [here](https://github.com/owulveryck/example-iaas) in the branch `simple-iaas-api-documentation-with-swagger`

The final YAML file can be found [here](https://github.com/owulveryck/example-iaas/blob/simple-iaas-api-documentation-with-swagger/swagger.yaml)
+++
date = "2015-10-23T09:54:27+01:00"
draft = false
title = "Simple polling, a cloud native app - part 1"

+++

In this series of posts I'll explain how to setup a simple polling app, the cloud way.
This app, written in go, will be hosted on a PAAS, and I've chosen the [Google App Engine](https://cloud.google.com/appengine/docs) for convenience.

I will not explain in this post how to setup the Development environment as it is described [here](https://cloud.google.com/appengine/docs/go/gettingstarted/devenvironment)

# A word about the Hosting

Google Apps Engine is a cloud service aim to host applications without worrying about scalability, and technical architecture of the hosting environement.
The application is seen as a web service and proxyfied and load balanced in several compute nodes.
The storage service is provided via a schemaless NoSQL datastore, featuring:

* high availability
* consistency
* ...

and basically all the strong features you would expect from a decent production ready database provider.

To be simple: Take care of the functionnality of your app and respect the cloud principles (see for example the [12factor.net](http://12factor.net) ), and google will take care that it can run in the best conditions.

# The principles of the application

The application is composed of a stateless engine, a data bucket and some web pages for the presentation.

## The engine

The engine is the core of the application. It is the "computational element" that will take care of the inputs of the user and interact with the storage.
It is a [GO](http://golang.org) developement.

## The bucket

It is the data warehouse. It will store the participant name and its answer. It will be NoSQL based, the key will be the username and the value its answer.

## The web pages

* the question "will you participate" and a form input where you will be able to write your name and three buttons "yes", "no" and ""maybe"".
```
Will you participate 
+------------------+  +-----+ +-----+ +-------+
|  Your name       |  | YES | |  NO | | Maybe |
+------------------+  +-----+ +-----+ +-------+
```

* a simple table with two columns:

* One will hold the participant name
* The other one will display its response

```
+---------------------+-------+
|  John doe           | YES   |
+---------------------+-------+
|  Johnny Vacances    | NO    |
+---------------------+-------+
|  Foo Bar            | YES   |
+---------------------+-------+
|  Toto Titi          | NO    |
+---------------------+-------+
|  Pascal Obistro     | YES   |
+---------------------+-------+
```

# Setting up the development environment

First, we will create a directory that will host the sources of our application in our `GOPATH/src`.
_Note_: For convenience I've created a github repo named "google-app-example" to host the complete source.

```
~ mkdir -p $GOPATH/src/github.com/owulveryck/google-app-example
~ cd $GOPATH/src/github.com/owulveryck/google-app-example
~ git init
~ git remote add origin https://github.com/owulveryck/google-app-example
```


## Hello World!

Let's create the hello world first to validate the whole development chain.
As written in the doc, create the two files `hello.go` and `app.yaml`.
Obviously the `simple-polling.go` file will hold the code of the application. Let's focus a bit on the _app.yaml_ file.
The documentation of the _app.yaml_ file is [here](https://cloud.google.com/appengine/docs/go/config/appconfig). The goal of this file is to specifiy the runtime configuration of the engine.
This simple file replace the "integration" task for an application typed "born in the datacenter"

Here is my app.yaml
```yaml
application: simple-polling
version: 1
runtime: go
api_version: go1

handlers:
- url: /.*
  script: _go_app
```

And then we try our application with the command `goapp serve $GOPATH/src/github.com/owulveryck/google-app-example/`
which should display something similar to:
```
INFO     2015-10-26 21:02:10,295 devappserver2.py:763] Skipping SDK update check.
INFO     2015-10-26 21:02:10,468 api_server.py:205] Starting API server at: http://localhost:52457
INFO     2015-10-26 21:02:12,011 dispatcher.py:197] Starting module "default" running at: http://localhost:8080
INFO     2015-10-26 21:02:12,014 admin_server.py:116] Starting admin server at: http://localhost:8000
```

Then I can open my browser (or curl) and point it to http://localhost:8080 to see my brand new "Hello World!" displayed

```
~ curl http://localhost:8080
Hello, world!
```

This ends the part 1 of this serie of articles.
---
author: Olivier Wulveryck
date: 2016-11-19T15:11:59+01:00
description: description
draft: true
keywords:
- key
- words
tags:
- one
- two
title: sound fingerprint
topics:
- topic 1
type: post
---

---
categories:
- category
date: 2017-03-02T17:19:20+01:00
description: ""
draft: true
images:
- /assets/images/default-post.png
tags:
- tag1
- tag2
title: testalex
---

icoucouc

# part I

je veux mettre du `code`

```shell
curl http://
```
---
categories:
- cloud
- distributed systems
date: 2017-02-28T20:57:38+01:00
description: "This is the second part of a series on my attempt to build a deployement language on a cloud scale"
draft: false
images:
- https://upload.wikimedia.org/wikipedia/commons/9/9d/SICP_cover.jpg
tags:
- go
- zygomys
- Lisp
- linda
title: To go and touch Linda's Lisp
---

The title is not a typo nor dyslexia. I will really talk about Lisp.

In a [previous post](/2017/02/03/linda-31yo-with-5-starving-philosophers.../index.html) I explained my will to implement the dining of the philosophers with Linda in GO.

The ultimate goal is to use a distributed and abstract language to go straight from the design to the runtime of an application.

# The problem I've faced

I want to use a GO implementation for the Linda language because a go binary is a container by itself. Therefore if I build my linda language within go, I will be able to run it easily across the computes nodes without any more dependencies.
The point is that the Linda implementation may be seen as a go package. Therefore every implementation of every algorithm must be coded in go. Therefore I will lack a lot of flexibility as I will need one agent per host __and__ per algorithm. For example the binary that will solve the problem of the dining of the philosophers will only be useful for this specific problem.

What would be nice it to use an embedded scripting language. This language would implement the Linda primitives (_in, rd, eval, out_). And the go binary would be in charge to communicate with the tuple space.

## Tuple space: _I want your Sexp_

I have thought a lot about a way to encode my tuples for the tuple space.
Of course go as a lot of encoding available:

- json
- xml
- protobuf
- gob

None of them gave me entire satisfaction. The reason is that go is strongly typed. A tuple must be internally represented by an empty *interface{}* to remain flexible.
Obviously I would need to use a lot of reflexion in my code. And reflexion is not always easy. And a bad implementation leads to an unpredictable code.

To keep it simple (and idiomatic) I took a little refresh about the principles of the reflection. So I took my [book](https://books.google.fr/books/about/The_Go_Programming_Language.html?id=SJHvCgAAQBAJ) about go (I bought it when I started learning the language).

In this book there is a full example about encoding and decoding [s-expression](https://en.wikipedia.org/wiki/S-expression). And what is an s-expression? A tuple! 

__Eureka__!

## Lisp/zygomys

I started working on s-expression... I could have used the parser described in my book and that would be enough for the purpose of my test.
I could have created a package _encoding/sexpr_ and that would do the job.

But the more I was reading about s-expression, the more I was digging in list processing. 

List processing, s-expression, embedded language, functional programming.... That was it: I really needed a lisp based embedded language to implement my linda solution.

![xkcd 297](https://imgs.xkcd.com/comics/lisp_cycles.png)

I found [Zygomys](https://github.com/glycerine/zygomys). This project was a perfect fit for my needs because it seemed stable enough and easily extensible.
The main drawback is that its author decided not to use the godoc format. That is a bit annoying but the documentation exists and is in a wiki. On the other hand the author has replied to all of my solicitations. So I gave it a go.

# The POC

## Implementing the linda primitives in the REPL

The linda primitives are implemented as GO functions. I will more or less use the same structure as the one I have already used in my first attempt.
The go functions will be exported into the repl as documented in the [wiki of zygomys](https://github.com/glycerine/zygomys/wiki/Go-API)

For example the Out function is implemented with this signature:

{{< highlight go >}}
func (l *Linda) Out(env *zygo.Glisp, name string, args []zygo.Sexp) (zygo.Sexp, error) {
    ...
}
{{</ highlight >}}

and it will be presented to the repl by this command:

{{< highlight go >}}
lda := &linda.Linda{}
env := zygo.NewGlisp()
env.AddFunction("out", lda.Out)
{{</ highlight >}}

I have decided to let the linda implementation in a separate package and to implement the repl ad a separate command.

## _etcd_ as a tuple space

My trivial implementation of tuple space based on channels was inaccurate. So I need to implement something more robust.
In the future the tuple space will be distributed at the scale of the cloud.

A raft-based key value store is nowadays a good choice. 
I have chosen to play with etcd by now (but I will try consul later on).

For the moment I will run a single instance locally.

### Linda and etcd
The _out_ and _eval_ statements will write the tuple as the value of a key prefixed by something fixed per session and suffixed by a uuid.

The _In_ statement is a bit trickier:

It will read all the tuples prefixed by the constant defined and try to match the tuple passed as argument.
If it succeeds it will try to delete it.
If it succeeds it will return the tuple. This is needed to avoid a race condition on a single tuple.

If no matching tuple is present in the tuple space, the function watch any PUT event. If the value associated to the event matches the arguments, it tries to delete the tuple from the kv store and returns it.

## Implementing the algorithm in _zygomys_

This tasks gave me a lot of pain.
To be honest I started (once more) to read the book [structure and interpretation of computer program](https://mitpress.mit.edu/sicp/full-text/book/book.html).

There is a beautiful and functional way to implement the algorithm in lisp. I am sure about that.

But I will figure it out later.

By now, what I did was simply to transpile the algorithm in the lisp syntax thanks to the `begin` statement of _zygo_ (see the [section Sequencing in the wiki of zygomys](https://github.com/glycerine/zygomys/wiki/Language)).

This is how it looks like:

{{< highlight lisp >}}
(defn phil [i num] (
  (begin
    (think i)
    (in "room ticket")
    (in "chopstick" i)
    (in "chopstick" (mod (+ i 1) num))
    (eat i)
    (out "chopstick" i)
    (out "chopstick" (mod (+ i 1) num))
    (out "room ticket")
    (phil i num))))
{{</ highlight >}}

## Execution

_etcd_ daemon needs to be launched first. And by now it needs a clean database to avoid any side effect (I still have a lot of TODO in my code).

Then launch the linda repl with the example:

<pre>
=> localhost cmd git:(master) # ./cmd ../example/dinner/dinner.zy
Creating chopstick 0
Creating philosopher 0
Creating room ticket
0 is thinking
Creating chopstick 1
Creating philosopher 1
Creating room ticket
1 is thinking
Creating chopstick 2
Creating philosopher 2
Creating room ticket
2 is thinking
Creating chopstick 3
Creating philosopher 3
Creating room ticket
3 is thinking
Creating chopstick 4
Creating philosopher 4
4 is thinking
/4 is thinking
4 is in the room
4 took the 4's chopstick
4 took the 0's chopstick
4 is eating
/0 is thinking
0 is in the room
/3 is thinking
3 is in the room
/2 is thinking
2 is in the room
3 took the 3's chopstick
2 took the 2's chopstick
...
</pre>

# Conclusion and future work

This implementation is a start. The code needs a lot of tweaking though. The linda implementation is still far to be complete.

But by now I won't pass much time in implementing some more feature because I have enough to play with my philosophers.
What I would like to do next is to distribute my philosophers and use the real power of etcd.

I would like to start the REPL on 5 different hosts all linked by etcd.
Then I will inject my lisp code in the tuple space and wait for the philosophers to think and eat.

If you are curious, the code is on [the github of the ditrit project](https://github.com/ditrit/go-linda)

Meanwhile, let's have a drink and relax with a good sound:

<iframe src="https://embed.spotify.com/?uri=spotify:track:4QwzVlAJSkcLeCNQ6Ug30P&theme=white" width="280" height="80" frameborder="0" allowtransparency="true"></iframe>
---
author: Olivier Wulveryck
date: 2015-11-20T10:09:30Z
description: A tosca lifecycle represented as a digraph
draft: false
tags:
- TOSCA
- Digraph
- Graph Theory
- golang
title: TOSCA lifecycle as a digraph
topics:
- TOSCA
type: post
---

# About TOSCA

The [TOSCA](https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=tosca) acronym stands for 
_Topology and Orchestration Specification for Cloud Applications_. It's an [OASIS](https://www.oasis-open.org) standard.

The purpose of the TOSCA project is to represent an application by its topology and formalize it using the TOSCA grammar.

The [[TOSCA-Simple-Profile-YAML-v1.0]](http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/csprd01/TOSCA-Simp$le-Profile-YAML-v1.0-csprd01.html) 
current specification in YAML introduces the following concepts.

> - TOSCA YAML service template: A YAML document artifact containing a (TOSCA) service template that represents a Cloud application.
> - TOSCA processor: An engine or tool that is capable of parsing and interpreting a TOSCA YAML service template for a particular purpose. For example, the purpose could be validation, translation or visual rendering.
> - TOSCA orchestrator (also called orchestration engine): A TOSCA processor that interprets a TOSCA YAML service template then instantiates and deploys the described application in a Cloud.
> - TOSCA generator: A tool that generates a TOSCA YAML service template. An example of generator is a modeling tool capable of generating or editing a TOSCA YAML service template (often such a tool would also be a TOSCA processor).
> - TOSCA archive (or TOSCA Cloud Service Archive, or CSAR): a package artifact that contains a TOSCA YAML service template and other artifacts usable by a TOSCA orchestrator to deploy an application.

## My work with TOSCA

I do believe that TOSCA may be a very good leverage to port a "legacy application" (aka _born in the datacenter_ application) into a cloud ready application without rewriting it completely to be cloud compliant.
To be clear, It may act on the hosting and execution plan of the application, and not on the application itself.

A single wordpress installation in a TOSCA way as written [here](http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/csprd01/TOSCA-Simple-Profile-YAML-v1.0-csprd01.html#_Toc430015847) is represented like that

<img class="img-square img-responsive" src="http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.0/csprd01/TOSCA-Simple-Profile-YAML-v1.0-csprd01_files/image035.png" alt="Single Wordpress representation"/>

While I was learnig GO, I have developped a [TOSCA lib](https://github.com/owulveryck/toscalib) and a [TOSCA processor](https://github.com/owulveryck/toscaviewer) which are, by far, not _idiomatic GO_.

Here are two screenshots of the rendering in a web page made with my tool (and the graphviz product):

<hr/>
*The graph representation of a _Single instance wordpress_*
<img class="img-responsive" src="/assets/images/toscaviewer_template_def.png" alt="Tosca view ofthe single instance wordpress"/>


*The graph representation of a lifecycle of _Single instance wordpress_*
<img class="img-responsive" src="/assets/images/toscaviewer_lifecycle_def.png" alt="Lifecycle representation of the single wordpress instance representation"/>
<hr/>

The TOSCA file is parsed with the help of the `TOSCALIB` and then it fills an adjacency matrix (see [FillAdjacencyMatrix](https://godoc.org/github.com/owulveryck/toscalib#ToscaDefinition.FillAdjacencyMatrix))

The [graphviz](http://graphviz.org) take care of the (di)graph representation.

What I would like to do now, is a little bit more: I would like to play with the graph and query it
Then I should perform requests on this graph. For example I could ask:

* _What are the steps to go from the state Initial of the application, to the state running_ ?
* _What are the steps to go from stop to delete_
* ...

and that would be **the premise of a TOSCA orchestrator**.

## The digraph go code

I've recently discoverd the [digraph](https://github.com/golang/tools/tree/master/cmd/digraph) tool, that I will use for querying the graphs.
The `digraph` is represented as a map with a node as a key and its immediates successors as values:

```go
// A graph maps nodes to the non-nil set of their immediate successors.
type graph map[string]nodeset

type nodeset map[string]bool
```

## From TOSCA to digraph

What I must do is to parse the adjacency matrix, get the "lifecycle action" related to the id and fill the graph g.

# Let's go 

Considering the digraph code, what I need to do is simply to override the `parse` method.

## Principle

I will fill the `graph` with a string composed of _nodename:action_ as key.
For example, if I need to do a "Configure" action of node "A" after a "Start" action on node "B", I will have the following entry in the map:

```go
g["B:Start"] = "A:Configure"
```

So What I need to do is to parse the adjjacency matrix, do a matching with the row id and the "node:action" name, and fill the `graph g` with the matching of the corresponding "node:action".

I will fill a `map` with the id of the node:action as key and the corresponding label as values:
```gloang
for node, template := range toscaTemplate.TopologyTemplate.NodeTemplates {
        ids[template.GetConfigureIndex()] = fmt.Sprintf("%v:Configure", node)
        ids[template.GetCreateIndex()] = fmt.Sprintf("%v:Create", node)
        ids[template.GetDeleteIndex()] = fmt.Sprintf("%v:Delete", node)
        ids[template.GetInitialIndex()] = fmt.Sprintf("%v:Initial", node)
        ids[template.GetPostConfigureSourceIndex()] = fmt.Sprintf("%v:PostConfigureSource", node)
        ids[template.GetPostConfigureTargetIndex()] = fmt.Sprintf("%v:PostconfigureTarget", node)
        ids[template.GetPreConfigureSourceIndex()] = fmt.Sprintf("%v:PreConfigureSource", node)
        ids[template.GetPreConfigureTargetIndex()] = fmt.Sprintf("%v:PreConfigureTarget", node)
        ids[template.GetStartIndex()] = fmt.Sprintf("%v:Start", node)
        ids[template.GetStopIndex()] = fmt.Sprintf("%v:Stop", node)
}
```

Then I can easily fill the `graph g` from the adjacency matrix:

```gloang
row, col := toscaTemplate.AdjacencyMatrix.Dims()
        for r := 1; r < row; r++ {
                for c := 1; c < col; c++ {
                        if adjacencyMatrix.At(r, c) == 1 {
                                g.addEdges(ids[r], ids[c])
                        }
                }
        }
```

That's it

# The final function

Here is the final parse function
```go
func parse(rd io.Reader) (graph, error) {
        g := make(graph)
        // Parse the input graph.
        var toscaTemplate toscalib.ToscaDefinition
        err := toscaTemplate.Parse(rd)
        if err != nil {
                return nil, err
        }
        // a map containing the ID and the corresponding action
        ids := make(map[int]string)
        // Fill in the graph with the toscaTemplate via the adjacency matrix
        for node, template := range toscaTemplate.TopologyTemplate.NodeTemplates {
                // Find the edges of the current node and add them to the graph

                ids[template.GetConfigureIndex()] = fmt.Sprintf("%v:Configure", node)
                ids[template.GetCreateIndex()] = fmt.Sprintf("%v:Create", node)
                ids[template.GetDeleteIndex()] = fmt.Sprintf("%v:Delete", node)
                ids[template.GetInitialIndex()] = fmt.Sprintf("%v:Initial", node)
                ids[template.GetPostConfigureSourceIndex()] = fmt.Sprintf("%v:PostConfigureSource", node)
                ids[template.GetPostConfigureTargetIndex()] = fmt.Sprintf("%v:PostconfigureTarget", node)
                ids[template.GetPreConfigureSourceIndex()] = fmt.Sprintf("%v:PreConfigureSource", node)
                ids[template.GetPreConfigureTargetIndex()] = fmt.Sprintf("%v:PreConfigureTarget", node)
                ids[template.GetStartIndex()] = fmt.Sprintf("%v:Start", node)
                ids[template.GetStopIndex()] = fmt.Sprintf("%v:Stop", node)
        }

        row, col := toscaTemplate.AdjacencyMatrix.Dims()
        for r := 1; r < row; r++ {
                for c := 1; c < col; c++ {
                        if adjacencyMatrix.At(r, c) == 1 {
                                g.addEdges(ids[r], ids[c])
                        }
                }
        }
        return g, nil
}

```
# Grab the source and compile it

I have a github repo with the source.
It is go-gettable 
```
go get github.com/owulveryck/digraph
cd $GOPATH/src/github.com/owulveryck/digraph && go build
```

**EDIT** As I continue to work on this tool, I have created a "blog" branch in the github which holds the version related to this post

# Example

I will use the the same example as described below: the single instance wordpress.

I've extracted the YAML and placed in in the file [tosca_single_instance_wordpress.yaml](https://github.com/owulveryck/toscaviewer/blob/master/examples/tosca_single_instance_wordpress.yaml).

Let's query the nodes first:
```sh
curl -s https://raw.githubusercontent.com/owulveryck/toscaviewer/master/examples/tosca_single_instance_wordpress.yaml | ./digraph nodes
mysql_database:Configure
mysql_database:Create
mysql_database:Start
mysql_dbms:Configure
mysql_dbms:Create
mysql_dbms:Start
server:Configure
server:Create
server:Start
webserver:Configure
webserver:Create
webserver:Start
wordpress:Configure
wordpress:Create
wordpress:Start

```

so far, so good...

Now, I can I go from a `Server:Create` to a running instance `wordpress:Start`

```
curl -s https://raw.githubusercontent.com/owulveryck/toscaviewer/master/examples/tosca_single_instance_wordpress.yaml | ./digraph somepath server:Create wordpress:Start
server:Create
server:Configure
server:Start
mysql_dbms:Create
mysql_dbms:Configure
mysql_dbms:Start
mysql_database:Create
mysql_database:Configure
mysql_database:Start
wordpress:Create
wordpress:Configure
wordpress:Start
```

Cool!

# Conclusion

The tool sounds ok. What I may add:

- a command to display the full lifecycle (finding the entry and the exit points in the matrix and call somepath with it)
- get the tosca `artifacts` and display them instead of the label to generate a deployment plan
- execute the command in `goroutines` to make them concurrent


And of course validate any other TOSCA definition to go through a bug hunting party
---
categories:
- category
date: 2017-04-30T21:16:38+02:00
description: ""
draft: true
images:
- http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.1/csprd02/TOSCA-Simple-Profile-YAML-v1.1-csprd02_files/image004.png
tags:
- tag1
- tag2
title: tosca n quads
---

# Tosca

![http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.1/csprd02/TOSCA-Simple-Profile-YAML-v1.1-csprd02_files/image004.png](http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.1/csprd02/TOSCA-Simple-Profile-YAML-v1.1-csprd02_files/image004.png)

{{< highlight yaml >}}
tosca_definitions_version: tosca_simple_yaml_1_0
 
description: Template for deploying MySQL and database content.
 
topology_template:
  inputs:
    # omitted here for brevity
 
  node_templates:
    my_db:
      type: tosca.nodes.Database.MySQL
      properties:
        name: { get_input: database_name }
        user: { get_input: database_user }
        password: { get_input: database_password }
        port: { get_input: database_port }
      artifacts:
        db_content:
          file: files/my_db_content.txt
          type: tosca.artifacts.File
      requirements:
        - host: mysql
      interfaces:
        Standard:
          create:
            implementation: db_create.sh
            inputs:
              # Copy DB file artifact to server's staging area
              db_data: { get_artifact: [ SELF, db_content ] }
 
    mysql:
      type: tosca.nodes.DBMS.MySQL
      properties:
        root_password: { get_input: mysql_rootpw }
        port: { get_input: mysql_port }
      requirements:
        - host: db_server
 
    db_server:
      type: tosca.nodes.Compute
      capabilities:
        # omitted here for brevity
{{</ highlight >}}


# n-quads 


{{< highlight yaml >}}
"topology_template" "input" "database_name" .
"topology_template" "input" "database_user" .
"topology_template" "input" "database_password" .
"topology_template" "input" "database_port" .
"topology_template" "input" "mysql_rootpw" .
"topology_template" "input" "mysql_port" .
"my_db" "type" "tosca.nodes.Database.MySQL" .
"my_db" "property" "name" .
"name" "value" "database_name" .
"my_db" "property" "user" .
"user" "value" "database_user" .
"my_db" "property" "password" .
"password" "value" "database_password" .
"my_db" "property" "dbport" .
"dbport" "value" "database_port" .
"my_db" "artifact" "db_content" .
"db_content" "file" "files/my_db_content.txt" .
"db_content" "type" "tosca.artifacts.File" .
"my_db" "interface" "Standard" .
"my_db" "interface_create" "create" .
"my_db" "requires" "mysql" .
"create" "implementation" "db_create.sh" .
"create" "input" "db_data" .
"db_data" "value" "db_content" .
"mysql" "type" "tosca.nodes.DBMS.MySQL" .
"mysql" "property" "root_password" .
"root_password" "value" "mysql_rootpw" .
"mysql" "property" "port" .
"port" "value" "mysql_port" .
"mysql" "requires" "db_server" .
"db_server" "type" "tosca.nodes.Compute" .
{{</ highlight >}}

Let's visualize that:

{{< highlight bash >}}
#!/usr/bin/zsh
(
  echo "digraph G {"
  cat $1 | while read subject predicate object trash 
  do 
      echo "$subject -> $object [ label = "$predicate" ];"
  done
  echo "}"
) | dot -Tsvg > output.svg
{{</ highlight >}}

![Output](/assets/images/tosca-n-quads.svg)
---
author: Olivier Wulveryck
date: 2016-06-09T14:54:26+02:00
description: Simple web page generated on the fly on the client side with jquery and underscore.js whit data provided in json by a webservice
draft: true
keywords:
- json
- bootstrap
- jquery
- underscore.js
tags:
- json
- bootstrap
- jsquery
- underscore.js
title: A skeleton to display and web-format data got from web service (in JSON)
type: post
---


---
author: Olivier Wulveryck
date: 2015-11-30T13:17:41Z
description: Two reasons why I usually use KSH93 as script engine
draft: false
keywords:
- ksh
- shell
- getopts
- man
tags:
- ksh
- bash
- shell
- getopts
- man
title: KSH93 cool features for scripting
type: post
---

From time to time, I'm involved into a trolling conversation when any linux kiddie tells me:

_Bash is really the superior shell_ 

I totally disagree, but as I'm getting older, I don't argue anymore.

Anyway, in this post I will expose two arguments, or I should say two reasons, why I usually use `ksh93` to run my scripts.

**Note** I'm really talking about the engine of the script, (the shebang definition). 
set I'm used to the bourn shell syntax therefore I also exclude any C shell from the comparison.
My `$SHELL` for interactivity is `zsh` because it's efficient enough
and it has a bunch of really cool features I won't discuss in this post (maybe later)

# Read, loops, forks and efficiency...

More than 10 years ago, as I was working for a project at IBM, my excellent team leader told me to refer to this book: 
[Unix Power Tools](http://shop.oreilly.com/product/9780596003302.do). I did learn a lot with it.

And one feature I've always used is the `while read` loop.

## The use case
Let's take this script as example:

```sh
$ cat test                                                                                                         
for i in $(seq 1 500)
do
    echo $i | read a
    echo -ne "$a\r"
done
echo ""
```

It simply iterate 500 times and display the counter on the screen.

## The result of execution

Let's execute it in different shells

```sh
for i in bash zsh ksh                                                                                         
do
    echo "$i =>"
    eval $i test
done
bash =>

zsh =>
500
ksh =>
500
```

Bash is the only one which does not display the expected result.
The explanation is that the shell sees a pipe and the fork the process. The assignation to the variable `a` is in another context and therefore, 
when the father wants to display `$a` in the current shell, the variable is empty.

Wait, but why does `ksh` (and `zsh`) do display the correct result ? 
Simply because ksh and zsh have noticed that the command after the pipe was a builtin, and therefore that it was un-useful to fork.

### Strace to the rescue...

To prove it, let's check for syscalls with the `strace` tool, and count how many clones and calls are performed: 

```sh
$ for i in bash zsh ksh                                                                                         
do
    echo "$i =>"
    strace -c  $i  test 2>&1 | egrep "clone|calls"
done
bash =>
% time     seconds  usecs/call     calls    errors syscall
56.05    0.067081          67      1001           clone
zsh =>
% time     seconds  usecs/call     calls    errors syscall
71.57    0.057681         115       501           clone
ksh =>
% time     seconds  usecs/call     calls    errors syscall
68.50    0.042059          84       500           clone
```

_quod erat demonstrandum_, twice as much clone in bash thant in ksh|zsh.

## Efficiency

Of course this as an impact on performances, because fork are expensive, let's query the execution time:

```sh
for i in bash zsh ksh                                                                                         
do
    echo "$i =>"
    eval time $i test
done
bash =>

bash test  0,17s user 0,86s system 95% cpu 1,079 total
zsh =>
500
zsh test  0,08s user 0,46s system 82% cpu 0,648 total
ksh =>
500
ksh test  0,07s user 0,46s system 65% cpu 0,819 total
```

This sounds clear to me...


# The KSH93 Getopts unknown feature

Another cool feature I've discovered recently is the little addon of the getopts feature.

I wanted to use the `getopts` built in in a script. As usual, I did _RTFM_ (because I never know when to use colon etc.).

Here is the extract of the man page of ksh93 relative to the getopts function:


<pre>
<B>getopts</B> [ <B>-a</B> <I>name</I> ] <I>optstring vname</I> [ <I>arg</I> ... ]

Checks <I>arg</I> for legal options.  If <I>arg</I> is omitted, the positional parameters are used.

An option argument begins with a <B>+</B> or a <B>-</B>.  An option not beginning with <B>+</B> or <B>-</B> or the argument <B>--</B> ends the options.
Options beginning with <B>+</B> are only recognized when <I>optstring</I> begins with a <B>+</B>.

<I>optstring</I> contains the letters that <B>getopts</B> recognizes.
If a letter is followed by a <B>:</B>, that option is expected to have an argument.
The options can be separated from the argument by blanks.
The option <B>-?</B> causes <B>getopts</B> to generate a usage message on standard error.
The <B>-a</B> argument can be used to specify the name to use for the usage message, which defaults to <B>$0</B>.

<B>getopts</B> places the next option letter it finds inside variable <I>vname</I> each time it is invoked.
The option letter will be prepended with a <B>+</B> when <I>arg</I> begins with a <B>+</B>.
The index of the next <I>arg</I> is stored in <FONT SIZE="-1"><B>OPTIND</B>.
</FONT> The option argument, if any, gets stored in <FONT SIZE="-1"><B>OPTARG</B>.  </FONT> 

A leading <B>:</B> in <I>optstring</I> causes <B>getopts</B> to store the letter of an invalid option in <FONT SIZE="-1"><B>OPTARG</B>, </FONT>
and to set <I>vname</I> to <B>?</B> for an unknown option and to <B>:</B> when a required option argument is missing.
Otherwise, <B>getopts</B> prints an error message.
The exit status is non-zero when there are no more options.

<P> There is no way to specify any of the options <B>:</B>, <B>+</B>, <B>-</B>, <B>?</B>, <B>[</B>, and <B>]</B>.

The option <B>#</B> can only be specified as the first option. 
</pre>

This particular sentence, in the middle of the documentation peaked my interest

> The option -? causes getopts to generate a usage message on standard error.

What? We can generate usage with getopts? 

Cool, any script should be documented, but any documentation should not be difficult to implement.

<img class="img-responsive center-block" src="http://imgs.xkcd.com/comics/manuals.png"> 
<center>_https://xkcd.com/1343/_</center>


I did googled and found this 
[web page](http://docstore.mik.ua/orelly/unix3/korn/appb_11.htm) which is an extract from this book [Learning the Korn Shell](http://shop.oreilly.com/product/9780596001957.do)

An example is sometimes better than an explanation (and the book is complete on this subject)
## The example

### The script
```sh
#!/bin/ksh

ENV=dev
MPATH=/tmp
##
### Man usage and co...

USAGE="[-?The example script v1.0]"
USAGE+="[-author?Olivier Wulveryck]"
USAGE+="[-copyright?Copyright (C) My Blog]"
USAGE+="[+NAME?$0 --- The Example Script]"
USAGE+="[+DESCRIPTION?The description of the script]"
USAGE+="[u:user]:[user to run the command as:=$USER?Use the name of the user you want to sudo to: ]"
USAGE+="[e:env]:[environnement:=$ENV?environnement to use (eg: dev, prod) ]"
USAGE+="[p:path]:[Execution PATH:=$MPATH?prefix of the chroot]"
USAGE+="[+EXAMPLE?$0 action2]"
USAGE+='[+SEE ALSO?My Blog Post: http://blog.owulveryck.info/2015/11/30/ksh93-cool-features-for-scripting]'
USAGE+="[+BUGS?A few, maybe...]"

### Option Checking

while getopts "$USAGE" optchar ; do
    case $optchar in
        u)  USER=$OPTARG
        ;;
        e)  ENV=$OPTARG
        ;;
        p)  PATH=$OPTARG
        ;;
    esac
done
shift OPTIND-1
ACTION=$1
```
### The invocation

Here are two _singing_ examples of the usage output (sorry, I'm tired)
#### _Ballad of a thin man_
```sh
$ ./blog.ksh --man
NAME
  ./blog.ksh --- The Example Script

SYNOPSIS
  ./blog.ksh [ options ]

DESCRIPTION
  The description of the script

OPTIONS
  -u, --user=user to run the command as
                  Use the name of the user you want to sudo to: The default value is owulveryck.
  -e, --env=environnement
                  environnement to use (eg: dev, prod) The default value is dev.
  -p, --path=Execution PATH
                  prefix of the chroot The default value is /tmp.

EXAMPLE
  ./blog.ksh action2

SEE ALSO
  My Blog Post: http://blog.owulveryck.info/2015/11/30/ksh93-cool-features-for-scripting

BUGS
  A few, maybe...

IMPLEMENTATION
  version         The example script v1.0
  author          Olivier Wulveryck
  copyright       Copyright (C) My Blog
```

#### I'm gonna try _with a little help (from my friends)_

```sh
$ ./blog.ksh --help
Usage: ./blog.ksh [ options ]
OPTIONS
  -u, --user=user to run the command as
                  Use the name of the user you want to sudo to: The default value is owulveryck.
  -e, --env=environnement
                  environnement to use (eg: dev, prod) The default value is dev.
  -p, --path=Execution PATH
                  prefix of the chroot The default value is /tmp.
```

And let's try with an invalid option...

```sh
  ./blog.ksh -t
./blog.ksh: -t: unknown option
Usage: ./blog.ksh [-u user to run the command as] [-e environnement] [-p Execution PATH]
```

# Conclusion

By now, KSH93 remains my favorite engine for shell scripts, but is sometimes replaced by ZSH.

Actually, ZSH seems as "smart" and efficient, but this `getopts` feature is really nice for any script aim to be distributed widely.
---
date: 2017-04-14T23:00:59+02:00
description: "In this post I will describe my experience with extreme programming. I have tested it in conjunction with scrum, and I have been impressed by the results. I will try to explain why it is, according to me, a very good leverage of digital transformation for Ops team."
draft: false
images:
- /assets/images/Extreme_Programming.png
tags:
- agile
- xtreme programming
- ops
- dev
title: I have tried Extreme Programming within a sprint and I think it is an excellent agile method for the Ops!
---

# Part I: Agility

## 2003

I have discovered the notion of extreme programming more than 15 years ago. My job was to integrate *and* to develop pieces of code in Java for the IBM Websphere Business Integration server.
We were a small team with light programming skills. A part of our job was to operate the software, the other part was to develop. It was in 2003.

We were trying hard to stick to the specific framework we developed. 

Of course, in 2003, no French company I have been working for was talking about agility, The minimum viable product was not an option. The client wanted the full viable product delivered on time.

One of those morning where we were trying to find the motivation to do another step in the unknown, a colleague told us about a "new concept" he read about: Extreme Programming.

He explained that we should work in pairs. He told us that we should test every single feature even before actually implementing it, and so many other things... The ideas were good, but the milestones of the project were short. Obviously we were _too busy to innovate_ .

(By the time, as I grew up, I learned that _Good ideas don't always win!)_ 

{{< figure src="https://imgs.xkcd.com/comics/mobile_marketing.png" link="https://xkcd.com/1327/" caption="" >}}

<small>_Note:_ I intentionally put this XKCD as a reminder that a "good idea" is an abstract concept. Therefore, I insist on the fact that this blog reflects my own opinion. Even if I remain sure that it was a good idea, maybe it was not :)</small>

At the end of the project, when the [Deming wheel](https://en.wikipedia.org/wiki/PDCA) turned, I noticed that we were missing of agility.

## 2017

It is now 2017. Every IT crew thinks that agility is the way to work. 
Some of them have enough support from their management to actually implement an agile method.
Others may not be mature enough, but are pushed by the neverending decrease of the time to market to try new methods of work and delivery.

Agile and DevOps are, according the trends, the methods to use; it is seen as the holy grail.
But when it comes to agility, people are usually restricting it to an implementation of [scrum](https://en.wikipedia.org/wiki/Scrum_(software_development)) or [kanban](https://en.wikipedia.org/wiki/Kanban_(development)).

Others methods such as Extreme Programming remains rare. See for example this Google trends chart about agility in IT:

<script type="text/javascript" src="https://ssl.gstatic.com/trends_nrtr/981_RC01/embed_loader.js"></script> <script type="text/javascript"> trends.embed.renderExploreWidget("TIMESERIES", {"comparisonItem":[{"keyword":"/m/02t2n","geo":"","time":"2012-03-18 2017-03-18"},{"keyword":"/m/02zhbn","geo":"","time":"2012-03-18 2017-03-18"},{"keyword":"/m/0ck_p8","geo":"","time":"2012-03-18 2017-03-18"},{"keyword":"/m/01mwhw","geo":"","time":"2012-03-18 2017-03-18"}],"category":0,"property":""}, {"exploreQuery":"date=2012-03-18%202017-03-18&q=%2Fm%2F02t2n,%2Fm%2F02zhbn,%2Fm%2F0ck_p8,%2Fm%2F01mwhw","guestPath":"https://trends.google.com:443/trends/embed/"}); </script> 

On the other hand, [lean](https://en.wikipedia.org/wiki/Lean_software_development) concepts are usually referenced in every single agile documentation. But the echo of the lean principles is not strong enough. And still, IT crew usually refers to those methods as _only good for pure dev teams_ and _we are not devs_ (and trust me: if I had had a cent every time I faced this sentence, I would be rich). 

Ops should not be opposed to Devs. That is a fact, but why? 

Because Ops are also developers. Actually Ops are developing their business. 
In the era of "services everywhere" (XaaS), operational teams (as opposed to business team) must define contracts of services. Therefore, they must develop the services to fulfill the contracts.
They are a business team; even if their business is not related to the core business of the company they are working for.

Take a look at Amazon. AWS' core business is computer centric, but at the beginning it was only the "ops department" of a retailer.

# Part II: Using extreme programming in an "Ops" team

At the present time, I am sub-contractor for a retail company. My job is to give hints and hands to the operational teams. Their goal is to serve the business in a way efficient enough in order to follow the growth of the core business (and it is growing fast). 

## Context

Let me define the context of my job.
I am working in a team whose goal is to expose IaaS based on public cloud offers. Among other services, we want to provide to our customers a service of file transfer.
The transfer engine is an existing product. What we have decided to do is to add a RESTful API in front of the engine (this is a shortcut for clarity).

The team is composed of 4 people (mainly ops). But only one of them really knows the transfer engine. Therefore he has been designed as the legitimate implementer of the web service.

This person is my colleague [Alexandre](https://www.linkedin.com/in/alexandre-hisette-aa1076a/) from [Techsys](https://www.linkedin.com/company-beta/719121/). He is a certified system engineer. And best of all he did not tell me _I am not a dev_.

Regarding my job, I was assigned to another project that was also involving API management.

One last thing to know: the Team is also experimenting a Scrum method. We are "sprinting" for the releasing our products.

## How did it get extreme

Alex started to implement its API gateway. I managed to convince the team to use the go language (telling how is not the point of this article). We were exchanging about the implementation, the design, and the language.
At a certain point, for the past sprint, we started to work together by really sharing a screen.

When we were not sure about the design, we were instantly brainstorming around a coffee.

We decided to write the tests with a goal of 100% of code covered. 
When he was busy with something else, I wrote some tests for him, and his job, when I was by myself busy on something else, was to actually implement the code that was giving 100% of success.

Is that extreme programming? Let's recap.

## **What is Extreme Programming**

Even if this section is a vague copy/paste from [wikipedia](https://en.wikipedia.org/wiki/Extreme_programming) it is time to define some concepts of XP.
(I strongly encourage you to read the wikipedia article though)

Extreme programming is a software development methodology.

The activities of XP are:

* Coding
* Testing
* Listening
* Designing

Why is that extreme? Because all of the activities are taken to their extreme level. For example: regarding the tests, not only the business logic is tested. but every single component of the software is fully tested. (remember our goal of 100% code coverage? Yes we are extreme!)

Regarding the practices of extreme programming: there is 12 practices grouped in 4 areas (again [wikipedia](https://en.wikipedia.org/wiki/Extreme_programming_practices) is the place to go after this blog post).

* Fine-scale feedback
 * Pair programming
 * Planning game
 * Test-driven development
 * Whole team
* Continuous process
 * Continuous integration
 * Refactoring or design improvement
 * Small releases
* Shared understanding
 * Coding standards
 * Collective code ownership
 * Simple design
 * System metaphor
* Programmer welfare
 * Sustainable pace

## So are we extreme?

Yes! 

Because we are doing pair-programming. 

Because by using scrum, we used the planning poker. 

Because we are extremely testing our app.  

Because of the sprint releases, we are doing small releases

Because go impose the coding standards

Because we were proud of what we did, and this provided welfare

And probably many other things I cannot list in a single blog post.
# Conclusion

In the sprint review meeting, of course my own goals were not reached (The goals of my own project). I have passed too many times to work with my colleague to complete my own tasks.

My "product" was enhanced of 2% instead of 5%, but the other product, the one developed in pair, has increased by 30%. On the average, the quality of the whole service provided by the team made a greater gap.
Moreover we were both *proud* of what we accomplished. The code was clean, documented and tested. The product owner was very pleased of that.

* Learning was amplified
* team was empowered
* the service was more consistent as a whole (two heads are better than one)

Is that important that my product was not as advanced as it should have been? For the team, definitely not. My product is viable anyway, and the delay induced by the time "lost" is at maximum two weeks. And this delay will be filled if, by the next sprint, we decide to work together with my colleague on my project.

But that is a decision to take by the team and the product owners for the next sprint.

---
Credits:

* Image by DonWells (https://en.wikipedia.org/wiki/File:XP-feedback.gif) [CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons
---
categories:
- category
date: 2017-02-14T10:17:15+01:00
description: ""
draft: true
images:
- /assets/images/default-post.png
tags:
- tag1
- tag2
title: zfs_cloud_ubuntu16.04
---


I am using the device entries `/dev/xvd*` for testing purpose. Other choices may be best suitable for production.
Please refer to [ZFS on linux wiki](https://github.com/zfsonlinux/zfs/wiki/faq#selecting-dev-names-when-creating-a-pool) for more options.

```shell
sudo apt-get install zfsutils-linux

parted /dev/xvdb mklabel GPT
parted /dev/xvdc mklabel GPT
parted /dev/xvdd mklabel GPT

zpool create -m none -o ashift=12 tank raidz /dev/xvdc /dev/xvdb /dev/xvdd
```




RAIDZ is a little like RAID-5. I'm using RAID-Z1, meaning that from a 3-disk pool, I can lose one disk while maintaining the data access.

NOTE: Unlike RAID, once you build your RAIDZ, you cannot add new individual disks. It's a long story.

The -m none means that we do want to specify a mount point for this pool yet.

The -o ashift=12 forces ZFS to use 4K sectors instead of 512 byte sectors. Many new drives use 4K sectors, but lie to the OS about it for "compatability" reasons. My first ZFS filesystem used the 512-byte sectors in the beginning, and I had shocking performance (~10Mb/s write).


zpool create tank raidz /dev/xvdc /dev/xvdb /dev/xvdd 


